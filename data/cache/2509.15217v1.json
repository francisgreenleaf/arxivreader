{
  "success": true,
  "arxiv_id": "2509.15217v1",
  "processed_content": {
    "success": true,
    "arxiv_id": "2509.15217v1",
    "metadata": {
      "arxiv_id": "2509.15217v1",
      "title": "Generalizable Geometric Image Caption Synthesis",
      "authors": [],
      "abstract": "Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of 2.8%\u20134.8% in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with 2.4%-3.9% improvements in Art & Design and Tech & Engineering tasks in MMMU."
    },
    "content": "<div class=\"arxiv-content\">\n<div class=\"ltx_page_content\">\n<article class=\"ltx_document ltx_authors_1line\">\n<h1 class=\"ltx_title ltx_title_document\" id=\"generalizable-geometric-image-caption-synthesis\">Generalizable Geometric Image Caption Synthesis</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">\nAuthor1, Author2 \n<br class=\"ltx_break\"/>Affiliation \n<br class=\"ltx_break\"/>Univ \n<br class=\"ltx_break\"/>City\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">{Author1, Author2}email@email</span>\n<br class=\"ltx_break\"/>&amp;Author3 \n<br class=\"ltx_break\"/>Affiliation \n<br class=\"ltx_break\"/>Univ \n<br class=\"ltx_break\"/>City\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_typewriter\">email@email</span>\n<br class=\"ltx_break\"/>\n</span></span>\n<span class=\"ltx_author_before\">\u2003\u2003</span><span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">\n<span class=\"ltx_text ltx_font_bold\">Yue Xin<sup class=\"ltx_sup\">1,2*<math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"m1\" intent=\":literal\"><semantics><mo>\u2020</mo><annotation encoding=\"application/x-tex\">\\dagger</annotation></semantics></math></sup></span>,\u2003<span class=\"ltx_text ltx_font_bold\">Wenyuan Wang<sup class=\"ltx_sup\">1,3*<math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>\u2020</mo><annotation encoding=\"application/x-tex\">\\dagger</annotation></semantics></math></sup></span>,\u2003<span class=\"ltx_text ltx_font_bold\">Rui Pan<sup class=\"ltx_sup\">1*</sup></span>,\n<br class=\"ltx_break\"/><span class=\"ltx_text ltx_font_bold\">Ruida Wang<sup class=\"ltx_sup\">1</sup></span>,\u2003<span class=\"ltx_text ltx_font_bold\">Howard Meng<sup class=\"ltx_sup\">1</sup></span>,\u2003<span class=\"ltx_text ltx_font_bold\">Renjie Pi<sup class=\"ltx_sup\">4</sup></span>,\u2003<span class=\"ltx_text ltx_font_bold\">Shizhe Diao<sup class=\"ltx_sup\">4</sup></span>,\u2003<span class=\"ltx_text ltx_font_bold\">Tong Zhang<sup class=\"ltx_sup\">1</sup>\n<br class=\"ltx_break\"/><sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">1</span></sup></span>University of Illinois Urbana-Champaign (UIUC), \n<br class=\"ltx_break\"/><sup class=\"ltx_sup\">2</sup>Shanghai Jiao Tong University, \u2003<sup class=\"ltx_sup\">3</sup>Rutgers University, \u2003<sup class=\"ltx_sup\">4</sup>NVIDIA\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/><a class=\"ltx_ref ltx_href\" href=\"https://machinephoenix.github.io/GeoReasoning_blog/\" title=\"\">Project Page</a> \u2003<a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/ScaleMath/GeoReasoning\" title=\"\">Dataset</a> \u2003<a class=\"ltx_ref ltx_href\" href=\"https://github.com/MachinePhoenix/GeoReasoning\" title=\"\">Code</a>\n</span></span>\n</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\" id=\"abstract\">Abstract</h6>\n<p class=\"ltx_p\">Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of 2.8%\u20134.8% in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with 2.4%-3.9% improvements in Art &amp; Design and Tech &amp; Engineering tasks in MMMU.</p>\n</div>\n<span class=\"ltx_note ltx_role_footnotetext\" id=\"footnotex1\"><sup class=\"ltx_note_mark\">*</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">*</sup><span class=\"ltx_note_type\">footnotetext: </span>Core contributors.</span></span></span><span class=\"ltx_note ltx_role_footnotetext\" id=\"footnotex2\"><sup class=\"ltx_note_mark\">$\\dagger$</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">$\\dagger$</sup><span class=\"ltx_note_type\">footnotetext: </span>Work done during a research internship at UIUC.</span></span></span>\n<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"1-introduction\">\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p3\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p4\">\n<p class=\"ltx_p\">To bridge the gap between visual and linguistic modalities, we propose an RL-based data refinement engine that iteratively enhances data quality. Utilizing this pipeline, we introduce a novel geometry dataset comprising 10,000 image-caption pairs. To the best of our knowledge, this is the first high-quality dataset in which visual and textual information are fully aligned and generalize well to out-of-distribution tasks, making it a valuable resource for improving cross-modal reasoning. Experimental results demonstrate that our dataset significantly enhances models\u2019 cross-modal reasoning abilities and their performance on geometric image textualization tasks. Furthermore, models trained on our dataset exhibit strong generalization capabilities on other mathematics-focused benchmarks, including MathVerse and MathVista.\nIn summary, our main contributions are summarized as follows:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p5\">\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.I1.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.I1.i2.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S1.I1.i3.p1\">\n<p class=\"ltx_p\">Extensive experiments show that the improvements facilitated by GeoReasoning extend beyond geometric tasks to non-geometric mathematical tasks and even to non-mathematical domains such as art and engineering.</p>\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"460\" id=\"S1.F1.g1\" src=\"https://arxiv.org/html/x1.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span><span class=\"ltx_text ltx_font_bold\">Examples of generalization</span>. MLLMs learn from our synthetic geometric mathematical problems and generalize to algebraic cases with even non-geometric input images.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"2-related-works\">\n<span class=\"ltx_tag ltx_tag_section\">2 </span>Related Works</h2>\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"21-data-generation\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.1 </span>Data Generation</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p2\">\n<p class=\"ltx_p\">Despite the advances, existing pipelines still struggle to guarantee full modality alignment, i.e., captions frequently omit visual details, while images lack exhaustively aligned textual descriptions.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"22-image-captioning\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.2 </span>Image Captioning</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS2.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS2.p2\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"3-methods\">\n<span class=\"ltx_tag ltx_tag_section\">3 </span>Methods</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p1\">\n<p class=\"ltx_p\">In this section, we introduce our Geo-Image-Textualization data generation pipeline first, followed by our RAFT method used for data refinement.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"31-geo-image-textualization-data-generation-engine\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Geo-Image-Textualization Data Generation Engine</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.p1\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F2\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" height=\"200\" id=\"S3.F2.g1\" src=\"https://arxiv.org/html/x2.png\" width=\"179\"/></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" height=\"201\" id=\"S3.F2.g2\" src=\"https://arxiv.org/html/x3.png\" width=\"203\"/></div>\n<div class=\"ltx_flex_cell ltx_flex_size_3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_square\" height=\"224\" id=\"S3.F2.g3\" src=\"https://arxiv.org/html/x4.png\" width=\"185\"/></div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span><span class=\"ltx_text ltx_font_bold\">Symbolically synthesized geometric images</span>. These geometry problems are symbolically composed from our relation library, corresponding to easy, medium, and hard difficulty levels, respectively, where the pink ticks and red arcs indicate equal-length segments and equal angles. The symbolic engine can generate images with infinite types and difficulties. For visual clarity, this figure has a fixed set of colors, font sizes, and line thicknesses compared to the original images in our constructed dataset. Please refer to the original dataset for precise details.</figcaption>\n</figure>\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"311-relation-sampling\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.1.1 </span>Relation Sampling</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.SSS1.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.SSS1.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"355\" id=\"S3.F3.g1\" src=\"https://arxiv.org/html/x5.png\" width=\"627\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span><span class=\"ltx_text ltx_font_bold\">The geometry data synthesis pipeline</span>, where a graph-based representation similar to AutoGeo\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15217v1#bib.bib16\" title=\"\">huang2025autogeo, </a>)</cite> is employed for generating the final geometry images. The relation library comprises over 50 basic geometric relationships that can be composed into complex ones, providing comprehensive coverage for geometric problems of various difficulties. The image-caption pair is utilized for the SFT stage, while the caption-QA pair is for the RLVR stage.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S3.SS1.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"312-image-caption-pair-generation\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.1.2 </span>Image-Caption Pair Generation</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.SSS2.p1\">\n<p class=\"ltx_p\">The geometric graph encodes all relevant entities, including points, lines, and circles, enabling the straightforward rendering of basic geometric elements, similar to AutoGeo. However, a fundamental limitation of AutoGeo is that the captions cannot be directly inferred from the rendered images because the visual content and the textual description are not semantically aligned. We argue that this misalignment constitutes a critical bottleneck in cross-modal reasoning.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.SSS2.p2\">\n\n<ol class=\"ltx_enumerate\" id=\"S3.I1\">\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i2.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">3.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i3.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">4.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i4.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S3.I1.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">5.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i5.p1\">\n\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.SSS2.p3\">\n<p class=\"ltx_p\">For each clause in the symbolic representation, we apply a refined, rule-based template to convert it into natural language. These captions accurately describe the geometric diagram, including object relationships, special angle values, and other visual properties. Additionally, the captions explicitly state the lengths of specific line segments when such information is visually annotated in the image. By ensuring that all semantic content present in the image is mirrored in the caption, we achieve full cross-modal alignment.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.SSS2.p4\">\n<p class=\"ltx_p\">Building upon these strategies, we construct a comprehensive image-caption generation engine. The engine is entirely rule-based, offering a fast, rigorous, reliable, and cost-effective solution for producing high-quality image-caption pairs to serve as a solid foundation for downstream multi-modal reasoning tasks.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"32-question-answer-pair-generation\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Question-Answer Pair Generation</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p1\">\n<p class=\"ltx_p\">The most fundamental requirements for generating questions lie in three aspects. First, the generated question should be based on the caption, i.e., should not be irrelevant to the caption. Second, any information already present in the caption should be removed, as this would dilute the impact of the caption and make the evaluation of caption quality harder. Last, the question should be compatible with the given information, so that it can be logically answered based on what is provided.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"33-rlvr-framework-for-data-refinement\">\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>RLVR Framework for Data Refinement</h3>\n<figure class=\"ltx_figure\" id=\"S3.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"292\" id=\"S3.F4.g1\" src=\"https://arxiv.org/html/x6.png\" width=\"789\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span><span class=\"ltx_text ltx_font_bold\">The RLVR training framework</span>. In Stage 1, the model is trained to develop a preliminary ability to generate image captions. In Stage 2, an alternating optimization strategy is employed to jointly refine the generated captions and enhance the model\u2019s overall performance. The data of Stage 1 comes from the rule-based image-caption generation pipeline illustrated in Figure\u00a0<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.15217v1#S3.F3\" title=\"Figure 3 \u2023 3.1.1 Relation Sampling \u2023 3.1 Geo-Image-Textualization Data Generation Engine \u2023 3 Methods \u2023 Generalizable Geometric Image Caption Synthesis\"><span class=\"ltx_text ltx_ref_tag\">3</span></a>.</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p1\">\n\n</div>\n<section class=\"ltx_subsubsection\" id=\"S3.SS3.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"331-cold-start-phase\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.3.1 </span>Cold-Start Phase</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS1.p1\">\n<p class=\"ltx_p\">To initialize the model\u2019s ability to generate geometrically aligned captions, we first perform supervised fine-tuning (SFT) on the base vision language model using the GeoReasoning-10K dataset. This phase minimizes the standard cross-entropy loss:</p>\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"A6.EGx1\">\n<tbody id=\"S3.E1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\mathcal{L}_{\\text{SFT}}=-\\mathbb{E}_{(I,c^{\\star})\\sim\\mathcal{D_{0}}}[\\log P_{\\theta_{0}}(c|I)]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E1.m1\" intent=\":literal\"><semantics><mrow><msub><mi class=\"ltx_font_mathcaligraphic\">\u2112</mi><mtext>SFT</mtext></msub><mo>=</mo><mrow><mo>\u2212</mo><mrow><msub><mi>\ud835\udd3c</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>I</mi><mo>,</mo><msup><mi>c</mi><mo>\u22c6</mo></msup><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mn class=\"ltx_font_mathcaligraphic\" mathvariant=\"script\">0</mn></msub></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">\u2061</mo><msub><mi>P</mi><msub><mi>\u03b8</mi><mn>0</mn></msub></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>c</mi><mo fence=\"false\">|</mo><mi>I</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\mathcal{L}_{\\text{SFT}}=-\\mathbb{E}_{(I,c^{\\star})\\sim\\mathcal{D_{0}}}[\\log P_{\\theta_{0}}(c|I)]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S3.SS3.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"332-rlvr-phase\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.3.2 </span>RLVR Phase</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS2.p1\">\n\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS2.Px1\">\n<h5 class=\"ltx_title ltx_title_paragraph\" id=\"rollout-experience-generation\">Rollout Experience Generation</h5>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS2.Px1.p1\">\n\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS2.Px2\">\n<h5 class=\"ltx_title ltx_title_paragraph\" id=\"model-retraining\">Model retraining</h5>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS2.Px2.p1\">\n\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"A6.EGx2\">\n<tbody id=\"S3.E2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\theta_{t+1}=\\arg\\max_{\\theta_{t}}\\mathbb{E}_{(I,c_{\\text{best}})\\sim\\mathcal{D}_{t+1}}[\\log P_{\\theta_{t}}(c_{\\text{best}}|I)]\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>\u03b8</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace=\"0.167em\">\u2061</mo><mrow><munder><mi>max</mi><msub><mi>\u03b8</mi><mi>t</mi></msub></munder><mo lspace=\"0.167em\">\u2061</mo><msub><mi>\ud835\udd3c</mi><mrow><mrow><mo stretchy=\"false\">(</mo><mi>I</mi><mo>,</mo><msub><mi>c</mi><mtext>best</mtext></msub><mo stretchy=\"false\">)</mo></mrow><mo>\u223c</mo><msub><mi class=\"ltx_font_mathcaligraphic\">\ud835\udc9f</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></msub></mrow></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">[</mo><mrow><mrow><mi>log</mi><mo lspace=\"0.167em\">\u2061</mo><msub><mi>P</mi><msub><mi>\u03b8</mi><mi>t</mi></msub></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mrow><msub><mi>c</mi><mtext>best</mtext></msub><mo fence=\"false\">|</mo><mi>I</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">]</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\theta_{t+1}=\\arg\\max_{\\theta_{t}}\\mathbb{E}_{(I,c_{\\text{best}})\\sim\\mathcal{D}_{t+1}}[\\log P_{\\theta_{t}}(c_{\\text{best}}|I)]</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS2.Px2.p2\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S3.SS3.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"333-reward-function\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">3.3.3 </span>Reward function</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS3.p1\">\n\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"A6.EGx3\">\n<tbody id=\"S3.E3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle R(c,I)=\\lambda_{r}\\cdot R_{\\text{reasoning}}(c,q)+(1-\\lambda_{r})\\cdot R_{\\text{caption}}(c,c^{\\star})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E3.m1\" intent=\":literal\"><semantics><mrow><mrow><mi>R</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>\u03bb</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><msub><mi>R</mi><mtext>reasoning</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>\u2212</mo><msub><mi>\u03bb</mi><mi>r</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">\u22c5</mo><msub><mi>R</mi><mtext>caption</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>\u22c6</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle R(c,I)=\\lambda_{r}\\cdot R_{\\text{reasoning}}(c,q)+(1-\\lambda_{r})\\cdot R_{\\text{caption}}(c,c^{\\star})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n</div>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS3.Px1\">\n<h5 class=\"ltx_title ltx_title_paragraph\" id=\"reasoning-reaward\">Reasoning reaward</h5>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS3.Px1.p1\">\n\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"A6.EGx4\">\n<tbody id=\"S3.E4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle R_{\\text{Reasoning}}=s_{c}\\cdot\\mathbb{I}(a=a^{\\star})+(1-s_{c})\\cdot\\mathbb{F}(a)\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>R</mi><mtext>Reasoning</mtext></msub><mo>=</mo><mrow><mrow><mrow><msub><mi>s</mi><mi>c</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mi>\ud835\udd40</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mi>a</mi><mo>=</mo><msup><mi>a</mi><mo>\u22c6</mo></msup></mrow><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>\u2212</mo><msub><mi>s</mi><mi>c</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">\u22c5</mo><mi>\ud835\udd3d</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>a</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle R_{\\text{Reasoning}}=s_{c}\\cdot\\mathbb{I}(a=a^{\\star})+(1-s_{c})\\cdot\\mathbb{F}(a)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n\n</div>\n</section>\n<section class=\"ltx_paragraph\" id=\"S3.SS3.SSS3.Px2\">\n<h5 class=\"ltx_title ltx_title_paragraph\" id=\"caption-reward\">Caption reward</h5>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.SSS3.Px2.p1\">\n\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"A6.EGx5\">\n<tbody id=\"S3.E5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle R_{\\text{caption}}=w_{r}\\cdot ROUGE(c,c^{\\star})+(1-w_{r})\\cdot BLEU(c,c^{\\star})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.E5.m1\" intent=\":literal\"><semantics><mrow><msub><mi>R</mi><mtext>caption</mtext></msub><mo>=</mo><mrow><mrow><mrow><msub><mi>w</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mi>R</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>O</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>G</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>\u22c6</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>\u2212</mo><msub><mi>w</mi><mi>r</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">\u22c5</mo><mi>B</mi></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>L</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>E</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>U</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>\u22c6</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle R_{\\text{caption}}=w_{r}\\cdot ROUGE(c,c^{\\star})+(1-w_{r})\\cdot BLEU(c,c^{\\star})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(5)</span></td>\n</tr></tbody>\n</table>\n\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"153\" id=\"S3.F5.g1\" src=\"https://arxiv.org/html/x7.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span><span class=\"ltx_text ltx_font_bold\">The reward function</span>. Given the Generated caption (G), Question (Q), and Answer (A), the reward function measures the caption\u2019s quality from two aspects: 1) its reasoning reward, and 2) caption reward, as forumated as <math alttext=\"R(c,I)=\\lambda_{r}\\cdot R_{\\text{reasoning}}(c,q)+(1-\\lambda_{r})\\cdot R_{\\text{caption}}(c,c^{\\star})\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F5.m2\" intent=\":literal\"><semantics><mrow><mrow><mi>R</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi>I</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>\u03bb</mi><mi>r</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><msub><mi>R</mi><mtext>reasoning</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><mi>q</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>+</mo><mrow><mrow><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>\u2212</mo><msub><mi>\u03bb</mi><mi>r</mi></msub></mrow><mo rspace=\"0.055em\" stretchy=\"false\">)</mo></mrow><mo rspace=\"0.222em\">\u22c5</mo><msub><mi>R</mi><mtext>caption</mtext></msub></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>c</mi><mo>,</mo><msup><mi>c</mi><mo>\u22c6</mo></msup><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">R(c,I)=\\lambda_{r}\\cdot R_{\\text{reasoning}}(c,q)+(1-\\lambda_{r})\\cdot R_{\\text{caption}}(c,c^{\\star})</annotation></semantics></math>. <span class=\"ltx_text ltx_font_bold\">Reasoning reward</span> stands for the caption\u2019s relevance to the image and question, especially the ability to capture the key reasoning information for solving the question. <span class=\"ltx_text ltx_font_bold\">Caption reward</span> is an auxiliary reward signal that measures the caption\u2019s similarity to the ground truth caption.</figcaption>\n</figure>\n</section>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"4-experiments\">\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Experiments</h2>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"41-experimental-setup\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Experimental Setup</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS1.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"42-in-domain-performance-of-georeasoning-10k\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>In-Domain Performance of GeoReasoning-10K</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p3\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S4.T1\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span><span class=\"ltx_text ltx_font_bold\">Better In-Domain Performance</span>. Accuracy of Gemma3-4B models trained on 10k random samples of each dataset over 4 trials. Results on several subtasks in MathVerse and MathVista are also reported here.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:345.0pt;height:69.8pt;vertical-align:-33.3pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-98.1pt,19.9pt) scale(0.63747954863356,0.63747954863356) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding:1pt 2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"5\" style=\"padding:1pt 2.5pt;\">MathVista (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">\u2191</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\" style=\"padding:1pt 2.5pt;\">MathVerse (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">\u2191</mo><annotation encoding=\"application/x-tex\">\\uparrow</annotation></semantics></math>)</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td\" style=\"padding:1pt 2.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">Overall</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">Geometry</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">Algebra</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">Science</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:1pt 2.5pt;\">Statistic</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:1pt 2.5pt;\">Overall</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:1pt 2.5pt;\">Vision</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:1pt 2.5pt;\">-Dominant</td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:1pt 2.5pt;\">Text</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:1pt 2.5pt;\">-Dominant</td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:1pt 2.5pt;\">Text</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding:1pt 2.5pt;\">-Lite</td>\n</tr>\n</table></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">Base</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">46.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">60.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">53.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:1pt 2.5pt;\">43.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">25.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">24.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">32.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:1pt 2.5pt;\">25.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">AutoGeo</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">47.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m3\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.8</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">62.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m4\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">60.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m5\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.9</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">52.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m6\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:1pt 2.5pt;\">44.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m7\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.9</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">24.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m8\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">22.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m9\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">35.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m10\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">26.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m11\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">GeoPeP</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">47.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m12\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">61.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m13\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.3</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">59.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m14\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.8</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">54.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m15\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.6</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:1pt 2.5pt;\">44.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m16\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.8</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">24.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m17\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.2</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">21.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m18\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.9</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">33.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m19\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.3</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">25.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m20\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">GeoGPT4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">47.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m21\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.2</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">60.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m22\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">59.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m23\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">54.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m24\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:1pt 2.5pt;\">44.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m25\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.0</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">25.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m26\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">22.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m27\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.8</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">36.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m28\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">26.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m29\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">Geo170K</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">47.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m30\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.3</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">62.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m31\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.5</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">60.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m32\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.2</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">53.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m33\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:1pt 2.5pt;\">43.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m34\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.4</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">25.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m35\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.1</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">22.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m36\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.0</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">35.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m37\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.7</td>\n<td class=\"ltx_td ltx_align_left\" style=\"padding:1pt 2.5pt;\">26.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m38\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\">GeoReasoning</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">48.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m39\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">62.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m40\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">61.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m41\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">54.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m42\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.2</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">46.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m43\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">25.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m44\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.1</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">24.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m45\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.8</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">36.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m46\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.4</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:1pt 2.5pt;\"><span class=\"ltx_text ltx_font_bold\">28.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.m47\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5</span></td>\n</tr>\n</table>\n</span></div>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p4\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F6\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S4.F6.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"620\" id=\"S4.F6.sf1.g1\" src=\"https://arxiv.org/html/x8.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">(a) </span>MathVista</figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S4.F6.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"620\" id=\"S4.F6.sf2.g1\" src=\"https://arxiv.org/html/x9.png\" width=\"830\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">(b) </span>MathVerse</figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span><span class=\"ltx_text ltx_font_bold\">Better Scalability</span>. The accuracy of models fine-tuned on different capacities and mathematical datasets on downstream evaluation benchmarks: (a) MathVista. (b) MathVerse.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"43-out-of-domain-performance-of-georeasoning-10k\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Out-of-Domain Performance of GeoReasoning-10K</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS3.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S4.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span><span class=\"ltx_text ltx_font_bold\">Better Out-of-Domain Performance</span>. Accuracy of models evaluated on all subtasks of MMMU over 5 trials, where \u201cA&amp;D\u201d, \u201cBusi\u201d, \u201cSci\u201d, \u201cH&amp;M\u201d, \u201cHuman\u201d, \u201cTech\u201d are short for \u201cArt and Design\u201d, \u201cBusiness\u201d, \u201cScience\u201d, \u201cHealth and Medicine\u201d, \u201cHumanities and Social Science\u201d, \u201cTech and Engineering\u201d, respectively.</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:80.3pt;vertical-align:-37.8pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-13.4pt,2.5pt) scale(0.94177223210669,0.94177223210669) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Overall</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">A&amp;D</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Busi</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Sci</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">H&amp;M</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Human</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">Tech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Base</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m1\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">57.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m2\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>4.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m3\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">34.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m4\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m5\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m6\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">29.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m7\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">AutoGeo</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m8\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m9\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m10\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">34.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m11\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">47.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m12\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m13\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m14\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GeoPeP</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m15\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m16\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">40.4<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m17\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">34.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m18\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">45.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m19\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m20\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">32.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m21\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GeoGPT4V</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">44.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m22\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">60.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m23\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.1<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m24\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">34.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m25\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m26\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.3<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m27\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m28\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Geo170K</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">42.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m29\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">58.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m30\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">43.6<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m31\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m32\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">46.8<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m33\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">59.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m34\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">30.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m35\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">GeoReasoning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">44.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m36\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">60.2<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m37\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">44.5<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m38\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">36.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m39\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>2.0</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">46.7<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m40\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">60.0<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m41\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>0.5</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\"><span class=\"ltx_text ltx_font_bold\">32.9<math alttext=\"\\pm\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T2.m42\" intent=\":literal\"><semantics><mo>\u00b1</mo><annotation encoding=\"application/x-tex\">\\pm</annotation></semantics></math>1.3</span></td>\n</tr>\n</table>\n</span></div>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS3.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S4.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"475\" id=\"S4.F7.g1\" src=\"https://arxiv.org/html/x10.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>An engineering case, where the base model\u2019s answer is relatively general and the analysis of shape is not rigorous enough, while the model after training on GeoReasoning is more detailed and accurate in observing shape and has spatial reasoning ability.</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS3.p3\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"44-ablation-study\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.4 </span>Ablation Study</h3>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S4.T3\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Accuracy of Gemma3-4B models of various stages evaluated on MathVista and MathVerse.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">MathVerse</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">MathVista</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">Gemma3-4B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">25.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">46.2</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">Gemma3-4B-Coldstart</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">25.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">47.6</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">Gemma3-4B-RAFT</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">26.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">49.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">Gemma3-4B-Coldstart-RAFT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS4.p2\">\n<p class=\"ltx_p\">It can be concluded that both cold-start and RLVR are effective. Furthermore, RLVR is helpful for both the base model and the one after cold-start, and cold-start is essential for enabling the model to acquire task-related capabilities, especially when the base model performs poorly in the given domain, such as MathVerse.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"5-conclusion\">\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Conclusion</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S5.p1\">\n\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\" id=\"references\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2.5-vl technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2502.13923</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 24185\u201324198, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nHaotian Liu, Chunyuan Li, Yuheng Li, Bo\u00a0Li, Yuanhan Zhang, Sheng Shen, and Yong\u00a0Jae Lee.\n\n</span>\n<span class=\"ltx_bibblock\">Llava-next: Improved reasoning, ocr, and world knowledge, January 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Openai o1 system card.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2412.16720</span>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\nHao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu\u00a0Liu, and Hongsheng Li.\n\n</span>\n<span class=\"ltx_bibblock\">Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, 37:8612\u20138642, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\nRenrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu\u00a0Qiao, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">European Conference on Computer Vision</span>, pages 169\u2013186. Springer, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.\n\n</span>\n<span class=\"ltx_bibblock\">Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2310.02255</span>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\nRenrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Mavis: Mathematical visual instruction tuning with an automatic data engine.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2407.08739</span>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\nKe\u00a0Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li.\n\n</span>\n<span class=\"ltx_bibblock\">Measuring multimodal mathematical reasoning with math-vision dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Advances in Neural Information Processing Systems</span>, 37:95095\u201395169, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2501.12948</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nYihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang.\n\n</span>\n<span class=\"ltx_bibblock\">Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2503.17352</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nYingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu\u00a0Yang.\n\n</span>\n<span class=\"ltx_bibblock\">Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2503.07536</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nWenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin.\n\n</span>\n<span class=\"ltx_bibblock\">Vision-r1: Incentivizing reasoning capability in multimodal large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2503.06749</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and T.\u00a0Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Raft: Reward ranked finetuning for generative foundation model alignment.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ArXiv</span>, abs/2304.06767, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nTrieu\u00a0H Trinh, Yuhuai Wu, Quoc\u00a0V Le, He\u00a0He, and Thang Luong.\n\n</span>\n<span class=\"ltx_bibblock\">Solving olympiad geometry without human demonstrations.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Nature</span>, 625(7995):476\u2013482, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nZihan Huang, Tao Wu, Wang Lin, Shengyu Zhang, Jingyuan Chen, and Fei Wu.\n\n</span>\n<span class=\"ltx_bibblock\">Autogeo: Automating geometric image dataset creation for enhanced geometry understanding.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE Transactions on Multimedia</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nYanpeng Sun, Shan Zhang, Wei Tang, Aotian Chen, Piotr Koniusz, Kai Zou, Yuan Xue, and Anton van\u00a0den Hengel.\n\n</span>\n<span class=\"ltx_bibblock\">Mathglance: Multimodal large language models do not know where to look in mathematical diagrams.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2503.20745</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nJunxiao Wang, Ting Zhang, Heng Yu, Jingdong Wang, and Hua Huang.\n\n</span>\n<span class=\"ltx_bibblock\">Magicgeo: Training-free text-guided geometric diagram generation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2502.13855</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi\u00a0Qian, Ji\u00a0Zhang, and Fei Huang.\n\n</span>\n<span class=\"ltx_bibblock\">mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.\n\n</span>\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</span>, pages 13040\u201313051, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\n\n</span>\n<span class=\"ltx_bibblock\">Minigpt-4: Enhancing vision-language understanding with advanced large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2304.10592</span>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nLe\u00a0Xue, Manli Shu, Anas Awadalla, Jun Wang, An\u00a0Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael\u00a0S Ryoo, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">xgen-mm (blip-3): A family of open large multimodal models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2408.08872</span>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nRenjie Pi, Jianshu Zhang, Jipeng Zhang, Rui Pan, Zhekai Chen, and Tong Zhang.\n\n</span>\n<span class=\"ltx_bibblock\">Image textualization: An automatic framework for creating accurate and detailed image descriptions.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2406.07502</span>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nYiting Lu, Jiakang Yuan, Zhen Li, Shitian Zhao, Qi\u00a0Qin, Xinyue Li, Le\u00a0Zhuo, Licheng Wen, Dongyang Liu, Yuewen Cao, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Omnicaptioner: One captioner to rule them all.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2504.07089</span>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nAn\u00a0Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo\u00a0Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et\u00a0al.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2. 5 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2412.15115</span>, 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nClement Farabet and Tris Warkentin.\n\n</span>\n<span class=\"ltx_bibblock\">Introducing gemma 3: The most capable model you can run on a single gpu or tpu.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://blog.google/technology/developers/gemma-3/\" title=\"\">https://blog.google/technology/developers/gemma-3/</a>, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nShihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Song, and Bo\u00a0Zheng.\n\n</span>\n<span class=\"ltx_bibblock\">GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv e-prints</span>, page arXiv:2406.11503, June 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nJiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong.\n\n</span>\n<span class=\"ltx_bibblock\">G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv e-prints</span>, page arXiv:2312.11370, December 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nJiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin.\n\n</span>\n<span class=\"ltx_bibblock\">GeoQA: A geometric question answering benchmark towards multimodal numerical reasoning.\n\n</span>\n<span class=\"ltx_bibblock\">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <span class=\"ltx_text ltx_font_italic\">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</span>, pages 513\u2013523, Online, August 2021. Association for Computational Linguistics.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nKe\u00a0Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li.\n\n</span>\n<span class=\"ltx_bibblock\">Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv e-prints</span>, page arXiv:2402.14804, February 2024.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge\u00a0Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu\u00a0Su, and Wenhu Chen.\n\n</span>\n<span class=\"ltx_bibblock\">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv e-prints</span>, page arXiv:2311.16502, November 2023.\n\n</span>\n</li>\n</ul>\n</section>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<section class=\"ltx_appendix\" id=\"A1\">\n<h2 class=\"ltx_title ltx_title_appendix\" id=\"appendix-a-question-answer-pair-generation-prompt\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Question-Answer Pair Generation Prompt</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p1\">\n<p class=\"ltx_p\">The rule-and-LLM-based pipeline contains two stages. We first design a prompt that satisfies all the above conditions, using a relatively low temperature (0.2 in our experiments) to encourage the large model (Gemini 2.5 Flash) to generate initial questions based on the caption, while also labeling those that are inconsistent with the caption. For the inconsistent questions, we then switch to a different prompt, encouraging the model to incorporate additional information and formulate new questions accordingly, while increasing the temperature to 0.8. This process continues until a self-consistent question is generated for the first time.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p2\">\n<p class=\"ltx_p\">The prompt of the first question generation stage is set as:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p3\">\n<span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"466.98\" id=\"A1.p3.pic1\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 600 466.98\" width=\"600\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,466.98) matrix(1 0 0 -1 0 0)\"><g fill=\"#404040\" fill-opacity=\"1.0\" style=\"--ltx-fill-color:#404040;\"><path d=\"M 0 13.19 L 0 453.79 C 0 461.08 5.91 466.98 13.19 466.98 L 586.81 466.98 C 594.09 466.98 600 461.08 600 453.79 L 600 13.19 C 600 5.91 594.09 0 586.81 0 L 13.19 0 C 5.91 0 0 5.91 0 13.19 Z\" style=\"stroke:none\"></path></g><g fill=\"#F2F2F2\" fill-opacity=\"1.0\" style=\"--ltx-fill-color:#F2F2F2;\"><path d=\"M 1.38 13.19 L 1.38 444.16 L 598.62 444.16 L 598.62 13.19 C 598.62 6.67 593.33 1.38 586.81 1.38 L 13.19 1.38 C 6.67 1.38 1.38 6.67 1.38 13.19 Z\" style=\"stroke:none\"></path></g><g fill=\"#000000\" fill-opacity=\"1.0\" style=\"--ltx-fill-color:#000000;\"><path d=\"M 1.38 445.54 L 1.38 453.79 C 1.38 460.31 6.67 465.6 13.19 465.6 L 586.81 465.6 C 593.33 465.6 598.62 460.31 598.62 453.79 L 598.62 445.54 Z\" style=\"stroke:none\"></path></g><g fill-opacity=\"1.0\" transform=\"matrix(1.0 0.0 0.0 1.0 13.19 452.17)\"><foreignobject color=\"#FFFFFF\" height=\"12.18\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :41.45em;--fo_height:0.69em;--fo_depth :0.19em;\" transform=\"matrix(1 0 0 -1 0 9.49)\" width=\"573.61\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_bottom\" style=\"width:36.05em;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt1</span></span>\n</span></span></span></foreignobject></g><g fill-opacity=\"1.0\" transform=\"matrix(1.0 0.0 0.0 1.0 13.19 12.72)\"><foreignobject color=\"#000000\" height=\"427.03\" overflow=\"visible\" style=\"--ltx-fg-color:#000000;--fo_width :41.45em;--fo_height:30.61em;--fo_depth :0.25em;\" transform=\"matrix(1 0 0 -1 0 423.57)\" width=\"573.61\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_bottom\" style=\"width:39.48em;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a helpful dataset processor. Please:</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">1. Generate a mathemetical question according to the given description of a geometric image with the following requirements:\n<br class=\"ltx_break\"/>1.1 The problem should base on the given description, i.e., you must **NOT** generate problems that are unrelated to the given description. \n<br class=\"ltx_break\"/>1.2 The problem difficulty should not be too low, such as determining some information in the description. \n<br class=\"ltx_break\"/>1.3 It should also not be too hard, like introducing too much extra information, but anyway you can introduce some extra information to form a good geometric problem. \n<br class=\"ltx_break\"/>1.4 You should **NOT** include or repeat any information in the description, and just contain the real question. For example, if the description says: \u2018Line segment AB is present. The length of BA is 1.24.\u2019, then when you generate the question, you should not include the length of AB.\n<br class=\"ltx_break\"/>1.5 If the question is inconsistent with the given description, the final answer should be \u2018None\u2019.\n<br class=\"ltx_break\"/>2. Answer the question you just provided, and express the final answer to two decimal places. The final answer should be in \\\\boxed{{answer}}.\n<br class=\"ltx_break\"/></span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Description: \n<br class=\"ltx_break\"/>{description}\n<br class=\"ltx_break\"/>Generated Question:\n<br class=\"ltx_break\"/>{question}\n<br class=\"ltx_break\"/>Generated Response:\n<br class=\"ltx_break\"/>{response}\n<br class=\"ltx_break\"/>Final Answer:\n<br class=\"ltx_break\"/>\\\\boxed{{answer}}</span></span>\n</span></span></span></foreignobject></g></g></svg></span>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p4\">\n<p class=\"ltx_p\">The prompt of the question re-generation stage is set as:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p5\">\n<span class=\"ltx_inline-block\"><svg class=\"ltx_picture\" height=\"500.19\" id=\"A1.p5.pic1\" overflow=\"visible\" version=\"1.1\" viewbox=\"0 0 600 500.19\" width=\"600\"><g fill=\"#000000\" stroke=\"#000000\" stroke-width=\"0.4pt\" style=\"--ltx-stroke-color:#000000;--ltx-fill-color:#000000;\" transform=\"translate(0,500.19) matrix(1 0 0 -1 0 0)\"><g fill=\"#404040\" fill-opacity=\"1.0\" style=\"--ltx-fill-color:#404040;\"><path d=\"M 0 13.19 L 0 487 C 0 494.28 5.91 500.19 13.19 500.19 L 586.81 500.19 C 594.09 500.19 600 494.28 600 487 L 600 13.19 C 600 5.91 594.09 0 586.81 0 L 13.19 0 C 5.91 0 0 5.91 0 13.19 Z\" style=\"stroke:none\"></path></g><g fill=\"#F2F2F2\" fill-opacity=\"1.0\" style=\"--ltx-fill-color:#F2F2F2;\"><path d=\"M 1.38 13.19 L 1.38 477.37 L 598.62 477.37 L 598.62 13.19 C 598.62 6.67 593.33 1.38 586.81 1.38 L 13.19 1.38 C 6.67 1.38 1.38 6.67 1.38 13.19 Z\" style=\"stroke:none\"></path></g><g fill=\"#000000\" fill-opacity=\"1.0\" style=\"--ltx-fill-color:#000000;\"><path d=\"M 1.38 478.75 L 1.38 487 C 1.38 493.52 6.67 498.81 13.19 498.81 L 586.81 498.81 C 593.33 498.81 598.62 493.52 598.62 487 L 598.62 478.75 Z\" style=\"stroke:none\"></path></g><g fill-opacity=\"1.0\" transform=\"matrix(1.0 0.0 0.0 1.0 13.19 485.38)\"><foreignobject color=\"#FFFFFF\" height=\"12.18\" overflow=\"visible\" style=\"--ltx-fg-color:#FFFFFF;--fo_width :41.45em;--fo_height:0.69em;--fo_depth :0.19em;\" transform=\"matrix(1 0 0 -1 0 9.49)\" width=\"573.61\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_bottom\" style=\"width:36.05em;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Prompt1</span></span>\n</span></span></span></foreignobject></g><g fill-opacity=\"1.0\" transform=\"matrix(1.0 0.0 0.0 1.0 13.19 12.72)\"><foreignobject color=\"#000000\" height=\"460.23\" overflow=\"visible\" style=\"--ltx-fg-color:#000000;--fo_width :41.45em;--fo_height:33.01em;--fo_depth :0.25em;\" transform=\"matrix(1 0 0 -1 0 456.77)\" width=\"573.61\"><span class=\"ltx_foreignobject_container\"><span class=\"ltx_foreignobject_content\">\n<span class=\"ltx_inline-block ltx_minipage ltx_align_bottom\" style=\"width:39.48em;\">\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">You are a helpful dataset processor. Please:\n1. Generate a mathemetical question according to the given description of a geometric image with the following requirements:\n<br class=\"ltx_break\"/>1.1 The problem should base on the given description, i.e., you must **NOT** generate problems that are unrelated to the given description. \n<br class=\"ltx_break\"/>1.2 You can introduce some extra information to form a good geometric problem.\n<br class=\"ltx_break\"/>1.3 If you find that it is hard to generate some difficult questions, just give a simple question. For example, when the lengths of all four sides of a quadrilateral are given, you can no longer assume it is a parallelogram or rectangle. In such cases, the problem may only allow for questions like asking for the perimeter, or determining the length of a segment when a certain point divides a side into an n-equal part, etc.\n<br class=\"ltx_break\"/>1.4 You should **NOT** include or repeat any information in the description, and just contain the real question. For example, if the description says: \u2018Line segment AB is present. The length of BA is 1.24.\u2019, then when you generate the question, you should not include the length of AB.\n<br class=\"ltx_break\"/>1.5 If the question is inconsistent with the given description, the final answer should be \u2018None\u2019.\n<br class=\"ltx_break\"/>2. Answer the question you just provided, and express the final answer to two decimal places. The final answer should be in \\\\boxed{{answer}}.</span></span>\n<span class=\"ltx_p\"><span class=\"ltx_text ltx_font_typewriter\">Description: \n<br class=\"ltx_break\"/>{description}\n<br class=\"ltx_break\"/>Generated Question:\n<br class=\"ltx_break\"/>{question}\n<br class=\"ltx_break\"/>Generated Response:\n<br class=\"ltx_break\"/>{response}\n<br class=\"ltx_break\"/>Final Answer:\n<br class=\"ltx_break\"/>\\\\boxed{{answer}}</span></span>\n</span></span></span></foreignobject></g></g></svg></span>\n</div>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A2\">\n<h2 class=\"ltx_title ltx_title_appendix\" id=\"appendix-b-experimental-setup-and-dataset-information\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Experimental Setup and Dataset Information</h2>\n<section class=\"ltx_subsection\" id=\"A2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"b1-experimental-setup\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.1 </span>Experimental Setup</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS1.p1\">\n<p class=\"ltx_p\">The training and optimization pipeline contains two stages:</p>\n<ol class=\"ltx_enumerate\" id=\"A2.I1\">\n<li class=\"ltx_item\" id=\"A2.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1.</span>\n<div class=\"ltx_para\" id=\"A2.I1.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2.</span>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.I1.i2.p1\">\n\n<ul class=\"ltx_itemize\" id=\"A2.I1.i2.I1\">\n<li class=\"ltx_item\" id=\"A2.I1.i2.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"A2.I1.i2.I1.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I1.i2.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.I1.i2.I1.i2.p1\">\n<p class=\"ltx_p\">2.2) Model Retraining: Fine-tune the model on the selected dataset for 1 epoch using the same hyperparameters as the cold start phase.</p>\n</div>\n</li>\n</ul>\n</div>\n</li>\n</ol>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS1.p2\">\n<p class=\"ltx_p\">To ensure consistency, we adopt the official evaluation codebases of both MathVerse and MathVista, using the GPT-4o-mini API to evaluate the performance of our MLLM. Specifically, following each benchmark\u2019s official setup, we use GPT-4o-mini to extract and assess the correctness of answers for MathVerse, and to extract answers for MathVista.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS1.p3\">\n<p class=\"ltx_p\">We evaluate MLLMs on MathVerse, MathVista, and MMMU using A100 by VLLM. We employ Gemma3-4B as our base model and fine-tune it on Georeasoning-10K using 4 L20 GPUs. The training process is distributed using torchrun with the DeepSpeed ZeRO-3 optimization strategy.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"b2-data-source\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.2 </span>Data Source</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS2.p1\">\n<p class=\"ltx_p\">GeoReasoning-10K dataset is generated through rule-based methods and further refined using the RAFT framework. The question-answer pairs are generated by Gemini 2.5-Flash with a specific prompt.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"b3-license\">\n<span class=\"ltx_tag ltx_tag_subsection\">B.3 </span>License</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.SS3.p1\">\n<ul class=\"ltx_itemize\" id=\"A2.I2\">\n<li class=\"ltx_item\" id=\"A2.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"A2.I2.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"A2.I2.i2.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"A2.I2.i3.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"A2.I2.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"A2.I2.i4.p1\">\n<p class=\"ltx_p\">All third-party datasets and models used in this work are under their respective licenses, and we ensure compliance with their terms of use.</p>\n</div>\n</li>\n</ul>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A3\">\n<h2 class=\"ltx_title ltx_title_appendix\" id=\"appendix-c-case-studies\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Case Studies</h2>\n<div class=\"ltx_para\" id=\"A3.p1\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"474\" id=\"A3.F8.g1\" src=\"https://arxiv.org/html/x11.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>A geometric case.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"479\" id=\"A3.F9.g1\" src=\"https://arxiv.org/html/x12.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>An arithmetic case.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"490\" id=\"A3.F10.g1\" src=\"https://arxiv.org/html/x13.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>A numeric case.</figcaption>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n<div class=\"ltx_para\" id=\"A3.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"A3.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"760\" id=\"A3.F11.g1\" src=\"https://arxiv.org/html/x14.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>A physics case.</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"A3.F12\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"572\" id=\"A3.F12.g1\" src=\"https://arxiv.org/html/x15.png\" width=\"747\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>An economics case.</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"A3.p3\">\n<p class=\"ltx_p\">All these examples indicate that training on geometric caption tasks stimulates the reasoning capacity of models.</p>\n</div>\n</section>\n<section class=\"ltx_appendix\" id=\"A4\">\n<h2 class=\"ltx_title ltx_title_appendix\" id=\"appendix-d-comparison-to-other-datasets\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Comparison to Other Datasets</h2>\n<div class=\"ltx_para\" id=\"A4.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"A4.T4\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Performance of Gemma3-4B models trained on the total capacity of our dataset and counterpart datasets.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">Capacity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">MathVerse</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">MathVista</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">Geo170K</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">117k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">22.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">46.8</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 15.0pt;\">GeoPeP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">100k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">22.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">47.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 15.0pt;\">GeoGPT4V</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">23k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">24.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">46.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 15.0pt;\">AutoGeo</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">100k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">24.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">46.1</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 15.0pt;\">MathVision</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">3k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">24.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">46.9</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\" style=\"padding:0.5pt 15.0pt;\">GeoQA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">5k</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">24.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">46.0</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\">GeoReasoning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">10k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">25.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">48.6</span></td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\n</section>\n<section class=\"ltx_appendix\" id=\"A5\">\n<h2 class=\"ltx_title ltx_title_appendix\" id=\"appendix-e-additional-ablation-studies\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Additional Ablation Studies</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.p1\">\n\n</div>\n<section class=\"ltx_subsection\" id=\"A5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"e1-ablation-study-on-various-domains\">\n<span class=\"ltx_tag ltx_tag_subsection\">E.1 </span>Ablation Study on Various Domains</h3>\n<div class=\"ltx_para\" id=\"A5.SS1.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"A5.T5\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Accuracy of Gemma3-4B models at various stages tested on MathVista</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">baseline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">cold-start</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">46.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">47.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">48.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">48.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">49.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">49.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">geometry</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">60.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">62.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">63.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">64.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">63.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">60.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">64.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">arithmetic</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">42.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">45.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">44.8</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">45.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">45.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">47.6</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">46.5</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">algebraic</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">60.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">62.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">62.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">62.3</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">59.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">63.3</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">numeric</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">26.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">31.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">29.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">31.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">31.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">31.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">31.9</span></td>\n</tr>\n</table>\n</figure>\n<figure class=\"ltx_table\" id=\"A5.T6\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Accuracy of Gemma3-4B models at various stages tested on MathVerse</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">baseline</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">cold-start</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-3</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">raft-5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">all</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">25.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">25.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">25.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">25.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">25.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\">26.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">text dominant</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">32.0</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">35.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">35.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">35.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">35.1</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">36.5</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">36.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">text lite</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">25.9</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">27.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">28.2</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">28.5</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">27.4</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">26.6</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 8.0pt;\">26.3</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 8.0pt;\">vision intensive</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">24.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">24.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">24.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">24.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">23.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\">26.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 8.0pt;\"><span class=\"ltx_text ltx_font_bold\">26.5</span></td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS1.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"A5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"e2-ablation-study-on-hyperparameters\">\n<span class=\"ltx_tag ltx_tag_subsection\">E.2 </span>Ablation Study on Hyperparameters</h3>\n<div class=\"ltx_para\" id=\"A5.SS2.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"A5.T7\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Accuracy of RAFTed models with various hyperparameters evaluated on MathVista and MathVerse, where <math alttext=\"\\lambda_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m2\" intent=\":literal\"><semantics><msub><mi>\u03bb</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\lambda_{r}</annotation></semantics></math> stands for the weight of reasoning reward.</figcaption>\n<table class=\"ltx_tabular ltx_centering ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">MathVista</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">MathVerse</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">\n<math alttext=\"\\lambda_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m3\" intent=\":literal\"><semantics><msub><mi>\u03bb</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\lambda_{r}</annotation></semantics></math> = 1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 15.0pt;\">49.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">27.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">\n<math alttext=\"\\lambda_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m4\" intent=\":literal\"><semantics><msub><mi>\u03bb</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\lambda_{r}</annotation></semantics></math>=0.7</td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\"><span class=\"ltx_text ltx_font_bold\">50.0</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"padding:0.5pt 15.0pt;\">27.4</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\" style=\"padding:0.5pt 15.0pt;\">\n<math alttext=\"\\lambda_{r}\" class=\"ltx_Math\" display=\"inline\" id=\"A5.T7.m5\" intent=\":literal\"><semantics><msub><mi>\u03bb</mi><mi>r</mi></msub><annotation encoding=\"application/x-tex\">\\lambda_{r}</annotation></semantics></math>=0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\">48.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding:0.5pt 15.0pt;\">27.5</td>\n</tr>\n</table>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"A5.SS2.p3\">\n<p class=\"ltx_p\">In addition, it is observed in the result that the performance is not very sensitive to the selection of this hyperparameter, indicating the robustness of our RAFT method.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_appendix\" id=\"A6\">\n<h2 class=\"ltx_title ltx_title_appendix\" id=\"appendix-f-broader-impacts\">\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix F </span>Broader Impacts</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p1\">\n<p class=\"ltx_p\">The provided dataset pipeline and the generated dataset contribute to enhancing the generalizable reasoning abilities of multimodal large language models (MLLMs). In narrow domains, they are particularly effective for improving the geometric problem-solving capabilities of MLLMs, while in broader domains, they support the development of mathematical reasoning skills applicable to everyday scenarios. As the dataset is limited to geometric mathematical problems, it is considered safe for release and is unlikely to pose direct negative social impacts.</p>\n</div>\n</section>\n</article>\n</div>\n\n</div>",
    "sections": [
      {
        "id": "generalizable-geometric-image-caption-synthesis",
        "title": "Generalizable Geometric Image Caption Synthesis",
        "level": 1
      },
      {
        "id": "abstract",
        "title": "Abstract",
        "level": 6
      },
      {
        "id": "1-introduction",
        "title": "1 Introduction",
        "level": 2
      },
      {
        "id": "2-related-works",
        "title": "2 Related Works",
        "level": 2
      },
      {
        "id": "21-data-generation",
        "title": "2.1 Data Generation",
        "level": 3
      },
      {
        "id": "22-image-captioning",
        "title": "2.2 Image Captioning",
        "level": 3
      },
      {
        "id": "3-methods",
        "title": "3 Methods",
        "level": 2
      },
      {
        "id": "31-geo-image-textualization-data-generation-engine",
        "title": "3.1 Geo-Image-Textualization Data Generation Engine",
        "level": 3
      },
      {
        "id": "311-relation-sampling",
        "title": "3.1.1 Relation Sampling",
        "level": 4
      },
      {
        "id": "312-image-caption-pair-generation",
        "title": "3.1.2 Image-Caption Pair Generation",
        "level": 4
      },
      {
        "id": "32-question-answer-pair-generation",
        "title": "3.2 Question-Answer Pair Generation",
        "level": 3
      },
      {
        "id": "33-rlvr-framework-for-data-refinement",
        "title": "3.3 RLVR Framework for Data Refinement",
        "level": 3
      },
      {
        "id": "331-cold-start-phase",
        "title": "3.3.1 Cold-Start Phase",
        "level": 4
      },
      {
        "id": "332-rlvr-phase",
        "title": "3.3.2 RLVR Phase",
        "level": 4
      },
      {
        "id": "rollout-experience-generation",
        "title": "Rollout Experience Generation",
        "level": 5
      },
      {
        "id": "model-retraining",
        "title": "Model retraining",
        "level": 5
      },
      {
        "id": "333-reward-function",
        "title": "3.3.3 Reward function",
        "level": 4
      },
      {
        "id": "reasoning-reaward",
        "title": "Reasoning reaward",
        "level": 5
      },
      {
        "id": "caption-reward",
        "title": "Caption reward",
        "level": 5
      },
      {
        "id": "4-experiments",
        "title": "4 Experiments",
        "level": 2
      },
      {
        "id": "41-experimental-setup",
        "title": "4.1 Experimental Setup",
        "level": 3
      },
      {
        "id": "42-in-domain-performance-of-georeasoning-10k",
        "title": "4.2 In-Domain Performance of GeoReasoning-10K",
        "level": 3
      },
      {
        "id": "43-out-of-domain-performance-of-georeasoning-10k",
        "title": "4.3 Out-of-Domain Performance of GeoReasoning-10K",
        "level": 3
      },
      {
        "id": "44-ablation-study",
        "title": "4.4 Ablation Study",
        "level": 3
      },
      {
        "id": "5-conclusion",
        "title": "5 Conclusion",
        "level": 2
      },
      {
        "id": "references",
        "title": "References",
        "level": 2
      },
      {
        "id": "appendix-a-question-answer-pair-generation-prompt",
        "title": "Appendix A Question-Answer Pair Generation Prompt",
        "level": 2
      },
      {
        "id": "appendix-b-experimental-setup-and-dataset-information",
        "title": "Appendix B Experimental Setup and Dataset Information",
        "level": 2
      },
      {
        "id": "b1-experimental-setup",
        "title": "B.1 Experimental Setup",
        "level": 3
      },
      {
        "id": "b2-data-source",
        "title": "B.2 Data Source",
        "level": 3
      },
      {
        "id": "b3-license",
        "title": "B.3 License",
        "level": 3
      },
      {
        "id": "appendix-c-case-studies",
        "title": "Appendix C Case Studies",
        "level": 2
      },
      {
        "id": "appendix-d-comparison-to-other-datasets",
        "title": "Appendix D Comparison to Other Datasets",
        "level": 2
      },
      {
        "id": "appendix-e-additional-ablation-studies",
        "title": "Appendix E Additional Ablation Studies",
        "level": 2
      },
      {
        "id": "e1-ablation-study-on-various-domains",
        "title": "E.1 Ablation Study on Various Domains",
        "level": 3
      },
      {
        "id": "e2-ablation-study-on-hyperparameters",
        "title": "E.2 Ablation Study on Hyperparameters",
        "level": 3
      },
      {
        "id": "appendix-f-broader-impacts",
        "title": "Appendix F Broader Impacts",
        "level": 2
      }
    ],
    "has_math": true
  },
  "cached_at": 1758326363.993431
}