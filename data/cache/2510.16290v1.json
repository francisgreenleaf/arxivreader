{
  "success": true,
  "arxiv_id": "2510.16290v1",
  "processed_content": {
    "success": true,
    "arxiv_id": "2510.16290v1",
    "metadata": {
      "arxiv_id": "2510.16290v1",
      "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models",
      "authors": [],
      "abstract": ".\nVideo anomaly detection (VAD) has rapidly advanced by recent development of Vision-Language Models (VLMs). While these models offer superior zero-shot detection capabilities, their immense computational cost and unstable visual grounding performance hinder real-time deployment. To overcome these challenges, we introduce Cerberus, a two-stage cascaded system designed for efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules offline, and combines lightweight filtering with fine-grained VLM reasoning during online inference. The performance gains of Cerberus come from two key innovations: motion mask prompting and rule-based deviation detection. The former directs the VLM\u2019s attention to regions relevant to motion, while the latter identifies anomalies as deviations from learned norms rather than enumerating possible anomalies. Extensive evaluations on four datasets show that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a 151.79\u00d7\\times speedup, and 97.2% accuracy comparable to the state-of-the-art VLM-based VAD methods, establishing it as a practical solution for real-time video analytics.",
      "date": "(2025)"
    },
    "content": "<div class=\"arxiv-content\">\n<div class=\"ltx_page_content\">\n<article class=\"ltx_document ltx_authors_1line ltx_leqno\">\n<div class=\"ltx_para\" id=\"p1\">\n<span class=\"ltx_ERROR undefined\">\\setcctype</span>\n<p class=\"ltx_p\">by</p>\n</div>\n<h1 class=\"ltx_title ltx_title_document\" id=\"cerberus-real-time-video-anomaly-detection-via-cascaded-vision-language-models\">Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">\nYue Zheng<sup class=\"ltx_sup\">1</sup>,\nXiufang Shi<sup class=\"ltx_sup\">1</sup>,\nJiming Chen<sup class=\"ltx_sup\">2, 3</sup>,\nYuanchao Shu<sup class=\"ltx_sup\">2,\u2020</sup>\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\"><span class=\"ltx_text ltx_affiliation_institution\"><sup class=\"ltx_sup\">1</sup>Zhejiang University of Technology, <sup class=\"ltx_sup\">2</sup>Zhejiang University, <sup class=\"ltx_sup\">3</sup>Hangzhou Dianzi University</span><span class=\"ltx_text ltx_affiliation_country\"></span>\n</span></span></span>\n</div>\n<div class=\"ltx_dates\">(2025)</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\" id=\"abstract\">Abstract.</h6>\n\n</div>\n<div class=\"ltx_acknowledgements\">\u2020\u00a0Corresponding author: Yuanchao Shu (ycshu@zju.edu.cn).\n</div>\n<span class=\"ltx_note ltx_note_frontmatter ltx_role_journalyear\"><sup class=\"ltx_note_mark\">\u2020</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">\u2020</sup><span class=\"ltx_note_type\">journalyear: </span>2025</span></span></span><span class=\"ltx_note ltx_note_frontmatter ltx_role_copyright\"><sup class=\"ltx_note_mark\">\u2020</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">\u2020</sup><span class=\"ltx_note_type\">copyright: </span>cc</span></span></span>\n<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"1-introduction\">\n<span class=\"ltx_tag ltx_tag_section\">1. </span>Introduction</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n\n</div>\n</li>\n</ul>\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"210\" id=\"S1.F1.g1\" src=\"https://arxiv.org/html/x1.png\" width=\"813\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 1</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Two common VLM-based VAD pipelines. Top: a modular two-step design where a VLM describes video content and an LLM reasons\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zanella et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib61\" title=\"\">2024</a>; Ye et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib58\" title=\"\">2025</a>; Yang et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib56\" title=\"\">2024a</a>)</cite>. Bottom: an integrated single-step design where a full-fledged VLM like Gemini\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Team et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib40\" title=\"\">2023</a>)</cite> and GPT-4o\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Achiam et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib3\" title=\"\">2023</a>)</cite> handles both perception and reasoning\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Zhao et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib65\" title=\"\">2025</a>; Li et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib21\" title=\"\">2024</a>)</cite>.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p3\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p6\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p7\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p8\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p9\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p10\">\n<ul class=\"ltx_itemize\" id=\"S1.I2\">\n<li class=\"ltx_item\" id=\"S1.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I2.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I2.i2.p1\">\n<p class=\"ltx_p\">We design a cascaded architecture that combines lightweight CLIP-based filtering with VLM-based reasoning, greatly improving efficiency without sacrificing accuracy.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I2.i3.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I2.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I2.i4.p1\">\n\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"2-background-and-motivation\">\n<span class=\"ltx_tag ltx_tag_section\">2. </span>Background and Motivation</h2>\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"21-vision-language-models\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.1. </span>Vision-Language Models</h3>\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S2.T1\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:260.2pt;height:76.7pt;vertical-align:-35.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(26.3pt,-7.7pt) scale(1.25326697677619,1.25326697677619) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Type</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">AUC (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">GODS</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">I3D-RGB</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">61.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">RareAnom</td>\n<td class=\"ltx_td ltx_align_center\">I3D-RGB</td>\n<td class=\"ltx_td ltx_align_center\">68.33</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">PE-Core-L14-336</td>\n<td class=\"ltx_td ltx_align_center\">CLIP</td>\n<td class=\"ltx_td ltx_align_center\">64.31</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Qwen2.5-VL-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">VLM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">82.51</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 1</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Comparison of detection accuracy on a subset of <span class=\"ltx_text ltx_font_typewriter\">XD-Violence</span> dataset across different methods</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S2.F2\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S2.F2.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_portrait\" height=\"335\" id=\"S2.F2.sf1.g1\" src=\"https://arxiv.org/html/x2.png\" width=\"252\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S2.F2.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_square\" height=\"338\" id=\"S2.F2.sf2.g1\" src=\"https://arxiv.org/html/x3.png\" width=\"331\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 2</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Architectures of CLIP and a typical VLM.</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p3\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S2.T2\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:303.5pt;height:79.5pt;vertical-align:-36.5pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(34.9pt,-9.1pt) scale(1.29826047485737,1.29826047485737) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Time (s)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Memory (GB)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">YOLOv10-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.86</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Kinetics-I3D</td>\n<td class=\"ltx_td ltx_align_center\">0.38</td>\n<td class=\"ltx_td ltx_align_center\">1.18</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">PE-Core-L14-336</td>\n<td class=\"ltx_td ltx_align_center\">0.84</td>\n<td class=\"ltx_td ltx_align_center\">3.19</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Qwen2.5-VL-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">8.48</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">17.85</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 2</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Computational overhead comparison on a NVIDIA L40S GPU for processing 10 frames.</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p4\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F3\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S2.F3.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"186\" id=\"S2.F3.sf1.g1\" src=\"https://arxiv.org/html/x4.png\" width=\"332\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S2.F3.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"186\" id=\"S2.F3.sf2.g1\" src=\"https://arxiv.org/html/x5.png\" width=\"332\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 3</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">An example of attentional distraction: the VLM focuses on the salient foreground objects, thereby missing crucial contextual cues like the traffic sign and the distant violating vehicle.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"22-opportunities-in-anomaly-detection\">\n<span class=\"ltx_tag ltx_tag_subsection\">2.2. </span>Opportunities in Anomaly Detection</h3>\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\n<p class=\"ltx_p\">We ground our motivation in three key opportunities of real-world VAD tasks.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS2.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"259\" id=\"S2.F4.g1\" src=\"https://arxiv.org/html/Figures/fig_sec2/recall_proportion.png\" width=\"449\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 4</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Trade-off between anomaly recall and frame filtering proportion under different Top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"S2.F4.m2\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math>.</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS2.p3\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S2.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:303.5pt;height:42.9pt;vertical-align:-18.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(20.2pt,-2.9pt) scale(1.15386590252402,1.15386590252402) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Precision (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Recall (%)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">AUC (%)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">SHTech</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">91.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">27.13</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.93</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Campus</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">68.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">21.81</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">69.23</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 3</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Detection performance of anomaly-matching with anomalies enumerated by Deepseek-R1\u00a0<cite class=\"ltx_cite ltx_citemacro_citep\">(Guo et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2510.16290v1#bib.bib15\" title=\"\">2025</a>)</cite>.</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS2.p4\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"3-system-overview\">\n<span class=\"ltx_tag ltx_tag_section\">3. </span>System Overview</h2>\n<figure class=\"ltx_figure\" id=\"S3.F5\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"379\" id=\"S3.F5.g1\" src=\"https://arxiv.org/html/x6.png\" width=\"665\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 5</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">The system overview of <span class=\"ltx_text ltx_font_typewriter\">Cerberus</span>. </span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p2\">\n\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"S9.EGx1\">\n<tbody id=\"S3.Ex1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\max_{\\rho}\\quad\" class=\"ltx_Math\" display=\"inline\" id=\"S3.Ex1.m1\" intent=\":literal\"><semantics><mrow><munder><mi>max</mi><mi>\u03c1</mi></munder><mspace style=\"width:1.167em;\" width=\"1.167em\"></mspace></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\max_{\\rho}\\quad</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Throughput}_{\\text{Cerberus{}}}=\\frac{1}{\\bar{T}_{M_{\\text{C}}}+\\rho\\cdot\\bar{T}_{M_{\\text{F}}}}\" class=\"ltx_Math\" display=\"inline\" id=\"S3.Ex1.m2\" intent=\":literal\"><semantics><mrow><msub><mtext>Throughput</mtext><mtext>Cerberus</mtext></msub><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mn>1</mn><mrow><msub><mover accent=\"true\"><mi>T</mi><mo>\u00af</mo></mover><msub><mi>M</mi><mtext>C</mtext></msub></msub><mo>+</mo><mrow><mi>\u03c1</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><msub><mover accent=\"true\"><mi>T</mi><mo>\u00af</mo></mover><msub><mi>M</mi><mtext>F</mtext></msub></msub></mrow></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Throughput}_{\\text{Cerberus{}}}=\\frac{1}{\\bar{T}_{M_{\\text{C}}}+\\rho\\cdot\\bar{T}_{M_{\\text{F}}}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"S3.Ex2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\">s.t.</span></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{Recall}(M_{\\text{C}})\\geq\\theta,\" class=\"ltx_Math\" display=\"inline\" id=\"S3.Ex2.m2\" intent=\":literal\"><semantics><mrow><mrow><mrow><mtext>Recall</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>M</mi><mtext>C</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mi>\u03b8</mi></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{Recall}(M_{\\text{C}})\\geq\\theta,</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"S3.Ex3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_eqn_cell\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{AUC}(\\text{Cerberus{}})\\geq\\text{AUC}(\\text{Baseline})-\\epsilon.\" class=\"ltx_Math\" display=\"inline\" id=\"S3.Ex3.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mtext>AUC</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Cerberus</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2265</mo><mrow><mrow><mtext>AUC</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mtext>Baseline</mtext><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2212</mo><mi>\u03f5</mi></mrow></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{AUC}(\\text{Cerberus{}})\\geq\\text{AUC}(\\text{Baseline})-\\epsilon.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S3.p3\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p4\">\n\n</div>\n<div class=\"ltx_para\" id=\"S3.p5\">\n\n</div>\n<div class=\"ltx_para\" id=\"S3.p6\">\n\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"4-offline-induction\">\n<span class=\"ltx_tag ltx_tag_section\">4. </span>Offline Induction</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"41-primary-rule-generation\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.1. </span>Primary Rule Generation</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p2\">\n\n<table class=\"ltx_equation ltx_eqn_table table table-responsive\" id=\"S4.E1\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_left\">(1)</span></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"D_{\\text{normal}}=\\{\\text{VLM}(s_{\\text{normal}_{i}},p_{\\text{desc}})|s_{\\text{normal}_{i}}\\in S_{\\text{normal}}\\}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E1.m1\" intent=\":literal\"><semantics><mrow><msub><mi>D</mi><mtext>normal</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>VLM</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>s</mi><msub><mtext>normal</mtext><mi>i</mi></msub></msub><mo>,</mo><msub><mi>p</mi><mtext>desc</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo lspace=\"0em\" rspace=\"0em\">|</mo><mrow><msub><mi>s</mi><msub><mtext>normal</mtext><mi>i</mi></msub></msub><mo>\u2208</mo><msub><mi>S</mi><mtext>normal</mtext></msub></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">D_{\\text{normal}}=\\{\\text{VLM}(s_{\\text{normal}_{i}},p_{\\text{desc}})|s_{\\text{normal}_{i}}\\in S_{\\text{normal}}\\}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p3\">\n\n<table class=\"ltx_equation ltx_eqn_table table table-responsive\" id=\"S4.E2\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_left\">(2)</span></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"R_{\\text{normal}}=\\{\\text{LLM}(D_{\\text{normal}},p_{\\text{rule}})\\}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E2.m1\" intent=\":literal\"><semantics><mrow><msub><mi>R</mi><mtext>normal</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mrow><mtext>LLM</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mtext>normal</mtext></msub><mo>,</mo><msub><mi>p</mi><mtext>rule</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">}</mo></mrow></mrow><annotation encoding=\"application/x-tex\">R_{\\text{normal}}=\\{\\text{LLM}(D_{\\text{normal}},p_{\\text{rule}})\\}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p4\">\n\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p5\">\n\n<table class=\"ltx_equation ltx_eqn_table table table-responsive\" id=\"S4.E3\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_left\">(3)</span></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"P_{\\text{candidate}}=P_{\\text{perturbed}}\\bigcup P_{\\text{normal}}\" class=\"ltx_Math\" display=\"block\" id=\"S4.E3.m1\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>candidate</mtext></msub><mo>=</mo><mrow><msub><mi>P</mi><mtext>perturbed</mtext></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo movablelimits=\"false\">\u22c3</mo><msub><mi>P</mi><mtext>normal</mtext></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">P_{\\text{candidate}}=P_{\\text{perturbed}}\\bigcup P_{\\text{normal}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n<p class=\"ltx_p\">where:</p>\n<table class=\"ltx_equationgroup ltx_centering ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"S9.EGx2\">\n<tbody id=\"S4.Ex4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext='\\displaystyle P_{\\text{perturbed}}=\\{``\\text{The scene depicts }\\{l\\}.\"\\mid l\\in L_{\\text{perturbed}}\\}.' class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.Ex4.m1\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>perturbed</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi mathvariant=\"normal\">\u2018</mi><mi mathvariant=\"normal\">\u2018</mi><mtext>The scene depicts\u00a0</mtext><mrow><mo stretchy=\"false\">{</mo><mi>l</mi><mo stretchy=\"false\">}</mo></mrow><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mi mathvariant=\"normal\">\u201d</mi><mo lspace=\"0em\" rspace=\"0.167em\">\u2223</mo><mi>l</mi><mo>\u2208</mo><msub><mi>L</mi><mtext>perturbed</mtext></msub><mo stretchy=\"false\">}</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle P_{\\text{perturbed}}=\\{``\\text{The scene depicts }\\{l\\}.\"\\mid l\\in L_{\\text{perturbed}}\\}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n<tbody id=\"S4.Ex5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext='\\displaystyle P_{\\text{normal}}=\\{``\\text{The normal scene depicts }\\{r\\}.\"\\mid r\\in R_{\\text{normal}}\\}.' class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.Ex5.m1\" intent=\":literal\"><semantics><mrow><msub><mi>P</mi><mtext>normal</mtext></msub><mo>=</mo><mrow><mo stretchy=\"false\">{</mo><mi mathvariant=\"normal\">\u2018</mi><mi mathvariant=\"normal\">\u2018</mi><mtext>The normal scene depicts\u00a0</mtext><mrow><mo stretchy=\"false\">{</mo><mi>r</mi><mo stretchy=\"false\">}</mo></mrow><mo lspace=\"0em\" rspace=\"0.167em\">.</mo><mi mathvariant=\"normal\">\u201d</mi><mo lspace=\"0em\" rspace=\"0.167em\">\u2223</mo><mi>r</mi><mo>\u2208</mo><msub><mi>R</mi><mtext>normal</mtext></msub><mo stretchy=\"false\">}</mo></mrow><mo lspace=\"0em\">.</mo></mrow><annotation encoding=\"application/x-tex\">\\displaystyle P_{\\text{normal}}=\\{``\\text{The normal scene depicts }\\{r\\}.\"\\mid r\\in R_{\\text{normal}}\\}.</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p6\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"42-auxiliary-anomalies-customization\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.2. </span>Auxiliary Anomalies Customization</h3>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"43-rule-evolution\">\n<span class=\"ltx_tag ltx_tag_subsection\">4.3. </span>Rule Evolution</h3>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS3.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS3.p3\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"5-online-inference\">\n<span class=\"ltx_tag ltx_tag_section\">5. </span>Online Inference</h2>\n<div class=\"ltx_para\" id=\"S5.p1\">\n\n</div>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"51-motion-mask-prompting\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.1. </span>Motion Mask Prompting</h3>\n<div class=\"ltx_para\" id=\"S5.SS1.p1\">\n\n<table class=\"ltx_equation ltx_eqn_table table table-responsive\" id=\"S5.E4\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_left\">(4)</span></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"p(D_{t})=\\frac{\\sum_{i=1}^{W}\\sum_{j=1}^{H}\\lvert F_{t}(i,j)-F_{t-1}(i,j)\\rvert}{W\\times H},\" class=\"ltx_Math\" display=\"block\" id=\"S5.E4.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>p</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>D</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><msubsup><mo>\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>W</mi></msubsup><mrow><msubsup><mo lspace=\"0.167em\" rspace=\"0em\">\u2211</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>H</mi></msubsup><mrow><mo stretchy=\"false\">|</mo><mrow><mrow><msub><mi>F</mi><mi>t</mi></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>\u2212</mo><mrow><msub><mi>F</mi><mrow><mi>t</mi><mo>\u2212</mo><mn>1</mn></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo>,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><mo stretchy=\"false\">|</mo></mrow></mrow></mrow><mrow><mi>W</mi><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mi>H</mi></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">p(D_{t})=\\frac{\\sum_{i=1}^{W}\\sum_{j=1}^{H}\\lvert F_{t}(i,j)-F_{t-1}(i,j)\\rvert}{W\\times H},</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F6\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S5.F6.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"209\" id=\"S5.F6.sf1.g1\" src=\"https://arxiv.org/html/x7.png\" width=\"365\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S5.F6.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"208\" id=\"S5.F6.sf2.g1\" src=\"https://arxiv.org/html/x8.png\" width=\"365\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S5.F6.sf3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"209\" id=\"S5.F6.sf3.g1\" src=\"https://arxiv.org/html/x9.png\" width=\"366\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(c)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S5.F6.sf4\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"208\" id=\"S5.F6.sf4.g1\" src=\"https://arxiv.org/html/x10.png\" width=\"365\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(d)</span> </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 6</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">The generation process of <span class=\"ltx_text ltx_font_italic\">motion mask prompting</span>. A motion mask (b) is derived from the original frame (a) to highlight moving subjects, which are then overlaid with red circles (c) or squares(d) prompts.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS1.p3\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p4\">\n<p class=\"ltx_p\">Together, these complementary cues balance sensitivity and efficiency: circles ensure high recall by capturing subtle signals, while squares improve precision by suppressing background noise.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"52-rule-based-deviation-detection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.2. </span>Rule-based Deviation Detection</h3>\n<div class=\"ltx_para\" id=\"S5.SS2.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S5.F7\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"482\" id=\"S5.F7.g1\" src=\"https://arxiv.org/html/x11.png\" width=\"814\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 7</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Example of rule-based deviation detection for VAD. The score is the difference between normal rules and perturbed labels, compared against the threshold.</span></figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS2.p3\">\n<table class=\"ltx_equation ltx_eqn_table table table-responsive\" id=\"S5.E5\">\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left\" rowspan=\"1\"><span class=\"ltx_tag ltx_tag_equation ltx_align_left\">(5)</span></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math alttext=\"S(s)=\\sum_{t\\in C_{\\text{top-}k}(s)}w_{t}\\cdot\\text{sim}(v_{s},v_{t}),\" class=\"ltx_Math\" display=\"block\" id=\"S5.E5.m1\" intent=\":literal\"><semantics><mrow><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.111em\">=</mo><mrow><munder><mo movablelimits=\"false\">\u2211</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><msub><mi>C</mi><mrow><mtext>top-</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>k</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></munder><mrow><mrow><msub><mi>w</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><mtext>sim</mtext></mrow><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><msub><mi>v</mi><mi>s</mi></msub><mo>,</mo><msub><mi>v</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding=\"application/x-tex\">S(s)=\\sum_{t\\in C_{\\text{top-}k}(s)}w_{t}\\cdot\\text{sim}(v_{s},v_{t}),</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.p4\">\n\n</div>\n<figure class=\"ltx_float ltx_algorithm\" id=\"alg1\">\n<div class=\"ltx_listing ltx_lst_numbers_left ltx_listing\">\n<div class=\"ltx_listingline\">\n<div class=\"ltx_inline-block ltx_transformed_outer\" style=\"width:390.3pt;height:733.8pt;vertical-align:-364.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-25.0pt,47.0pt) scale(0.886366504975667,0.886366504975667) ;\">\n<div class=\"ltx_block ltx_minipage ltx_align_middle\" style=\"width:433.6pt;\">\n<p class=\"ltx_p\"><span class=\"ltx_text ltx_font_bold\">Input:</span></p>\n\n</div>\n</span></div>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_text ltx_font_bold\">Output:</span> Detection results (<span class=\"ltx_text ltx_font_italic\">normal</span> / <span class=\"ltx_text ltx_font_italic\">abnormal</span>);\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">1</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">2</span><math alttext=\"v_{s}\\leftarrow\\text{CLIP.encode\\_image}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m6\" intent=\":literal\"><semantics><mrow><msub><mi>v</mi><mi>s</mi></msub><mo stretchy=\"false\">\u2190</mo><mrow><mtext>CLIP.encode_image</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v_{s}\\leftarrow\\text{CLIP.encode\\_image}(s)</annotation></semantics></math>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">3</span><span class=\"ltx_text ltx_font_bold\">for</span> <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"t\\in P_{\\text{candidate}}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m7\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>\u2208</mo><msub><mi>P</mi><mtext class=\"ltx_mathvariant_italic\"><em class=\"ltx_emph\" style=\"font-size:70%;\">candidate</em></mtext></msub></mrow><annotation encoding=\"application/x-tex\">t\\in P_{\\text{candidate}}</annotation></semantics></math></em> <span class=\"ltx_text ltx_font_bold\">do</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">4</span>\u2002<span class=\"ltx_rule\" style=\"width:1px;height:100%;--ltx-bg-color:black;display:inline-block;\">\u00a0</span>\u2003\n<math alttext=\"v_{t}\\leftarrow\\text{CLIP.encode\\_text}(t)\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m8\" intent=\":literal\"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub><mo stretchy=\"false\">\u2190</mo><mrow><mtext>CLIP.encode_text</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">v_{t}\\leftarrow\\text{CLIP.encode\\_text}(t)</annotation></semantics></math>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">5</span>\u2002<span class=\"ltx_rule\" style=\"width:1px;height:100%;--ltx-bg-color:black;display:inline-block;\">\u00a0</span>\u2003<math alttext=\"\\text{sim}(t)\\leftarrow\\cos\\big(v_{s},v_{t}\\big)\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m9\" intent=\":literal\"><semantics><mrow><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>t</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo stretchy=\"false\">\u2190</mo><mrow><mi>cos</mi><mo>\u2061</mo><mrow><mo maxsize=\"1.200em\" minsize=\"1.200em\">(</mo><msub><mi>v</mi><mi>s</mi></msub><mo>,</mo><msub><mi>v</mi><mi>t</mi></msub><mo maxsize=\"1.200em\" minsize=\"1.200em\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(t)\\leftarrow\\cos\\big(v_{s},v_{t}\\big)</annotation></semantics></math>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">6</span> end for\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">7</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">8</span>Select top-<math alttext=\"k\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m10\" intent=\":literal\"><semantics><mi>k</mi><annotation encoding=\"application/x-tex\">k</annotation></semantics></math> candidates <math alttext=\"C_{\\text{top-}k}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m11\" intent=\":literal\"><semantics><mrow><msub><mi>C</mi><mrow><mtext>top-</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>k</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">C_{\\text{top-}k}(s)</annotation></semantics></math> ranked by <math alttext=\"\\text{sim}(\\cdot)\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m12\" intent=\":literal\"><semantics><mrow><mtext>sim</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mo lspace=\"0em\" rspace=\"0em\">\u22c5</mo><mo stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{sim}(\\cdot)</annotation></semantics></math>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">9</span><span class=\"ltx_text ltx_font_bold\">for</span> <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"t\\in C_{\\textnormal{top-}k}(s)\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m13\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>\u2208</mo><mrow><msub><mi>C</mi><mrow><mtext><em class=\"ltx_emph ltx_font_upright\" style=\"font-size:70%;\">top-</em></mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>k</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">t\\in C_{\\textnormal{top-}k}(s)</annotation></semantics></math></em> <span class=\"ltx_text ltx_font_bold\">do</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">10</span>\u2002<span class=\"ltx_rule\" style=\"width:1px;height:100%;--ltx-bg-color:black;display:inline-block;\">\u00a0</span>\u2003\n<math alttext=\"w_{t}\\leftarrow+1\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m14\" intent=\":literal\"><semantics><mrow><msub><mi>w</mi><mi>t</mi></msub><mo stretchy=\"false\">\u2190</mo><mrow><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding=\"application/x-tex\">w_{t}\\leftarrow+1</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">if</span> <math alttext=\"t\\in P_{\\text{normal}}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m15\" intent=\":literal\"><semantics><mrow><mi>t</mi><mo>\u2208</mo><msub><mi>P</mi><mtext>normal</mtext></msub></mrow><annotation encoding=\"application/x-tex\">t\\in P_{\\text{normal}}</annotation></semantics></math> <span class=\"ltx_text ltx_font_bold\">else</span> <math alttext=\"-1\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m16\" intent=\":literal\"><semantics><mrow><mo>\u2212</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">11</span> end for\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">12</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">13</span><math alttext=\"S(s)\\leftarrow\\sum_{t\\in C_{\\text{top-}k}(s)}w_{t}\\cdot p_{t}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m17\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo rspace=\"0.111em\" stretchy=\"false\">\u2190</mo><mrow><msub><mo>\u2211</mo><mrow><mi>t</mi><mo>\u2208</mo><mrow><msub><mi>C</mi><mrow><mtext>top-</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>k</mi></mrow></msub><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow></mrow></msub><mrow><msub><mi>w</mi><mi>t</mi></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u22c5</mo><msub><mi>p</mi><mi>t</mi></msub></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">S(s)\\leftarrow\\sum_{t\\in C_{\\text{top-}k}(s)}w_{t}\\cdot p_{t}</annotation></semantics></math>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">14</span><span class=\"ltx_text ltx_font_bold\">if</span> <em class=\"ltx_emph ltx_font_italic\"><math alttext=\"S(s)&lt;\\tau\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.m18\" intent=\":literal\"><semantics><mrow><mrow><mi>S</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mi>s</mi><mo stretchy=\"false\">)</mo></mrow></mrow><mo>&lt;</mo><mi>\u03c4</mi></mrow><annotation encoding=\"application/x-tex\">S(s)&lt;\\tau</annotation></semantics></math></em> <span class=\"ltx_text ltx_font_bold\">then</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">15</span>\u2002<span class=\"ltx_rule\" style=\"width:1px;height:100%;--ltx-bg-color:black;display:inline-block;\">\u00a0</span>\u2003\n<span class=\"ltx_text ltx_font_bold\">return</span> <span class=\"ltx_text ltx_font_italic\">abnormal</span>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">16</span><span class=\"ltx_text ltx_font_bold\">else</span>\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">17</span>\u2002<span class=\"ltx_rule\" style=\"width:1px;height:100%;--ltx-bg-color:black;display:inline-block;\">\u00a0</span>\u2003\n<span class=\"ltx_text ltx_font_bold\">return</span> <span class=\"ltx_text ltx_font_italic\">normal</span>;\n\n</div>\n<div class=\"ltx_listingline\">\n<span class=\"ltx_tag ltx_tag_listingline\">18</span> end if\n</div>\n<div class=\"ltx_listingline\">\n</div>\n</div>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm\u00a01</span> </span>Deviation-driven anomaly detection with CLIP</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"53-fine-grained-captioning-and-detection\">\n<span class=\"ltx_tag ltx_tag_subsection\">5.3. </span>Fine-grained Captioning and Detection</h3>\n<div class=\"ltx_para\" id=\"S5.SS3.p1\">\n<p class=\"ltx_p\">While the coarse-grained filtering achieves a high recall (over 95%) for suspicious frames, it sacrifices precision by retaining many normal frames. Relying solely on this set for final decisions would generate excessive false alarms.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.p2\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.p3\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.p4\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS3.p5\">\n<p class=\"ltx_p\">The overall cascaded architecture balances recall and precision: coarse-grained filtering retains all potential anomalies while removing redundant frames, and fine-grained analysis provides interpretable reasoning for suspicious events, ensuring efficient and reliable online inference.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"6-evaluation\">\n<span class=\"ltx_tag ltx_tag_section\">6. </span>Evaluation</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n\n</div>\n<section class=\"ltx_subsection\" id=\"S6.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"61-experimental-setup\">\n<span class=\"ltx_tag ltx_tag_subsection\">6.1. </span>Experimental Setup</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS1.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S6.T4\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:303.5pt;height:74.5pt;vertical-align:-34.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(2.7pt,-0.7pt) scale(1.01818093501425,1.01818093501425) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\"># Testing</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Frames</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\"># Anomaly</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Classes</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Anomaly</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Ratio</span></td>\n</tr>\n</table></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">Avenue</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">15,324</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">25.23%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">SHTech</td>\n<td class=\"ltx_td ltx_align_center\">42,883</td>\n<td class=\"ltx_td ltx_align_center\">11</td>\n<td class=\"ltx_td ltx_align_center\">42.47%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">UBnormal</td>\n<td class=\"ltx_td ltx_align_center\">92,640</td>\n<td class=\"ltx_td ltx_align_center\">22</td>\n<td class=\"ltx_td ltx_align_center\">74.53%</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">Campus</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">384,059</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">16.63%</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 4</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Description of the VAD test datasets used.</span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS1.p2\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S6.T5\">\n<div class=\"ltx_flex_figure ltx_flex_table\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:69pt;vertical-align:-33.1pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-166.7pt,26.5pt) scale(0.565293617706926,0.565293617706926) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">Methods</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Avenue</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SHTech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">UBnormal</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Campus</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Avg.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Relative</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AUC</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(fps)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Relative</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AUC</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(fps)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Relative</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AUC</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(fps)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Relative</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AUC</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(fps)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Relative</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">AUC</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">(fps)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AnomlayRuler <span class=\"ltx_text ltx_font_bold\">(Mean)<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium\">1</span></sup></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.46</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">100%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.38</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">AnomlayRuler-base <span class=\"ltx_text ltx_font_bold\">(Mean)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">91.63%</td>\n<td class=\"ltx_td ltx_align_center\">0.82</td>\n<td class=\"ltx_td ltx_align_center\">92.31%</td>\n<td class=\"ltx_td ltx_align_center\">0.76</td>\n<td class=\"ltx_td ltx_align_center\">93.07%</td>\n<td class=\"ltx_td ltx_align_center\">0.63</td>\n<td class=\"ltx_td ltx_align_center\">88.43%</td>\n<td class=\"ltx_td ltx_align_center\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">91.36%</td>\n<td class=\"ltx_td ltx_align_center\">0.72</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">VLM with Rules <span class=\"ltx_text ltx_font_bold\">(Mean)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">88.64%</td>\n<td class=\"ltx_td ltx_align_center\">3.08</td>\n<td class=\"ltx_td ltx_align_center\">90.44%</td>\n<td class=\"ltx_td ltx_align_center\">3.39</td>\n<td class=\"ltx_td ltx_align_center\">82.38%</td>\n<td class=\"ltx_td ltx_align_center\">2.58</td>\n<td class=\"ltx_td ltx_align_center\">86.13%</td>\n<td class=\"ltx_td ltx_align_center\">3.55</td>\n<td class=\"ltx_td ltx_align_center\">87.15%</td>\n<td class=\"ltx_td ltx_align_center\">3.15</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">CLIP with Rules <span class=\"ltx_text ltx_font_bold\">(Mean)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">71.32%</td>\n<td class=\"ltx_td ltx_align_center\">118.32</td>\n<td class=\"ltx_td ltx_align_center\">77.45%</td>\n<td class=\"ltx_td ltx_align_center\">112.61</td>\n<td class=\"ltx_td ltx_align_center\">73.29%</td>\n<td class=\"ltx_td ltx_align_center\">87.82</td>\n<td class=\"ltx_td ltx_align_center\">68.84%</td>\n<td class=\"ltx_td ltx_align_center\">90.06</td>\n<td class=\"ltx_td ltx_align_center\">72.73%</td>\n<td class=\"ltx_td ltx_align_center\">102.29</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">Cerberus <span class=\"ltx_text ltx_font_bold\">(Orig.)</span>\u00a0<sup class=\"ltx_sup\">2</sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">96.87%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">98.13%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">96.84%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">3.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">97.13%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">6.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">97.24%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">4.74</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">Cerberus <span class=\"ltx_text ltx_font_bold\">(5%)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\">97.15%</td>\n<td class=\"ltx_td ltx_align_center\">17.24</td>\n<td class=\"ltx_td ltx_align_center\">98.11%</td>\n<td class=\"ltx_td ltx_align_center\">32.49</td>\n<td class=\"ltx_td ltx_align_center\">97.23%</td>\n<td class=\"ltx_td ltx_align_center\">28.09</td>\n<td class=\"ltx_td ltx_align_center\">96.84%</td>\n<td class=\"ltx_td ltx_align_center\">17.79</td>\n<td class=\"ltx_td ltx_align_center\">97.33%</td>\n<td class=\"ltx_td ltx_align_center\">23.96</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">Cerberus <span class=\"ltx_text ltx_font_bold\">(1%)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">96.82%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">53.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">97.66%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">72.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">97.15%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">57.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">97.21%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">45.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">97.21%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">57.68</td>\n</tr>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<ul class=\"ltx_itemize ltx_centering ltx_figure_panel\" id=\"S6.I1\">\n<li class=\"ltx_item\" id=\"S6.I1.ix1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1</span>\n<div class=\"ltx_para\" id=\"S6.I1.ix1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S6.I1.ix2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">2</span>\n<div class=\"ltx_para\" id=\"S6.I1.ix2.p1\">\n\n</div>\n</li>\n</ul>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 5</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Comparison of relative AUC and throughput for different VAD methods.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S6.F8\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"210\" id=\"S6.F8.g1\" src=\"https://arxiv.org/html/x12.png\" width=\"814\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 8</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Illustration of normalized throughput vs. relative AUC on different datasets. </span></figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS1.p3\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS1.p4\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"62-overall-performance\">\n<span class=\"ltx_tag ltx_tag_subsection\">6.2. </span>Overall Performance</h3>\n<div class=\"ltx_para\" id=\"S6.SS2.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS2.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S6.SS2.p3\">\n\n</div>\n<div class=\"ltx_para\" id=\"S6.SS2.p4\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S6.T6\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:390.3pt;height:64.4pt;vertical-align:-29.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(9.3pt,-1.5pt) scale(1.0503223683681,1.0503223683681) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\"># Rules</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Precision</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Recall</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">AUC</span></td>\n</tr>\n</table></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">Cerberus</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">10.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">89.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">48.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">82.73</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w/o. Action Captioning</td>\n<td class=\"ltx_td ltx_align_center\">-2.34</td>\n<td class=\"ltx_td ltx_align_center\">-35.71</td>\n<td class=\"ltx_td ltx_align_center\">-11.23</td>\n<td class=\"ltx_td ltx_align_center\">-18.56</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\">w/o. Context Captioning</td>\n<td class=\"ltx_td ltx_align_center\">-1.00</td>\n<td class=\"ltx_td ltx_align_center\">-5.27</td>\n<td class=\"ltx_td ltx_align_center\">-2.45</td>\n<td class=\"ltx_td ltx_align_center\">-6.13</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\">w/o. Rule Generalization</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">+2.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-8.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-14.29</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">-14.92</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 6</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation study of rule generation components: action captioning, context captioning, and rule generalization on <span class=\"ltx_text ltx_font_typewriter\">SHTech</span> dataset.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"63-evaluation-of-offline-induction\">\n<span class=\"ltx_tag ltx_tag_subsection\">6.3. </span>Evaluation of Offline Induction</h3>\n<div class=\"ltx_para\" id=\"S6.SS3.p1\">\n\n</div>\n<section class=\"ltx_subsubsection\" id=\"S6.SS3.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"631-effect-of-rule-generation\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">6.3.1. </span>Effect of Rule Generation</h4>\n<div class=\"ltx_para\" id=\"S6.SS3.SSS1.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S6.SS3.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"632-impact-of-anomaly-customization\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">6.3.2. </span>Impact of Anomaly Customization</h4>\n<div class=\"ltx_para\" id=\"S6.SS3.SSS2.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S6.T7\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:433.6pt;height:51.2pt;vertical-align:-23.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(5.6pt,-0.7pt) scale(1.02645163536761,1.02645163536761) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Avenue</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\">SHTech</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AUC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput (fps)</td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AUC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\">Throughput (fps)</td>\n</tr>\n</table></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Cerberus (Base)</td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">86.40</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.76</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">82.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.90</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Cerberus (<span class=\"ltx_text ltx_font_bold\">Customized</span>)</td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">+2.28</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">+0.22</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">+1.82</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">+0.31</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 7</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Impact of <span class=\"ltx_text ltx_font_italic\">auxiliary anomaly customization</span>, comparing <span class=\"ltx_text ltx_font_typewriter\">Cerberus</span> with and without customized anomalies on <span class=\"ltx_text ltx_font_typewriter\">Avenue</span> and <span class=\"ltx_text ltx_font_typewriter\">SHTech</span> datasets.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S6.F9\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"40\" id=\"S6.F9.g1\" src=\"https://arxiv.org/html/x13.png\" width=\"747\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S6.F9.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"214\" id=\"S6.F9.sf1.g1\" src=\"https://arxiv.org/html/x14.png\" width=\"382\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S6.F9.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"219\" id=\"S6.F9.sf2.g1\" src=\"https://arxiv.org/html/x15.png\" width=\"381\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 9</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Impact of <span class=\"ltx_text ltx_font_italic\">rule evolution</span> feedback mechanisms on AUC and throughput, showing individual and combined effects on <span class=\"ltx_text ltx_font_typewriter\">Avenue</span> and <span class=\"ltx_text ltx_font_typewriter\">SHTech</span> datasets.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S6.SS3.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"633-role-of-rule-evolution\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">6.3.3. </span>Role of Rule Evolution</h4>\n<div class=\"ltx_para\" id=\"S6.SS3.SSS3.p1\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"S6.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"64-evaluation-of-online-inference\">\n<span class=\"ltx_tag ltx_tag_subsection\">6.4. </span>Evaluation of Online Inference</h3>\n<div class=\"ltx_para\" id=\"S6.SS4.p1\">\n<p class=\"ltx_p\">Next, we evaluate each module in the online inference stage.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S6.T8\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:346.9pt;height:125.1pt;vertical-align:-60.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(3.7pt,-1.3pt) scale(1.02182639902621,1.02182639902621) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Anomaly Prop.</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">AUC</span></td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">Overhead (h)</span></td>\n</tr>\n</table></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Coarse-grained</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">filtering only</td>\n</tr>\n</table> </td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Orig.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.01</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">5%</td>\n<td class=\"ltx_td ltx_align_center\">67.84</td>\n<td class=\"ltx_td ltx_align_center\">0.09</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">1%</td>\n<td class=\"ltx_td ltx_align_center\">67.85</td>\n<td class=\"ltx_td ltx_align_center\">0.45</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" rowspan=\"3\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Fine-grained</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">reasoning only</td>\n</tr>\n</table> </td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Orig.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">84.24</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.35</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">5%</td>\n<td class=\"ltx_td ltx_align_center\">84.24</td>\n<td class=\"ltx_td ltx_align_center\">2.98</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">1%</td>\n<td class=\"ltx_td ltx_align_center\">84.23</td>\n<td class=\"ltx_td ltx_align_center\">14.63</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" rowspan=\"3\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">Cerberus</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\"><span class=\"ltx_text ltx_font_bold\">(Both stages)</span></td>\n</tr>\n</table> </td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Orig.</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">82.73</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">5%</td>\n<td class=\"ltx_td ltx_align_center\">82.72</td>\n<td class=\"ltx_td ltx_align_center\">0.30</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">82.71</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.69</td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 8</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Ablation study of cascaded architecture components showing AUC performance and overhead comparison under different anomaly proportions.</span></figcaption>\n</figure>\n<section class=\"ltx_subsubsection\" id=\"S6.SS4.SSS1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"641-impact-of-cascaded-architecture\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">6.4.1. </span>Impact of Cascaded Architecture</h4>\n<div class=\"ltx_para\" id=\"S6.SS4.SSS1.p1\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S6.F10\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape\" height=\"46\" id=\"S6.F10.g1\" src=\"https://arxiv.org/html/x16.png\" width=\"747\"/></div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S6.F10.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"201\" id=\"S6.F10.sf1.g1\" src=\"https://arxiv.org/html/x17.png\" width=\"398\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S6.F10.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"204\" id=\"S6.F10.sf2.g1\" src=\"https://arxiv.org/html/x18.png\" width=\"398\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 10</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">Comparison of different motion mask prompts on filtering efficiency and reasoning accuracy. In all cases, the recall of anomalies in coarse-grained filtering remains above 95%.</span></figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S6.F11\">\n<div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S6.F11.sf1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"269\" id=\"S6.F11.sf1.g1\" src=\"https://arxiv.org/html/x19.png\" width=\"399\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(a)</span> </span></figcaption>\n</figure>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<figure class=\"ltx_figure ltx_figure_panel ltx_align_center\" id=\"S6.F11.sf2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_img_landscape\" height=\"259\" id=\"S6.F11.sf2.g1\" src=\"https://arxiv.org/html/x20.png\" width=\"398\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">(b)</span> </span></figcaption>\n</figure>\n</div>\n</div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\"><span class=\"ltx_text\" style=\"font-size:90%;\">Figure 11</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">The performance of <span class=\"ltx_text ltx_font_typewriter\">Cerberus</span> with thresholds for <span class=\"ltx_text ltx_font_italic\">motion mask prompting</span> on a subset of <span class=\"ltx_text ltx_font_typewriter\">SHTech</span>.</span></figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S6.SS4.SSS2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"642-trade-offs-in-motion-mask-prompting\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">6.4.2. </span>Trade-offs in Motion Mask Prompting</h4>\n<div class=\"ltx_para\" id=\"S6.SS4.SSS2.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S6.SS4.SSS2.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"S6.SS4.SSS3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"643-comparison-of-rule-based-detection-methods\">\n<span class=\"ltx_tag ltx_tag_subsubsection\">6.4.3. </span>Comparison of Rule-based Detection Methods</h4>\n<div class=\"ltx_para\" id=\"S6.SS4.SSS3.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S6.T9\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:372.9pt;height:70pt;vertical-align:-32.2pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(22.4pt,-4.2pt) scale(1.13654618305805,1.13654618305805) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">SHTech</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold\">Campus</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Precision</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Recall</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AUC</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Precision</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">Recall</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">AUC</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">Anomaly-</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\">matching</td>\n</tr>\n</table></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">91.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">27.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">77.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">68.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">21.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">69.23</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">Cerberus</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">89.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">48.24</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">82.73</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">68.21</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">40.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\"><span class=\"ltx_text ltx_font_bold\">73.75</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\"><span class=\"ltx_text\" style=\"font-size:90%;\">Table 9</span>. </span><span class=\"ltx_text\" style=\"font-size:90%;\">End-to-end detection performance of different rule-based detection methods.</span></figcaption>\n</figure>\n</section>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S7\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"7-related-work\">\n<span class=\"ltx_tag ltx_tag_section\">7. </span>Related Work</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"S7.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S7.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"S7.p3\">\n\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S8\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"8-limitations-and-future-work\">\n<span class=\"ltx_tag ltx_tag_section\">8. </span>Limitations and Future Work</h2>\n<div class=\"ltx_para\" id=\"S8.p1\">\n\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S9\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"9-conclusion\">\n<span class=\"ltx_tag ltx_tag_section\">9. </span>Conclusion</h2>\n<div class=\"ltx_para\" id=\"S9.p1\">\n\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\" id=\"references\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">(1)</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Aboah (2021)</span>\n<span class=\"ltx_bibblock\">\nArmstrong Aboah. 2021.\n\n</span>\n<span class=\"ltx_bibblock\">A vision-based system for traffic anomaly detection using deep learning and decision trees.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPRW</span> (2021).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Achiam et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">Gpt-4 technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2303.08774</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Acsintoae et\u00a0al. (2022)</span>\n<span class=\"ltx_bibblock\">\nAndra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu\u00a0Tudor Ionescu, Fahad\u00a0Shahbaz Khan, and Mubarak Shah. 2022.\n\n</span>\n<span class=\"ltx_bibblock\">Ubnormal: New benchmark for supervised open-set video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bae et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nSangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">Fast and robust early-exiting framework for autoregressive language models with synchronized parallel decoding.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACL EMNLP</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bai et\u00a0al. (2025a)</span>\n<span class=\"ltx_bibblock\">\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2025a.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2308.12966</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bai et\u00a0al. (2025b)</span>\n<span class=\"ltx_bibblock\">\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et\u00a0al. 2025b.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen2. 5-vl technical report.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2502.13923</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bhardwaj et\u00a0al. (2022)</span>\n<span class=\"ltx_bibblock\">\nRomil Bhardwaj, Zhengxu Xia, Ganesh Ananthanarayanan, Junchen Jiang, Yuanchao Shu, Nikolaos Karianakis, Kevin Hsieh, Paramvir Bahl, and Ion Stoica. 2022.\n\n</span>\n<span class=\"ltx_bibblock\">Ekya: Continuous learning of video analytics models on edge compute servers.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">USENIX NSDI</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Bolya et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nDaniel Bolya, Po-Yao Huang, Peize Sun, Jang\u00a0Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et\u00a0al. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Perception encoder: The best visual embeddings are not at the output of the network.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2504.13181</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Cao et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nCongqi Cao, Yue Lu, Peng Wang, and Yanning Zhang. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">A new comprehensive benchmark for semi-supervised video anomaly detection and anticipation.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Carreira and Zisserman (2017)</span>\n<span class=\"ltx_bibblock\">\nJoao Carreira and Andrew Zisserman. 2017.\n\n</span>\n<span class=\"ltx_bibblock\">Quo vadis, action recognition? a new model and the kinetics dataset.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</span> (2017).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Chen et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJunxi Chen, Liang Li, Li Su, Zheng-jun Zha, and Qingming Huang. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Prompt-enhanced multiple instance learning for weakly supervised video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ding et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nZongcan Ding, Haodong Zhang, Peng Wu, Guansong Pang, Zhiwei Yang, Peng Wang, and Yanning Zhang. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2504.10320</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Feng et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nQianhan Feng, Wenshuo Li, Tong Lin, and Xinghao Chen. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Proceedings of the Computer Vision and Pattern Recognition Conference</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Guo et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et\u00a0al. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2501.12948</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et\u00a0al. (2018)</span>\n<span class=\"ltx_bibblock\">\nJunchen Jiang, Ganesh Ananthanarayanan, Peter Bodik, Siddhartha Sen, and Ion Stoica. 2018.\n\n</span>\n<span class=\"ltx_bibblock\">Chameleon: scalable adaptation of video analytics.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM SIGCOMM</span> (2018).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Jiang et\u00a0al. (2021)</span>\n<span class=\"ltx_bibblock\">\nShiqi Jiang, Zhiqi Lin, Yuanchun Li, Yuanchao Shu, and Yunxin Liu. 2021.\n\n</span>\n<span class=\"ltx_bibblock\">Flexible high-resolution object detection on edge devices with tunable latency.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM MobiCom</span> (2021).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et\u00a0al. (2022c)</span>\n<span class=\"ltx_bibblock\">\nChenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, Hehong Chen, Guohai Xu, Zheng Cao, et\u00a0al. 2022c.\n\n</span>\n<span class=\"ltx_bibblock\">mplug: Effective and efficient vision-language learning by cross-modal skip-connections.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACL EMNLP</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et\u00a0al. (2022a)</span>\n<span class=\"ltx_bibblock\">\nGuoqiu Li, Guanxiong Cai, Xingyu Zeng, and Rui Zhao. 2022a.\n\n</span>\n<span class=\"ltx_bibblock\">Scale-aware spatio-temporal relation learning for video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Springer ECCV</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et\u00a0al. (2022b)</span>\n<span class=\"ltx_bibblock\">\nShuo Li, Fang Liu, and Licheng Jiao. 2022b.\n\n</span>\n<span class=\"ltx_bibblock\">Self-training multi-sequence learning with transformer for weakly supervised video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">AAAI</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Li et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYunxin Li, Xinyu Chen, Baotian Hu, Longyue Wang, Haoyuan Shi, and Min Zhang. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Videovista: A versatile benchmark for video understanding and reasoning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2406.11303</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nJing Liu, Yang Liu, Jieyu Lin, Jielin Li, Liang Cao, Peng Sun, Bo Hu, Liang Song, Azzedine Boukerche, and Victor\u00a0CM Leung. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Networking systems for video anomaly detection: A tutorial and survey.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Comput. Surveys</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et\u00a0al. (2022)</span>\n<span class=\"ltx_bibblock\">\nXiaoming Liu, Zhanwei Zhang, Lingjuan Lyu, Zhaohan Zhang, Shuai Xiao, Chao Shen, and Philip\u00a0S Yu. 2022.\n\n</span>\n<span class=\"ltx_bibblock\">Traffic anomaly prediction based on joint static-dynamic spatio-temporal evolutionary learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE Transactions on Knowledge and Data Engineering</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Liu et\u00a0al. (2021)</span>\n<span class=\"ltx_bibblock\">\nZhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, and Guiqing Li. 2021.\n\n</span>\n<span class=\"ltx_bibblock\">A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE ICCV</span> (2021).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Lu et\u00a0al. (2013)</span>\n<span class=\"ltx_bibblock\">\nCewu Lu, Jianping Shi, and Jiaya Jia. 2013.\n\n</span>\n<span class=\"ltx_bibblock\">Abnormal event detection at 150 fps in matlab.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE ICCV</span> (2013).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Luo et\u00a0al. (2017)</span>\n<span class=\"ltx_bibblock\">\nWeixin Luo, Wen Liu, and Shenghua Gao. 2017.\n\n</span>\n<span class=\"ltx_bibblock\">A revisit of sparse coding based anomaly detection in stacked rnn framework.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE ICCV</span> (2017).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ma et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nJunxiao Ma, Jingjing Wang, Jiamin Luo, Peiying Yu, and Guodong Zhou. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM WWW</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Noghre et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nGhazal\u00a0Alinezhad Noghre, Armin\u00a0Danesh Pazho, and Hamed Tabkhi. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">An exploratory study on human-centric video anomaly detection through variational autoencoders and trajectory prediction.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE WACV</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">NVIDIA (2025)</span>\n<span class=\"ltx_bibblock\">\nNVIDIA. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">NVIDIA L40S.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.nvidia.com/en-us/data-center/l40s/\" title=\"\">https://www.nvidia.com/en-us/data-center/l40s/</a>. (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n<span class=\"ltx_bibblock\">Accessed on June 23, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">OpenCV (2013)</span>\n<span class=\"ltx_bibblock\">\nOpenCV. 2013.\n\n</span>\n<span class=\"ltx_bibblock\">OpenCV: Open Source Computer Vision Library.\n\n</span>\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/opencv/opencv\" title=\"\">https://github.com/opencv/opencv</a>. (2013).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n<span class=\"ltx_bibblock\">Accessed on June 26, 2025.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Pu et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYujiang Pu, Xiaoyu Wu, Lulu Yang, and Shengjin Wang. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Learning prompt-enhanced context features for weakly-supervised video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE Transactions on Image Processing</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Radford et\u00a0al. (2021)</span>\n<span class=\"ltx_bibblock\">\nAlec Radford, Jong\u00a0Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et\u00a0al. 2021.\n\n</span>\n<span class=\"ltx_bibblock\">Learning transferable visual models from natural language supervision.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ICML</span> (2021).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ramachandra et\u00a0al. (2020)</span>\n<span class=\"ltx_bibblock\">\nBharathkumar Ramachandra, Michael\u00a0J Jones, and Ranga\u00a0Raju Vatsavai. 2020.\n\n</span>\n<span class=\"ltx_bibblock\">A survey of single-scene video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE transactions on pattern analysis and machine intelligence</span> (2020).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Reich and Schultz (2024)</span>\n<span class=\"ltx_bibblock\">\nDaniel Reich and Tanja Schultz. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Uncovering the Full Potential of Visual Grounding Methods in VQA.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACL ACL</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Shtedritski et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nAleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">What does clip know about a red circle? visual prompt engineering for vlms.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE ICCV</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sultani et\u00a0al. (2018a)</span>\n<span class=\"ltx_bibblock\">\nWaqas Sultani, Chen Chen, and Mubarak Shah. 2018a.\n\n</span>\n<span class=\"ltx_bibblock\">Real-world anomaly detection in surveillance videos.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2018).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sultani et\u00a0al. (2018b)</span>\n<span class=\"ltx_bibblock\">\nWaqas Sultani, Chen Chen, and Mubarak Shah. 2018b.\n\n</span>\n<span class=\"ltx_bibblock\">Real-world anomaly detection in surveillance videos.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2018).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Sun and Gong (2023)</span>\n<span class=\"ltx_bibblock\">\nShengyang Sun and Xiaojin Gong. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">Hierarchical semantic contrast for scene-aware video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tang et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJiaqi Tang, Hao Lu, Ruizheng Wu, Xiaogang Xu, Ke Ma, Cheng Fang, Bin Guo, Jiangbo Lu, Qifeng Chen, and Yingcong Chen. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Hawk: Learning to understand open-world video anomalies.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">NeurIPS</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Team et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nGemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew\u00a0M Dai, Anja Hauth, Katie Millican, et\u00a0al. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">Gemini: a family of highly capable multimodal models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2312.11805</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Thakare et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nKamalakar\u00a0Vijay Thakare, Debi\u00a0Prosad Dogra, Heeseung Choi, Haksub Kim, and Ig-Jae Kim. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">Rareanom: A benchmark video dataset for rare type anomalies.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Pattern Recognition</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Tian et\u00a0al. (2021)</span>\n<span class=\"ltx_bibblock\">\nYu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan\u00a0W Verjans, and Gustavo Carneiro. 2021.\n\n</span>\n<span class=\"ltx_bibblock\">Weakly-supervised video anomaly detection with robust temporal feature magnitude learning.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE ICCV</span> (2021).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Vaswani et\u00a0al. (2017)</span>\n<span class=\"ltx_bibblock\">\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.\n\n</span>\n<span class=\"ltx_bibblock\">Attention is all you need.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">NeurIPS</span> (2017).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nAo Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, et\u00a0al. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Yolov10: Real-time end-to-end object detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">NeurIPS</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nBenfeng Wang, Chao Huang, Jie Wen, Wei Wang, Yabo Liu, and Yong Xu. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Federated Weakly Supervised Video Anomaly Detection with Multimodal Prompt.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">AAAI</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang and Cherian (2019)</span>\n<span class=\"ltx_bibblock\">\nJue Wang and Anoop Cherian. 2019.\n\n</span>\n<span class=\"ltx_bibblock\">Gods: Generalized one-class discriminative subspaces for anomaly detection. In <span class=\"ltx_text ltx_font_italic\">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>. 8201\u20138211.\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wang et\u00a0al. (2021)</span>\n<span class=\"ltx_bibblock\">\nXuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. 2021.\n\n</span>\n<span class=\"ltx_bibblock\">Robust unsupervised video anomaly detection by multipath frame prediction.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE transactions on neural networks and learning systems</span> (2021).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et\u00a0al. (2022)</span>\n<span class=\"ltx_bibblock\">\nJhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-Luh Liu. 2022.\n\n</span>\n<span class=\"ltx_bibblock\">Self-supervised sparse representation for video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Springer ECCV</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et\u00a0al. (2020)</span>\n<span class=\"ltx_bibblock\">\nPeng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. 2020.\n\n</span>\n<span class=\"ltx_bibblock\">Not only look, but also listen: Learning multimodal violence detection under weak supervision.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Springer ECCV</span> (2020).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et\u00a0al. (2024a)</span>\n<span class=\"ltx_bibblock\">\nPeng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, and Yanning Zhang. 2024a.\n\n</span>\n<span class=\"ltx_bibblock\">Weakly supervised video anomaly detection and localization with spatio-temporal prompts.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM MM</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et\u00a0al. (2024b)</span>\n<span class=\"ltx_bibblock\">\nPeng Wu, Xuerong Zhou, Guansong Pang, Zhiwei Yang, Qingsen Yan, Peng Wang, and Yanning Zhang. 2024b.\n\n</span>\n<span class=\"ltx_bibblock\">Weakly supervised video anomaly detection and localization with spatio-temporal prompts.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM MM</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Wu et\u00a0al. (2024c)</span>\n<span class=\"ltx_bibblock\">\nPeng Wu, Xuerong Zhou, Guansong Pang, Lingru Zhou, Qingsen Yan, Peng Wang, and Yanning Zhang. 2024c.\n\n</span>\n<span class=\"ltx_bibblock\">Vadclip: Adapting vision-language models for weakly supervised video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">AAAI</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Xiao et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJunbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Can i trust your answer? visually grounded video question answering.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yan et\u00a0al. (2023)</span>\n<span class=\"ltx_bibblock\">\nCheng Yan, Shiyu Zhang, Yang Liu, Guansong Pang, and Wenjun Wang. 2023.\n\n</span>\n<span class=\"ltx_bibblock\">Feature prediction diffusion model for video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE ICCV</span> (2023).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yan et\u00a0al. (2026)</span>\n<span class=\"ltx_bibblock\">\nYuxuan Yan, Shiqi Jiang, Ting Cao, Yifan Yang, Qianqian Yang, Yuanchao Shu, Yuqing Yang, and Lili Qiu. 2026.\n\n</span>\n<span class=\"ltx_bibblock\">Empowering agentic video analytics systems with video language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">USENIX NSDI</span> (2026).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et\u00a0al. (2024a)</span>\n<span class=\"ltx_bibblock\">\nYuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, and Shao-Yuan Lo. 2024a.\n\n</span>\n<span class=\"ltx_bibblock\">Follow the rules: reasoning for video anomaly detection with large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Springer ECCV</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Yang et\u00a0al. (2024b)</span>\n<span class=\"ltx_bibblock\">\nZhiwei Yang, Jing Liu, and Peng Wu. 2024b.\n\n</span>\n<span class=\"ltx_bibblock\">Text prompt with normality guidance for weakly supervised video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Ye et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nMuchao Ye, Weiyang Liu, and Pan He. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Vera: Explainable video anomaly detection via verbalized learning of vision-language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">You et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nHaoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, and Yingyan\u00a0Celine Lin. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">When linear attention meets autoregressive decoding: Towards more effective and efficient linearized large language models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM ICML</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zaheer et\u00a0al. (2022)</span>\n<span class=\"ltx_bibblock\">\nM\u00a0Zaigham Zaheer, Arif Mahmood, M\u00a0Haris Khan, Mattia Segu, Fisher Yu, and Seung-Ik Lee. 2022.\n\n</span>\n<span class=\"ltx_bibblock\">Generative cooperative learning for unsupervised video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2022).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zanella et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nLuca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, and Elisa Ricci. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Harnessing large language models for training-free video anomaly detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et\u00a0al. (2017)</span>\n<span class=\"ltx_bibblock\">\nHaoyu Zhang, Ganesh Ananthanarayanan, Peter Bodik, Matthai Philipose, Paramvir Bahl, and Michael\u00a0J Freedman. 2017.\n\n</span>\n<span class=\"ltx_bibblock\">Live video analytics at scale with approximation and <math alttext=\"\\{\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib62.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">{</mo><annotation encoding=\"application/x-tex\">\\{</annotation></semantics></math>Delay-Tolerance<math alttext=\"\\}\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib62.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">}</mo><annotation encoding=\"application/x-tex\">\\}</annotation></semantics></math>.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">USENIX NSDI</span> (2017).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nYanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et\u00a0al. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">arXiv preprint arXiv:2506.05176</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhang et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYiwen Zhang, Xumiao Zhang, Ganesh Ananthanarayanan, Anand Iyer, Yuanchao Shu, Victor Bahl, Z\u00a0Morley Mao, and Mosharaf Chowdhury. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Vulcan: Automatic Query Planning for Live <math alttext=\"\\{\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib64.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">{</mo><annotation encoding=\"application/x-tex\">\\{</annotation></semantics></math>ML<math alttext=\"\\}\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib64.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">}</mo><annotation encoding=\"application/x-tex\">\\}</annotation></semantics></math> Analytics.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">USENIX NSDI</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhao et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nXinyi Zhao, Congjing Zhang, Pei Guo, Wei Li, Lin Chen, Chaoyue Zhao, and Shuai Huang. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">SmartHome-Bench: A Comprehensive Benchmark for Video Anomaly Detection in Smart Homes Using Multi-Modal Large Language Models.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zheng et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nYue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, and Jiming Chen. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">A review on edge large language models: Design, execution, and applications.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">Comput. Surveys</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zhu et\u00a0al. (2024)</span>\n<span class=\"ltx_bibblock\">\nJiaqi Zhu, Shaofeng Cai, Fang Deng, Beng\u00a0Chin Ooi, and Junran Wu. 2024.\n\n</span>\n<span class=\"ltx_bibblock\">Do LLMs Understand Visual Anomalies? Uncovering LLM\u2019s Capabilities in Zero-shot Anomaly Detection.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">ACM MM</span> (2024).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_role_refnum ltx_tag_bibitem\">Zong et\u00a0al. (2025)</span>\n<span class=\"ltx_bibblock\">\nYongshuo Zong, Qin Zhang, Dongsheng An, Zhihua Li, Xiang Xu, Linghan Xu, Zhuowen Tu, Yifan Xing, and Onkar Dabeer. 2025.\n\n</span>\n<span class=\"ltx_bibblock\">Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels.\n\n</span>\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\">IEEE CVPR</span> (2025).\n\n</span>\n<span class=\"ltx_bibblock\">\n</span>\n</li>\n</ul>\n</section>\n</article>\n</div>\n\n</div>",
    "sections": [
      {
        "id": "cerberus-real-time-video-anomaly-detection-via-cascaded-vision-language-models",
        "title": "Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models",
        "level": 1
      },
      {
        "id": "abstract",
        "title": "Abstract.",
        "level": 6
      },
      {
        "id": "1-introduction",
        "title": "1. Introduction",
        "level": 2
      },
      {
        "id": "2-background-and-motivation",
        "title": "2. Background and Motivation",
        "level": 2
      },
      {
        "id": "21-vision-language-models",
        "title": "2.1. Vision-Language Models",
        "level": 3
      },
      {
        "id": "22-opportunities-in-anomaly-detection",
        "title": "2.2. Opportunities in Anomaly Detection",
        "level": 3
      },
      {
        "id": "3-system-overview",
        "title": "3. System Overview",
        "level": 2
      },
      {
        "id": "4-offline-induction",
        "title": "4. Offline Induction",
        "level": 2
      },
      {
        "id": "41-primary-rule-generation",
        "title": "4.1. Primary Rule Generation",
        "level": 3
      },
      {
        "id": "42-auxiliary-anomalies-customization",
        "title": "4.2. Auxiliary Anomalies Customization",
        "level": 3
      },
      {
        "id": "43-rule-evolution",
        "title": "4.3. Rule Evolution",
        "level": 3
      },
      {
        "id": "5-online-inference",
        "title": "5. Online Inference",
        "level": 2
      },
      {
        "id": "51-motion-mask-prompting",
        "title": "5.1. Motion Mask Prompting",
        "level": 3
      },
      {
        "id": "52-rule-based-deviation-detection",
        "title": "5.2. Rule-based Deviation Detection",
        "level": 3
      },
      {
        "id": "53-fine-grained-captioning-and-detection",
        "title": "5.3. Fine-grained Captioning and Detection",
        "level": 3
      },
      {
        "id": "6-evaluation",
        "title": "6. Evaluation",
        "level": 2
      },
      {
        "id": "61-experimental-setup",
        "title": "6.1. Experimental Setup",
        "level": 3
      },
      {
        "id": "62-overall-performance",
        "title": "6.2. Overall Performance",
        "level": 3
      },
      {
        "id": "63-evaluation-of-offline-induction",
        "title": "6.3. Evaluation of Offline Induction",
        "level": 3
      },
      {
        "id": "631-effect-of-rule-generation",
        "title": "6.3.1. Effect of Rule Generation",
        "level": 4
      },
      {
        "id": "632-impact-of-anomaly-customization",
        "title": "6.3.2. Impact of Anomaly Customization",
        "level": 4
      },
      {
        "id": "633-role-of-rule-evolution",
        "title": "6.3.3. Role of Rule Evolution",
        "level": 4
      },
      {
        "id": "64-evaluation-of-online-inference",
        "title": "6.4. Evaluation of Online Inference",
        "level": 3
      },
      {
        "id": "641-impact-of-cascaded-architecture",
        "title": "6.4.1. Impact of Cascaded Architecture",
        "level": 4
      },
      {
        "id": "642-trade-offs-in-motion-mask-prompting",
        "title": "6.4.2. Trade-offs in Motion Mask Prompting",
        "level": 4
      },
      {
        "id": "643-comparison-of-rule-based-detection-methods",
        "title": "6.4.3. Comparison of Rule-based Detection Methods",
        "level": 4
      },
      {
        "id": "7-related-work",
        "title": "7. Related Work",
        "level": 2
      },
      {
        "id": "8-limitations-and-future-work",
        "title": "8. Limitations and Future Work",
        "level": 2
      },
      {
        "id": "9-conclusion",
        "title": "9. Conclusion",
        "level": 2
      },
      {
        "id": "references",
        "title": "References",
        "level": 2
      }
    ],
    "has_math": true
  },
  "cached_at": 1761057900.549112
}