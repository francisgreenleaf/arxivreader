{
  "success": true,
  "arxiv_id": "2510.19801v1",
  "processed_content": {
    "success": true,
    "arxiv_id": "2510.19801v1",
    "metadata": {
      "arxiv_id": "2510.19801v1",
      "title": "The Feasibility of Training\nSovereign Language Models in the Global South: A Study of Brazil and\nMexico",
      "authors": [],
      "abstract": "The rapid escalation of computational requirements for training large-scale language models has reinforced structural asymmetries between high-capacity jurisdictions and countries in the Global South. This paper examines the technical and fiscal feasibility of sovereign-scale language model training in Brazil and Mexico under conditions of constrained hardware access, energy availability, and fiscal ceilings. Using a dual-axis design that varies accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150 days), we estimate compute demand, energy consumption, capital expenditures, and regulatory compatibility for the training of a 10-trillion-token model. Our findings show that while all configurations remain below export-control and electrical infrastructure thresholds, fiscal viability is determined by hardware efficiency. H100-based scenarios achieve training feasibility at a total cost of 8\u201314 million USD, while A100 deployments require 19\u201332 million USD due to higher energy and hardware demand. We argue that extending training timelines should be treated as a policy lever to mitigate hardware constraints, enabling the production of usable, auditable, and locally aligned models without competing at the global frontier. This study contributes to the discourse on AI compute governance and technological sovereignty by highlighting context-sensitive strategies that allow middle-income countries to establish sustainable and strategically sufficient AI capabilities."
    },
    "content": "<div class=\"arxiv-content\">\n<div class=\"ltx_page_content\">\n<article class=\"ltx_document ltx_authors_1line\">\n<h1 class=\"ltx_title ltx_title_document\" id=\"the-feasibility-of-training-sovereign-language-models-in-the-global-south-a-study-of-brazil-and-mexico\">The Feasibility of Training\nSovereign Language Models in the Global South: A Study of Brazil and\nMexico</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Sandra Malagon\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">Carreras con Impacto\n</span>\n<span class=\"ltx_contact ltx_role_affiliation\">Aixo\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Monica A. Ulloa Ruiz\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">Carreras con Impacto\n</span>\n<span class=\"ltx_contact ltx_role_affiliation\">Aixo\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Tatiana Elizabeth Sandoval Plaza\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">Carreras con Impacto\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Gabriel Rafael Rosario Bol\u00edvar\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">Carreras con Impacto\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Valentina Garc\u00eda Mesa\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">Carreras con Impacto\n</span></span></span>\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">Ivanna Alvarado Morales\n</span><span class=\"ltx_author_notes\">\n<span class=\"ltx_contact ltx_role_affiliation\">Carreras con Impacto\n</span></span></span>\n</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\" id=\"abstract\">Abstract</h6>\n<p class=\"ltx_p\">The rapid escalation of computational requirements for training large-scale language models has reinforced structural asymmetries between high-capacity jurisdictions and countries in the Global South. This paper examines the technical and fiscal feasibility of sovereign-scale language model training in Brazil and Mexico under conditions of constrained hardware access, energy availability, and fiscal ceilings. Using a dual-axis design that varies accelerator generation (NVIDIA H100 vs. A100) and training duration (90 vs. 150 days), we estimate compute demand, energy consumption, capital expenditures, and regulatory compatibility for the training of a 10-trillion-token model. Our findings show that while all configurations remain below export-control and electrical infrastructure thresholds, fiscal viability is determined by hardware efficiency. H100-based scenarios achieve training feasibility at a total cost of 8\u201314 million USD, while A100 deployments require 19\u201332 million USD due to higher energy and hardware demand. We argue that extending training timelines should be treated as a policy lever to mitigate hardware constraints, enabling the production of usable, auditable, and locally aligned models without competing at the global frontier. This study contributes to the discourse on AI compute governance and technological sovereignty by highlighting context-sensitive strategies that allow middle-income countries to establish sustainable and strategically sufficient AI capabilities.</p>\n</div>\n<section class=\"ltx_section\" id=\"Sx1\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"1-introduction\">1. Introduction</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p1\">\n<p class=\"ltx_p\">The rapid development of frontier-scale language models has accelerated\nglobal demand for compute\u2014but also reinforced longstanding\ninequalities in technological access. Training these systems requires\nthousands of high-end GPUs running uninterrupted for weeks, supported by\nrobust electrical infrastructure, cooling systems, and data centre\nengineering. As a result, computational capacity is increasingly\nconcentrated in a handful of jurisdictions, leading to what researchers\nhave termed a new \u201cGPU North\u2013South divide\u201d (Sastry et al., 2024).\nCountries outside this core are not only less able to train frontier\nmodels, but may also struggle to adapt or audit pre-trained systems in\nways that reflect local linguistic, legal, and institutional priorities.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p2\">\n<p class=\"ltx_p\">At the same time, energy availability has emerged as a strategic\nconstraint in the deployment of AI infrastructure\u2014particularly in\ncountries of the Global South, where electrical grids may be less robust\nor subject to capacity and reliability limits. The International Energy\nAgency projects that electricity demand from AI and data centres could\nexceed 1,000 TWh annually by 2026, a figure comparable to the total\npower consumption of mid-sized economies (IEA, 2024). A single GPT-3\ntraining run consumed more than 1.3 GWh (Patterson et al., 2021), and\ninference demands have since grown even larger and more geographically\ndistributed. In many regions, access to high-end accelerators such as\nthe NVIDIA H100 is limited by export controls (BIS, 2025) or market\nconstraints\u2014leaving prior-generation chips like the A100, still\navailable through secondary markets and data centre liquidations, as the\nmost viable alternative. In such settings, both fiscal cost and\nelectricity availability become binding considerations, since older GPUs\nincur higher energy and hardware costs per unit of compute.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p3\">\n<p class=\"ltx_p\">This paper investigates the feasibility of training sovereign\nlarge-scale language models under these dual constraints. We ask whether\ncountries like Brazil and Mexico could produce usable, non-frontier\nmodels by trading hardware efficiency for longer training cycles\u2014using\nlegacy GPUs over extended periods to offset compute limitations while\nremaining within infrastructure and budgetary bounds.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p4\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p5\">\n<p class=\"ltx_p\">Our results offer three key contributions:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p6\">\n<ul class=\"ltx_itemize\" id=\"Sx1.I1\">\n<li class=\"ltx_item\" id=\"Sx1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx1.I1.i1.p1\">\n<p class=\"ltx_p\">We show that\u2014even with constrained infrastructure\u2014training a\nlarge, usable model remains technically and economically viable for\nTier 2 countries.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx1.I1.i2.p1\">\n<p class=\"ltx_p\">We demonstrate that A100-based deployments are less fiscally viable,\nas their lower efficiency drives up both electricity consumption and\ncapital expenditures.</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.I1.i3.p1\">\n<p class=\"ltx_p\">We argue that training time should be treated as a policy lever, not\nmerely a technical parameter: by extending training windows, countries\nin the Global South can adopt alternative compute strategies without\nrequiring immediate access to frontier accelerators.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p7\">\n<p class=\"ltx_p\">In doing so, we contribute to the broader discourse on compute\ngovernance, energy-aware AI planning, and technological sovereignty in\nthe Global South. Rather than focusing on frontier competitiveness, our\nanalysis highlights viable, context-sensitive strategies for training\nmodels that are \u201cgood enough\u201d\u2014usable, auditable, and locally\naligned\u2014even in the absence of cutting-edge hardware.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx2\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"2-methodology\">2. Methodology</h2>\n<section class=\"ltx_subsection\" id=\"Sx2.SSx1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"21-training-model-assumptions\">2.1 Training Model\nAssumptions</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx1.p1\">\n<p class=\"ltx_p\">We assume a total training compute budget of 3.0\u00d710\u02c624 FLOPs, based\non a benchmark consistent with DeepSeek\u2011V3\u2014a 671B-parameter\nMixture-of-Experts model trained over 14.8 trillion tokens with\napproximately 37 billion active parameters per forward-backward pass.\nThis estimate aligns with standard Transformer training approximations\nand the reporting methodology used by Epoch AI (2024), which tracks\nmodels trained at scales of 10\u02c623 FLOPs.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx1.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx1.p3\">\n<p class=\"ltx_p\">We define a strategically sufficient model as a large-scale language\nmodel that that does not meet the definition of frontier AI\u2014described\nby the UK Government (DSIT, 2025) as highly capable general-purpose\nmodels that can perform a wide variety of tasks and match or exceed the\ncapabilities of today\u2019s most advanced systems\u2014but achieves the level\nof performance required to support public functions like education,\nlegal reasoning, administrative automation, and local language\nalignment. These models are deployable on constrained infrastructure,\nauditable under public governance frameworks, and adaptable to national\nor institutional priorities.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx1.p4\">\n<p class=\"ltx_p\">For example, DeepSeek\u2011V3 was trained using approximately 10\u00d7 less\ncompute than GPT\u20114, which Epoch AI (2024) estimates at 1.8\u00d710\u02c625 to\n3.0\u00d710\u02c625 FLOPs. Despite this, it achieves an aggregate quality score\nof 80.0 on ArtificialAnalysis.ai (2025), outperforming all open-weight\nmodels and scoring within ~10 percentage points of GPT\u20114o.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx2.SSx2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"22-hardware-and-training-duration-scenarios\">2.2 Hardware and Training Duration\nScenarios</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx2.p1\">\n<p class=\"ltx_p\">We evaluate four infrastructure configurations combining two accelerator\nclasses\u2014NVIDIA H100 and NVIDIA A100\u2014with two training schedules: 90\ndays and 150 days. These scenarios are designed to capture key\ntrade-offs between hardware generation, energy efficiency, and\ndeployment feasibility.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx2.p2\">\n<p class=\"ltx_p\">The modeling assumptions are as follows:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx2.p3\">\n<ul class=\"ltx_itemize\" id=\"Sx2.I2\">\n<li class=\"ltx_item\" id=\"Sx2.I2.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I2.i1.p1\">\n<p class=\"ltx_p\">H100 GPUs are assigned a peak throughput of 2,000 TFLOPs using FP8\nprecision via NVIDIA\u2019s Transformer Engine (NVIDIA).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I2.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I2.i2.p1\">\n<p class=\"ltx_p\">A100 GPUs are assigned a peak throughput of 312 TFLOPs under FP16\nprecision, as specified in NVIDIA\u2019s official architectural\ndocumentation (NVIDIA).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I2.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.I2.i3.p1\">\n\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx2.p4\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx1\">\n<tbody id=\"Sx2.Ex1\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\">GPUs</span></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{\\text{Total FLOPs}}{\\text{Training time (s)}\\times\\text{Peak TFLOPs per GPU}\\times 0.552}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex1.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mtext>Total FLOPs</mtext><mrow><mtext>Training time (s)</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>Peak TFLOPs per GPU</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>0.552</mn></mrow></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{\\text{Total FLOPs}}{\\text{Training time (s)}\\times\\text{Peak TFLOPs per GPU}\\times 0.552}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx2.p5\">\n<p class=\"ltx_p\">Where training time is expressed in seconds over 90 or 150 days. This\nresults in GPU requirements ranging from ~350 H100s (in\nthe 150-day scenario) to over 2,200 A100s (in the 90-day scenario),\nillustrating the impact of both hardware efficiency and extended\ntraining schedules on infrastructure needs. Detailed breakdowns of GPU\nrequirements for each scenario are provided in Annex 1.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx2.SSx3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"23-estimation-of-resources\">2.3 Estimation of Resources</h3>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx3.SSSx1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"energy-consumption-mwh\">Energy consumption (MWh)</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx1.p1\">\n<p class=\"ltx_p\">Total energy consumption is calculated as the product of training\nduration, GPU thermal design power (TDP), and total GPU count, adjusted\nby a Power Usage Effectiveness (PUE) of 1.3 to reflect datacenter\noverhead. We assume:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx1.p2\">\n<ul class=\"ltx_itemize\" id=\"Sx2.I3\">\n<li class=\"ltx_item\" id=\"Sx2.I3.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I3.i1.p1\">\n<p class=\"ltx_p\">700\u2009W TDP per H100 GPU</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I3.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.I3.i2.p1\">\n<p class=\"ltx_p\">400\u2009W TDP per A100 GPU</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx1.p3\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx2\">\n<tbody id=\"Sx2.Ex2\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\">Energy (MWh)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{\\text{NGPU}\\times\\text{TDPadj}\\times\\text{PUE}\\times\\text{D}\\times 24}{10^{6}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex2.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><mtext>NGPU</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>TDPadj</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>PUE</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>D</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>24</mn></mrow><msup><mn>10</mn><mn>6</mn></msup></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{\\text{NGPU}\\times\\text{TDPadj}\\times\\text{PUE}\\times\\text{D}\\times 24}{10^{6}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx1.p4\">\n<p class=\"ltx_p\">These estimates yield energy requirements between 0.3 and 3.3 GWh\ndepending on hardware type and training schedule. Longer schedules and\nnewer hardware significantly reduce cumulative consumption.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx3.SSSx2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"peak-electrical-load-mw\">Peak electrical load (MW)</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx2.p1\">\n<p class=\"ltx_p\">We estimate peak demand by assuming full simultaneous usage of all GPUs\nin a given configuration:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx2.p2\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx3\">\n<tbody id=\"Sx2.Ex3\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><span class=\"ltx_text ltx_markedasmath\">Peak Load (MW)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\frac{N_{\\text{GPU}}\\times\\text{TDP}_{\\text{adj}}\\times\\text{PUE}}{10^{6}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex3.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mstyle displaystyle=\"true\"><mfrac><mrow><msub><mi>N</mi><mtext>GPU</mtext></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><msub><mtext>TDP</mtext><mtext>adj</mtext></msub><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>PUE</mtext></mrow><msup><mn>10</mn><mn>6</mn></msup></mfrac></mstyle></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\frac{N_{\\text{GPU}}\\times\\text{TDP}_{\\text{adj}}\\times\\text{PUE}}{10^{6}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx2.p3\">\n<p class=\"ltx_p\">Peak load ranges from 0.41 MW (H100, 150 days) to 1.49 MW (A100, 90\ndays), remaining within the envelope of medium-voltage distribution\ninfrastructure in most Tier 2 urban industrial parks.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx3.SSSx3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"capital-expenditures-capex-hardware-prices-are-set-at\">Capital Expenditures (CAPEX)\n<br class=\"ltx_break\"/>Hardware prices are set\nat:</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx3.p1\">\n<ul class=\"ltx_itemize\" id=\"Sx2.I4\">\n<li class=\"ltx_item\" id=\"Sx2.I4.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I4.i1.p1\">\n<p class=\"ltx_p\">33,000\u2009USD per H100 GPU</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I4.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.I4.i2.p1\">\n<p class=\"ltx_p\">12,000\u2009USD per A100 GPU</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx3.p2\">\n<p class=\"ltx_p\">A 30% integration overhead is applied to account for additional\ncomponents (CPUs, memory, SSDs, NICs, chassis, etc.). Tariff assumptions\nare country-specific:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx3.p3\">\n<ul class=\"ltx_itemize\" id=\"Sx2.I5\">\n<li class=\"ltx_item\" id=\"Sx2.I5.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I5.i1.p1\">\n<p class=\"ltx_p\">Brazil: 16% import duty on GPU hardware (International Trade\nAdministration, 2021)</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I5.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.I5.i2.p1\">\n<p class=\"ltx_p\">Mexico: 0% import duty, consistent with digital infrastructure\nexemptions (Gobierno de M\u00e9xico, 2023)</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx3.p4\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx4\">\n<tbody id=\"Sx2.Ex4\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{CAPEX}_{\\text{USD}}=\\text{Number of GPUs}\\times\\text{Price per GPU}\\times 1.30\\times(1+\\text{Import Tariff})\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex4.m1\" intent=\":literal\"><semantics><mrow><msub><mtext>CAPEX</mtext><mtext>USD</mtext></msub><mo>=</mo><mrow><mtext>Number of GPUs</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>Price per GPU</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>1.30</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>1</mn><mo>+</mo><mtext>Import Tariff</mtext></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle\\text{CAPEX}_{\\text{USD}}=\\text{Number of GPUs}\\times\\text{Price per GPU}\\times 1.30\\times(1+\\text{Import Tariff})</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx3.p5\">\n<p class=\"ltx_p\">CAPEX dominates total system cost in all scenarios, ranging from 1.06\nmillion USD to 13.49 million USD depending on configuration.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx3.SSSx4\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"operating-expenditures-opex\">Operating Expenditures\n(OPEX)</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p1\">\n<p class=\"ltx_p\">Electricity costs are based on the total MWh consumed per scenario and\nthe applicable industrial rates:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p2\">\n<ul class=\"ltx_itemize\" id=\"Sx2.I6\">\n<li class=\"ltx_item\" id=\"Sx2.I6.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I6.i1.p1\">\n<p class=\"ltx_p\">Brazil: 110\u2009USD/MWh (ANEEL, 2024)</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I6.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.I6.i2.p1\">\n<p class=\"ltx_p\">Mexico: 88\u2009USD/MWh (Comisi\u00f3n Federal de electricidad, 2025)</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p3\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx5\">\n<tbody id=\"Sx2.Ex5\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{OPEX}_{\\text{USD}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex5.m1\" intent=\":literal\"><semantics><msub><mtext>OPEX</mtext><mtext>USD</mtext></msub><annotation encoding=\"application/x-tex\">\\displaystyle\\text{OPEX}_{\\text{USD}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=\\text{Energy (MWh)}\\times\\text{Tariff (USD/MWh)}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex5.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mtext>Energy (MWh)</mtext><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mtext>Tariff (USD/MWh)</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=\\text{Energy (MWh)}\\times\\text{Tariff (USD/MWh)}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p4\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p5\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx6\">\n<tbody id=\"Sx2.Ex6\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{OPEX}_{\\text{USD}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex6.m1\" intent=\":literal\"><semantics><msub><mtext>OPEX</mtext><mtext>USD</mtext></msub><annotation encoding=\"application/x-tex\">\\displaystyle\\text{OPEX}_{\\text{USD}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=893\\times 110=98.230\\text{ USD }(0.098M)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex6.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>893</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>110</mn></mrow><mo>=</mo><mrow><mn>98.230</mn><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mtext>\u00a0USD\u00a0</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>0.098</mn><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=893\\times 110=98.230\\text{ USD }(0.098M)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p6\">\n<p class=\"ltx_p\">while in Mexico the same energy use at 88 USD/MWh resulted in</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p7\">\n<table class=\"ltx_equationgroup ltx_eqn_align ltx_eqn_table table table-responsive\" id=\"Sx7.EGx7\">\n<tbody id=\"Sx2.Ex7\"><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_td ltx_align_right ltx_eqn_cell\"><math alttext=\"\\displaystyle\\text{OPEX}_{\\text{USD}}\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex7.m1\" intent=\":literal\"><semantics><msub><mtext>OPEX</mtext><mtext>USD</mtext></msub><annotation encoding=\"application/x-tex\">\\displaystyle\\text{OPEX}_{\\text{USD}}</annotation></semantics></math></td>\n<td class=\"ltx_td ltx_align_left ltx_eqn_cell\"><math alttext=\"\\displaystyle=893\\times 88=78.584\\text{ USD }(0.079M)\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.Ex7.m2\" intent=\":literal\"><semantics><mrow><mi></mi><mo>=</mo><mrow><mn>893</mn><mo lspace=\"0.222em\" rspace=\"0.222em\">\u00d7</mo><mn>88</mn></mrow><mo>=</mo><mrow><mn>78.584</mn><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mtext>\u00a0USD\u00a0</mtext><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo stretchy=\"false\">(</mo><mrow><mn>0.079</mn><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi>M</mi></mrow><mo stretchy=\"false\">)</mo></mrow></mrow></mrow><annotation encoding=\"application/x-tex\">\\displaystyle=893\\times 88=78.584\\text{ USD }(0.079M)</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n</tr></tbody>\n</table>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p8\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx3.SSSx4.p9\">\n<p class=\"ltx_p\">Across all scenarios, OPEX remains below 0.4 million USD\u2014well under\n5% of total cost\u2014highlighting that training expenditure is\noverwhelmingly driven by hardware acquisition rather than energy use.\nFull cost breakdowns by country and scenario are provided in Annex 1.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx2.SSx4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"24-feasibility-constraints-and-evaluation-framework\">2.4 Feasibility Constraints and Evaluation\nFramework</h3>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx4.SSSx1\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"export-control-ceiling-leq-50000-gpus\">Export-control ceiling (<math alttext=\"\\leq\" class=\"ltx_Math\" display=\"inline\" id=\"Sx2.SSx4.SSSx1.m1\" intent=\":literal\"><semantics><mo>\u2264</mo><annotation encoding=\"application/x-tex\">\\leq</annotation></semantics></math> 50,000\nGPUs)</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx4.SSSx1.p1\">\n<p class=\"ltx_p\">We adopt a GPU cap of 50,000 units per country, in line with recent\nexport controls issued by the U.S. Bureau of Industry and Security (BIS,\n2025). These restrictions apply to high-end accelerators such as the\nH100 and A100, and are already enforced in key regions like China and\nthe Middle East. This threshold reflects growing global scrutiny over\ncompute flows and offers a conservative yet actionable boundary for\nnational-scale deployment.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx4.SSSx2\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"electrical-infrastructure-limit\">Electrical infrastructure\nlimit</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx4.SSSx2.p1\">\n<p class=\"ltx_p\">We establish a dual-tiered infrastructure constraint:</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx4.SSSx2.p2\">\n<ul class=\"ltx_itemize\" id=\"Sx2.I7\">\n<li class=\"ltx_item\" id=\"Sx2.I7.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx2.I7.i1.p1\">\n<p class=\"ltx_p\">A maximum peak load of 10 megawatts (MW) is adopted as a hard upper\nlimit for urban deployments without high-voltage interconnection.\nLoads beyond this level typically require dedicated substations and\nspecialized grid upgrades (SENER, 2023; Enel-SP, 2023).</p>\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx2.I7.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.I7.i2.p1\">\n<p class=\"ltx_p\">More critically, we identify 1\u2009MW as the practical threshold for\ndeployment without additional permitting or physical infrastructure\nupgrades in urban industrial parks and university zones. This is based\non capacity studies in Quer\u00e9taro Comisi\u00f3n Federal de Electricidad\n(2024) and S\u00e3o Paulo, where 1\u20132\u2009MW loads are generally supportable\nwithin existing 13\u201334\u2009kV distribution networks.</p>\n</div>\n</li>\n</ul>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx4.SSSx2.p3\">\n<p class=\"ltx_p\">1\u2009MW often triggers additional barriers: transformer reinforcement,\nredundant feeder lines, and, in some jurisdictions, environmental impact\nassessments or public utility filings. While not insurmountable, these\nburdens may delay or limit deployments by public institutions with\nrestricted engineering or legal capacity.</p>\n</div>\n</section>\n<section class=\"ltx_subsubsection\" id=\"Sx2.SSx4.SSSx3\">\n<h4 class=\"ltx_title ltx_title_subsubsection\" id=\"fiscal-feasibility-cap\">Fiscal feasibility cap</h4>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx4.SSSx3.p1\">\n<blockquote class=\"ltx_quote\">\n<p class=\"ltx_p\">We adopt 52 million USD as the fiscal ceiling for sovereign AI\ninfrastructure projects, inclusive of CAPEX and first-year OPEX. This\nthreshold was chosen taking as reference the first-year investment\nspending of a recent digital infrastructure project, Internet para Todos\nM\u00e9xico (CFE, 2024).</p>\n</blockquote>\n</div>\n</section>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx2.SSx5\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"25-case-selection-quer\u00e9taro-and-s\u00e3o-paulo\">2.5 Case Selection: Quer\u00e9taro and S\u00e3o\nPaulo</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx5.p1\">\n<p class=\"ltx_p\">To evaluate the real-world feasibility of sovereign AI infrastructure\ndeployment, we anchor our modeling in two reference locations:\nQuer\u00e9taro, in central Mexico, and S\u00e3o Paulo, in southeastern Brazil.\nBoth regions offer a realistic testbed for subnational AI clusters in\nupper-middle-income countries, combining medium-voltage grid access,\nindustrial concentration, and public infrastructure alignment.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx5.p2\">\n<p class=\"ltx_p\">In Quer\u00e9taro, national planning documents identify the municipalities of\nEl Marqu\u00e9s and Corregidora as focal points for industrial growth and\nmedium-voltage energy reinforcement. The Programa de Desarrollo del\nSistema El\u00e9ctrico Nacional (PRODESEN) outlines multiple investment\nprojects to expand substation capacity and ensure energy redundancy for\nnew industrial zones, many of which operate on 13\u201334\u2009kV distribution\nsystems (SENER, 2023). These networks can typically support sustained\nloads in the 3\u20135\u2009MW range without requiring high-voltage\ninterconnection. Within this context, AI training scenarios consuming up\nto 100 MWh/day and drawing peak loads below 5\u2009MW remain within feasible\ndeployment bounds for sovereign compute clusters.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx5.p3\">\n<p class=\"ltx_p\">Similarly, in S\u00e3o Paulo, public infrastructure data from the Operador\nNacional do Sistema El\u00e9trico (ONS) shows that industrial districts in\nBarueri, Campinas, and Guarulhos are routinely supplied by\nmedium-voltage feeders capable of managing 5\u201315\u2009MW loads without\ndedicated substations (ONS, 2009). These areas have historically hosted\ndatacenters and mission-critical services and are identified as priority\nregions under Brazil\u2019s Plano Nacional de Conectividade (MCTI, 2022).\nGrid conditions in these zones are compatible with the types of training\nconfigurations modeled in this study, especially those remaining under\n5\u2009MW of peak demand.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx2.SSx5.p4\">\n<p class=\"ltx_p\">While both sites present viable conditions for AI deployment at moderate\nscale, configurations exceeding 10\u2009MW\u2014used in some comparative\nscenarios\u2014would likely trigger additional permitting, infrastructure\nupgrades, or grid reinforcement, making them significantly more complex\nand less immediately viable.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"Sx3\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"3-results\">3. Results</h2>\n<section class=\"ltx_subsection\" id=\"Sx3.SSx1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"31-hardware-requirements\">3.1 Hardware Requirements</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx1.p1\">\n<p class=\"ltx_p\">The scale of compute required varies substantially across scenarios,\ndriven by both hardware generation and training duration. In the most\ndemanding case\u2014A100s over 90 days\u2014the cluster exceeds 2,200 units,\nwhile the most efficient configuration\u2014H100s over 150 days\u2014requires\nfewer than 400 units. This six-fold spread underscores the central role\nof accelerator choice and schedule design in determining feasibility.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx1.p2\">\n<p class=\"ltx_p\">The efficiency differential is especially pronounced across generations.\nWith a peak throughput of 2,000 TFLOPs, the H100 reduces total GPU\ndemand by an order of magnitude relative to the A100 at 312 TFLOPs.\nExtending training from 90 to 150 days compounds this effect, lowering\nrequirements by roughly 40 percent across both hardware classes. These\ndynamics set the practical envelope for sovereign-scale deployments, as\nsummarized in Figure 1.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"Sx3.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"235\" id=\"Sx3.F1.g1\" src=\"https://arxiv.org/html/media/image1.png\" width=\"381\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>GPU requirements by hardware class and training duration.</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx3.SSx2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"32-energy-consumption\">3.2 Energy Consumption</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx2.p1\">\n<p class=\"ltx_p\">Energy requirements remain within a moderate range across all scenarios.\nThe most efficient configuration (H100\u2013150d) consumes around 0.3 GWh\nover the course of training, while the most demanding (A100\u201390d)\nreaches 3.3 GWh. These values, detailed in Figure 2, correspond to daily\naverages that fall comfortably below the thresholds typically associated\nwith high-voltage interconnection. In practice, they remain compatible\nwith the supply conditions of medium-voltage industrial parks.</p>\n</div>\n<figure class=\"ltx_figure\" id=\"Sx3.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"236\" id=\"Sx3.F2.g1\" src=\"https://arxiv.org/html/media/image4.png\" width=\"381\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Total energy consumption (GWh).</figcaption>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx2.p2\">\n<p class=\"ltx_p\">Patterns in peak electrical load reinforce this conclusion. As reported\nin Figure 3, the largest configuration (A100\u201390d) requires 1.49 MW,\ncompared to only 0.41 MW in the H100\u2013150d case. Both figures remain\nunder the 10 MW ceiling used in our feasibility framework. Yet\nconfigurations that approach or cross the 1 MW practical threshold would\nlikely face additional permitting and infrastructure requirements in\nurban deployments, introducing a critical distinction between \u201cformally\nviable\u201d and \u201ceasily deployable.\u201d</p>\n</div>\n<figure class=\"ltx_figure\" id=\"Sx3.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"236\" id=\"Sx3.F3.g1\" src=\"https://arxiv.org/html/media/image5.png\" width=\"381\"/>\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>Peak electrical load (MW).</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx3.SSx3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"33-costs\">3.3 Costs</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx3.p1\">\n<p class=\"ltx_p\">Capital expenditures represent the dominant component of training costs\nacross all scenarios. Hardware requirements vary from 8.3 to 13.8\nmillion USD in the H100 configurations and from 19.3 to 32.3 million USD\nin the A100 configurations. As reported in Table 1, these values already\nincorporate a 30 percent integration overhead and country-specific\ntariff regimes. The contrast between Mexico\u2019s 0 (Gobierno de M\u00e9xico,\n2023; BIS,2025) percent import duty and Brazil\u2019s 16 percent duty is\nmaterial, raising Brazilian deployments by several million dollars at\nscale.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx3.p2\">\n<p class=\"ltx_p\">By comparison, operating expenditures remain marginal. Electricity costs\nfall between 0.08 and 0.36 million USD, depending on scenario and\nlocation (CFE, 2025; ANEEL, 2024). Table 1 also shows that even under\nthe most energy-intensive configuration, OPEX accounts for less than\nfive percent of total training costs. This confirms that fiscal\nfeasibility hinges overwhelmingly on accelerator generation and tariff\nregimes rather than energy pricing.</p>\n</div>\n<figure class=\"ltx_table\" id=\"Sx3.T1\">\n<table class=\"ltx_tabular table table-responsive\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Scenario</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Country</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">CAPEX (M USD)</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">OPEX (M USD)</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">TOTAL</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">H100 \u00b7 90d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Brazil</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">13,78</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,10</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">13,88</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">H100 \u00b7 90d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Mexico</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">13,82</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,08</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">13,90</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">H100 \u00b7 150d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Brazil</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">8,28</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,10</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">8,37</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">H100 \u00b7 150d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Mexico</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">8,32</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,08</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">8,39</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">A100 \u00b7 90d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Brazil</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">32,24</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,36</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">32,60</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">A100 \u00b7 90d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Mexico</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">32,26</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,29</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">32,54</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">A100 \u00b7 150d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Brazil</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">19,34</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,36</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">19,70</span>\n</span></span>\n</span>\n</th>\n</tr>\n<tr class=\"ltx_tr\">\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_bb ltx_border_b\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">A100 \u00b7 150d</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_bb ltx_border_b\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">Mexico</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_bb ltx_border_b\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">19,35</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_bb ltx_border_b\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">0,29</span>\n</span></span>\n</span>\n</th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_bb ltx_border_b\">\n<span class=\"ltx_inline-block\">\n<span class=\"ltx_p\" style=\"width:0.0pt;\">\n<span class=\"ltx_inline-block ltx_align_left ltx_minipage ltx_align_bottom\" style=\"width:433.6pt;\">\n<span class=\"ltx_p ltx_align_left\">19,64</span>\n</span></span>\n</span>\n</th>\n</tr>\n</thead>\n</table>\n</figure>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx3.p3\">\n<p class=\"ltx_p\"><em class=\"ltx_emph ltx_font_italic\">Table 1: Capital and operating expenditures (M USD) by hardware\nclass, training duration, and country.</em></p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx3.SSx4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"34-feasibility-assessment\">3.4 Feasibility Assessment</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx4.p1\">\n<p class=\"ltx_p\">Each configuration was evaluated against the three feasibility\nthresholds established by export controls, electrical infrastructure,\nand fiscal ceilings. The results, show that export and power constraints\nare formally non-binding: all scenarios remain well below the 50,000-GPU\nexport cap and the 10 MW power ceiling.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx4.p2\">\n<p class=\"ltx_p\">The fiscal constraint is ultimately driven by energy inefficiency: A100s\nrequire many more units, which inflates both electricity demand and\ncapital outlays. Because A100s deliver substantially lower throughput\nper unit than H100s, they require far more accelerators to reach the\nsame compute target. This larger fleet increases both cumulative\nelectricity demand and capital outlays, pushing total costs into the\n19\u201332 million.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx3.SSx4.p3\">\n<p class=\"ltx_p\">By contrast, the H100 configurations achieve the same training target\nwith fewer units, lower energy consumption, and total costs between 8\nand 14 million USD. This makes them the most cost-effective\nconfigurations that satisfy the full set of feasibility criteria.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"Sx4\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"4-discussion\">4. Discussion</h2>\n<section class=\"ltx_subsection\" id=\"Sx4.SSx1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"41-sovereign-compute-is-technically-feasible\">4.1 Sovereign Compute Is Technically\nFeasible</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx1.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx1.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx4.SSx2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"42-efficiency-is-the-decisive-variable\">4.2 Efficiency Is the Decisive\nVariable</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx2.p1\">\n<p class=\"ltx_p\">The decisive factor separating scenarios is hardware efficiency. A100\naccelerators deliver roughly six times fewer FLOPs per unit than H100s,\nwhich means that training the same model requires an order of magnitude\nmore devices. While individual A100s are cheaper, the need for\nadditional units increases both capital expenditures and electricity\nconsumption, raising total costs to 19\u201332 million USD. By contrast,\nH100 scenarios achieve the same training target with far fewer devices,\nkeeping total costs within 8\u201314 million USD.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx2.p2\">\n<p class=\"ltx_p\">This pattern is consistent with prior analyses. Sevilla et al. (2022)\nshow that generational improvements in accelerator efficiency\u2014both\nFLOPs per dollar and FLOPs per watt\u2014are the primary drivers of the\ndeclining cost of training. Sastry et al. (2024) similarly note that\nefficiency gains, not absolute budgets, determine whether national\nactors can feasibly deploy sovereign compute clusters.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx2.p3\">\n<p class=\"ltx_p\">Our results reinforce this conclusion: fiscal viability in the Global\nSouth depends less on headline budgets than on the choice of hardware\ngeneration. While both H100- and A100-based deployments fall within a\nfeasible range, the former are more affordable and cost-effective,\nplacing sovereign training projects inside a realistic fiscal envelope.\nEfficiency thus plays a dual role: it lowers overall outlays and keeps\ndeployments compatible with existing medium-voltage infrastructure,\ndelaying\u2014but not eliminating\u2014the need for substantial grid upgrades.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx4.SSx3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"43-export-controls-are-not-the-bottleneck\">4.3 Export Controls Are Not the\nBottleneck</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx3.p1\">\n<p class=\"ltx_p\">Export controls do not present a binding constraint for the scenarios\nevaluated in this study. All configurations fall well below the\n50,000-GPU ceiling that has been proposed as a reference point in recent\npolicy debates (Sastry et al., 2024). From a regulatory perspective,\nmiddle-income countries would therefore be able to import the required\nhardware without triggering export restrictions.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx3.p2\">\n<p class=\"ltx_p\">The more significant bottlenecks lie elsewhere. First, hardware\nefficiency determines whether costs remain within a realistic fiscal\nenvelope. Second, while current deployments fit within medium-voltage\nindustrial infrastructure, any attempt to scale toward larger clusters\nwould require substantial upgrades to national electricity grids. As\nnoted in national planning documents such as PRODESEN (SENER,2023) and\nEPE (Empresa de Pesquisa Energ\u00e9tica, 2023) long-term capacity\nprojections (Brazil), sustained sovereign compute will ultimately depend\non aligning AI ambitions with infrastructure investment.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"Sx4.SSx4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"44-policy-implications-for-the-global-south\">4.4 Policy Implications for the Global\nSouth</h3>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx4.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx4.p2\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx4.SSx4.p3\">\n<p class=\"ltx_p\">Finally, the results suggest that compute policy in the Global South\nshould integrate three components: (1) fiscal planning that prioritizes\nefficient accelerators; (2) infrastructure investments to prepare\nnational grids for future scaling (SENER, 2023; Empresa de Pesquisa\nEnerg\u00e9tica [EPE], 2023); and (3) governance mechanisms to ensure\nthat sovereign compute is treated as a public good rather than a purely\ncommercial asset. Together, these elements can position emerging\neconomies to develop AI capabilities that are usable, transparent, and\naligned with institutional needs, without incurring the unsustainable\ncosts of chasing frontier AI.</p>\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"Sx5\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"5-conclusion\">5. Conclusion</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx5.p1\">\n\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx5.p2\">\n<p class=\"ltx_p\">These findings highlight a central distinction for compute governance in\nthe Global South. Sovereign actors do not need to pursue frontier AI\nsystems\u2014whose costs run into the hundreds of millions\u2014but can\ninstead develop strategically sufficient models that are usable,\nauditable, and aligned with national priorities. In this sense, training\ntime emerges as a policy lever: extending schedules allows countries to\nadapt to hardware constraints without requiring immediate access to the\nmost advanced accelerators.</p>\n</div>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx5.p3\">\n<p class=\"ltx_p\">Looking ahead, the challenge will be to integrate sovereign compute into\nbroader strategies for digital infrastructure and energy planning.\nFiscal support for efficient accelerators, targeted investments in\nmedium-voltage grid capacity, and governance mechanisms that treat\ncompute as a public good can ensure that usable AI models are developed\nin ways that serve institutional and societal needs. By focusing on\ncontext-sensitive feasibility rather than frontier competitiveness,\ncountries like Brazil and Mexico can chart realistic pathways toward\ntechnological sovereignty in the age of AI.</p>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx6\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"references\">REFERENCES</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx6.p1\">\n<ul class=\"ltx_itemize\" id=\"Sx6.I8\">\n<li class=\"ltx_item\" id=\"Sx6.I8.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i2.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i3.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i4.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i5\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i5.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i6\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i6.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i7\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i7.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i8\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i8.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i9\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i9.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i10\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i10.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i11\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i11.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i12\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i12.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i13\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i13.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i14\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i14.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i15\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i15.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i16\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"Sx6.I8.i16.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"Sx6.I8.i17\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx6.I8.i17.p1\">\n\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"Sx7\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"annex-1\">Annex 1.</h2>\n<div class=\"ltx_para ltx_noindent\" id=\"Sx7.p1\">\n<p class=\"ltx_p\"><a class=\"ltx_ref ltx_href\" href=\"https://docs.google.com/spreadsheets/d/1NJ3s2nmV7fsEpphn5VulpQpYa5vAjBuO/edit?usp=sharing&amp;ouid=106941722368252898715&amp;rtpof=true&amp;sd=true\" title=\"\"><span class=\"ltx_text ltx_framed ltx_framed_underline\">Annex 1.xlsx</span></a></p>\n</div>\n</section><div about=\"\" class=\"ltx_rdf\" content=\"The Feasibility of Training Sovereign Language Models in the Global South: A Study of Brazil and Mexico\" property=\"dcterms:title\"></div>\n</article>\n</div>\n\n</div>",
    "sections": [
      {
        "id": "the-feasibility-of-training-sovereign-language-models-in-the-global-south-a-study-of-brazil-and-mexico",
        "title": "The Feasibility of Training\nSovereign Language Models in the Global South: A Study of Brazil and\nMexico",
        "level": 1
      },
      {
        "id": "abstract",
        "title": "Abstract",
        "level": 6
      },
      {
        "id": "1-introduction",
        "title": "1. Introduction",
        "level": 2
      },
      {
        "id": "2-methodology",
        "title": "2. Methodology",
        "level": 2
      },
      {
        "id": "21-training-model-assumptions",
        "title": "2.1 Training Model\nAssumptions",
        "level": 3
      },
      {
        "id": "22-hardware-and-training-duration-scenarios",
        "title": "2.2 Hardware and Training Duration\nScenarios",
        "level": 3
      },
      {
        "id": "23-estimation-of-resources",
        "title": "2.3 Estimation of Resources",
        "level": 3
      },
      {
        "id": "energy-consumption-mwh",
        "title": "Energy consumption (MWh)",
        "level": 4
      },
      {
        "id": "peak-electrical-load-mw",
        "title": "Peak electrical load (MW)",
        "level": 4
      },
      {
        "id": "capital-expenditures-capex-hardware-prices-are-set-at",
        "title": "Capital Expenditures (CAPEX)\nHardware prices are set\nat:",
        "level": 4
      },
      {
        "id": "operating-expenditures-opex",
        "title": "Operating Expenditures\n(OPEX)",
        "level": 4
      },
      {
        "id": "24-feasibility-constraints-and-evaluation-framework",
        "title": "2.4 Feasibility Constraints and Evaluation\nFramework",
        "level": 3
      },
      {
        "id": "export-control-ceiling-leq-50000-gpus",
        "title": "Export-control ceiling (\u2264\\leq 50,000\nGPUs)",
        "level": 4
      },
      {
        "id": "electrical-infrastructure-limit",
        "title": "Electrical infrastructure\nlimit",
        "level": 4
      },
      {
        "id": "fiscal-feasibility-cap",
        "title": "Fiscal feasibility cap",
        "level": 4
      },
      {
        "id": "25-case-selection-quer\u00e9taro-and-s\u00e3o-paulo",
        "title": "2.5 Case Selection: Quer\u00e9taro and S\u00e3o\nPaulo",
        "level": 3
      },
      {
        "id": "3-results",
        "title": "3. Results",
        "level": 2
      },
      {
        "id": "31-hardware-requirements",
        "title": "3.1 Hardware Requirements",
        "level": 3
      },
      {
        "id": "32-energy-consumption",
        "title": "3.2 Energy Consumption",
        "level": 3
      },
      {
        "id": "33-costs",
        "title": "3.3 Costs",
        "level": 3
      },
      {
        "id": "34-feasibility-assessment",
        "title": "3.4 Feasibility Assessment",
        "level": 3
      },
      {
        "id": "4-discussion",
        "title": "4. Discussion",
        "level": 2
      },
      {
        "id": "41-sovereign-compute-is-technically-feasible",
        "title": "4.1 Sovereign Compute Is Technically\nFeasible",
        "level": 3
      },
      {
        "id": "42-efficiency-is-the-decisive-variable",
        "title": "4.2 Efficiency Is the Decisive\nVariable",
        "level": 3
      },
      {
        "id": "43-export-controls-are-not-the-bottleneck",
        "title": "4.3 Export Controls Are Not the\nBottleneck",
        "level": 3
      },
      {
        "id": "44-policy-implications-for-the-global-south",
        "title": "4.4 Policy Implications for the Global\nSouth",
        "level": 3
      },
      {
        "id": "5-conclusion",
        "title": "5. Conclusion",
        "level": 2
      },
      {
        "id": "references",
        "title": "REFERENCES",
        "level": 2
      },
      {
        "id": "annex-1",
        "title": "Annex 1.",
        "level": 2
      }
    ],
    "has_math": true
  },
  "cached_at": 1761239082.4837189
}