{
  "success": true,
  "arxiv_id": "2509.25853v1",
  "processed_content": {
    "success": true,
    "arxiv_id": "2509.25853v1",
    "metadata": {
      "arxiv_id": "2509.25853v1",
      "title": "SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV",
      "authors": [],
      "abstract": "Large Language Model (LLM) inference requires substantial computational resources, yet CPU-based inference remains essential for democratizing AI due to the widespread availability of CPUs compared to specialized accelerators. However, efficient LLM inference on CPUs faces two fundamental challenges: (1) existing CPU architectures struggle with low-precision arithmetic required by quantized models, where optimal bit precision varies across models and layers; and (2) the memory-bound nature of the token generation phase creates severe performance bottlenecks.\nTo address these challenges, we propose SAIL (SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that efficiently supports arbitrary bit precisions with minimal overhead. SAIL integrates three key innovations: First, we introduce Batched LUT-based General Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory, enabling high data reuse through lookup tables and reducing memory movement. Second, our Pattern-Aware LUT optimization identifies and exploits redundancy in input activation patterns, reducing computation cycles by 13.8%. Third, we develop an in-memory type conversion algorithm that leverages PIM\u2019s parallelism for efficient de-/quantization operations, alleviating pressure on CPU\u2019s vector units.\nOur architecture requires only 2% hardware overhead and a single new instruction, while maintaining dual functionality as both compute and storage units. Experimental evaluations using a modified gem5 simulator demonstrate that SAIL achieves up to 10.7\u00d7 speedup and 19.9\u00d7 higher tokens per dollar compared to ARM Neoverse-N1 CPU baselines, and up to 7.04\u00d7 better cost efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient CPU-based LLM inference."
    },
    "content": "<div class=\"arxiv-content\">\n<div class=\"ltx_page_content\">\n<article class=\"ltx_document ltx_authors_1line\">\n<h1 class=\"ltx_title ltx_title_document\" id=\"sail-sram-accelerated-llm-inference-system-with-lookup-table-based-gemv\">\nSAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV\n</h1>\n<div class=\"ltx_authors\">\n<span class=\"ltx_creator ltx_role_author\">\n<span class=\"ltx_personname\">\nJingyao Zhang<sup class=\"ltx_sup\">\u2217</sup>, Jaewoo Park<math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"m2\" intent=\":literal\"><semantics><mo>\u2020</mo><annotation encoding=\"application/x-tex\">\\dagger</annotation></semantics></math>, Jongeun Lee<math alttext=\"\\dagger\" class=\"ltx_Math\" display=\"inline\" id=\"m3\" intent=\":literal\"><semantics><mo>\u2020</mo><annotation encoding=\"application/x-tex\">\\dagger</annotation></semantics></math>, and Elaheh Sadredini<sup class=\"ltx_sup\">\u2217</sup>\n</span></span>\n</div>\n<div class=\"ltx_abstract\">\n<h6 class=\"ltx_title ltx_title_abstract\" id=\"abstract\">Abstract</h6>\n<p class=\"ltx_p\">Large Language Model (LLM) inference requires substantial computational resources, yet CPU-based inference remains essential for democratizing AI due to the widespread availability of CPUs compared to specialized accelerators. However, efficient LLM inference on CPUs faces two fundamental challenges: (1) existing CPU architectures struggle with low-precision arithmetic required by quantized models, where optimal bit precision varies across models and layers; and (2) the memory-bound nature of the token generation phase creates severe performance bottlenecks.\nTo address these challenges, we propose SAIL (SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that efficiently supports arbitrary bit precisions with minimal overhead. SAIL integrates three key innovations: First, we introduce Batched LUT-based General Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory, enabling high data reuse through lookup tables and reducing memory movement. Second, our Pattern-Aware LUT optimization identifies and exploits redundancy in input activation patterns, reducing computation cycles by 13.8%. Third, we develop an in-memory type conversion algorithm that leverages PIM\u2019s parallelism for efficient de-/quantization operations, alleviating pressure on CPU\u2019s vector units.\nOur architecture requires only 2% hardware overhead and a single new instruction, while maintaining dual functionality as both compute and storage units. Experimental evaluations using a modified gem5 simulator demonstrate that SAIL achieves up to 10.7\u00d7 speedup and 19.9\u00d7 higher tokens per dollar compared to ARM Neoverse-N1 CPU baselines, and up to 7.04\u00d7 better cost efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient CPU-based LLM inference.</p>\n</div>\n<section class=\"ltx_section\" id=\"S1\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"i-introduction\">\n<span class=\"ltx_tag ltx_tag_section\">I </span><span class=\"ltx_text ltx_font_smallcaps\">Introduction</span>\n</h2>\n<div class=\"ltx_para\" id=\"S1.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p2\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p3\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p4\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p5\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S1.F1\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>\nEfficiency gain comparison between LUT-based and bit-serial computing <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib21\" title=\"\">21</a>]</cite> for 2-bit, 3-bit, and 4-bit quantization across various batch sizes. Dashed lines represent different bit-width quantizations.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S1.p6\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p7\">\n\n</div>\n<div class=\"ltx_para\" id=\"S1.p8\">\n<p class=\"ltx_p\">Our key contributions are:</p>\n<ul class=\"ltx_itemize\" id=\"S1.I1\">\n<li class=\"ltx_item\" id=\"S1.I1.i1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i1.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i2\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i2.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i3\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i3.p1\">\n\n</div>\n</li>\n<li class=\"ltx_item\" id=\"S1.I1.i4\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">\u2022</span>\n<div class=\"ltx_para\" id=\"S1.I1.i4.p1\">\n\n</div>\n</li>\n</ul>\n</div>\n</section>\n<section class=\"ltx_section\" id=\"S2\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"ii-background\">\n<span class=\"ltx_tag ltx_tag_section\">II </span><span class=\"ltx_text ltx_font_smallcaps\">Background</span>\n</h2>\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"ii-a-inference-and-quantization-of-llms\">\n<span class=\"ltx_tag ltx_tag_subsection\">II-A </span><span class=\"ltx_text ltx_font_italic\">Inference and Quantization of LLMs</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS1.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S2.SS1.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"ii-b-processing-in-memory\">\n<span class=\"ltx_tag ltx_tag_subsection\">II-B </span><span class=\"ltx_text ltx_font_italic\">Processing-in-Memory</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS2.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S2.SS2.p2\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S2.F2\">\n\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>\nLUT-based vector multiplication for a 4-bit input vector <math alttext=\"[A,B,C]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.F2.m3\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo>,</mo><mi>C</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[A,B,C]</annotation></semantics></math> and weights <math alttext=\"[W_{0},W_{1},W_{2}]\" class=\"ltx_Math\" display=\"inline\" id=\"S2.F2.m4\" intent=\":literal\"><semantics><mrow><mo stretchy=\"false\">[</mo><msub><mi>W</mi><mn>0</mn></msub><mo>,</mo><msub><mi>W</mi><mn>1</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[W_{0},W_{1},W_{2}]</annotation></semantics></math> using bit-serial computation with NBW = 3.\n</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"ii-c-low-precision-matrix-multiplication-using-lookup-tables\">\n<span class=\"ltx_tag ltx_tag_subsection\">II-C </span><span class=\"ltx_text ltx_font_italic\">Low-precision Matrix Multiplication Using Lookup Tables</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS3.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S2.SS3.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S2.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"ii-d-design-for-lut-gemv-in-pim\">\n<span class=\"ltx_tag ltx_tag_subsection\">II-D </span><span class=\"ltx_text ltx_font_italic\">Design for LUT-GEMV in PIM</span>\n</h3>\n<div class=\"ltx_para\" id=\"S2.SS4.p1\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S3\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"iii-our-approach-sail\">\n<span class=\"ltx_tag ltx_tag_section\">III </span><span class=\"ltx_text ltx_font_smallcaps\">Our Approach: <span class=\"ltx_text ltx_font_italic\">SAIL</span></span>\n</h2>\n<div class=\"ltx_para\" id=\"S3.p1\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F3\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>\n(a) Data flow of SAIL. (b) Operation of common CPU-based inference vs. SAIL.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.p2\">\n\n</div>\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iii-a-tensor-level-scheduling-and-pipeline-design\">\n<span class=\"ltx_tag ltx_tag_subsection\">III-A </span><span class=\"ltx_text ltx_font_italic\">Tensor-level Scheduling and Pipeline Design</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS1.p1\">\n\n</div>\n<figure class=\"ltx_figure\" id=\"S3.F4\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>\n(a) Detailed diagram of computation flow over the proposed architecture. With ping-pong cache, matrix with size of M<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S3.F4.m2\" intent=\":literal\"><semantics><mo>\u00d7</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math>M is written into one half of the cache and then read by the C-SRAM. The C-SRAM perform computation and aggregation of partial results to generate the final results of GEMV. (b) The pipeline diagram. The designed pipeline can be full without bubbles. The write, read and computation (including aggregation) can be fully overlapped.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.SS1.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iii-b-compatibility-with-kv-cache\">\n<span class=\"ltx_tag ltx_tag_subsection\">III-B </span><span class=\"ltx_text ltx_font_italic\">Compatibility with KV-cache</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S3.F5\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>\nDiagram of matrix mapping over compute SRAM. For common matrix multiplication (e.g., Q/K/V and feed-forward layer), weights at the same row are split into different C-SRAM arrays. For KV-cache related computation (e.g., one vector multiplied by a transposed matrix built on KV-cache entries), weights at the same column are split into different C-SRAM arrays.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.SS2.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p2\">\n<p class=\"ltx_p\">Our proposed method supports both quantized and non-quantized KV-caches. After each LUT-based GEMV is completed, the output (e.g., a key entry) is sent to the CPU\u2019s vector engine for dequantization. At this point, if a quantized KV-cache is used, the CPU\u2019s vector engine performs an additional light-weight re-quantization step according to the KV-cache\u2019s quantization requirements before storing it in memory. If a non-quantized KV-cache is used, the output is stored directly in memory. The CPU only processes a single vector per token, making the load negligible.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS2.p3\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iii-c-design-space-exploration\">\n<span class=\"ltx_tag ltx_tag_subsection\">III-C </span><span class=\"ltx_text ltx_font_italic\">Design Space Exploration</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S3.F6\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>\nCycle count trends for different precision levels and NBW (Number of Basis Weights) values across various batch sizes. The optimal NBW varies depending on the batch size and precision level.</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S3.SS3.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S3.SS3.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iii-d-pattern-aware-lut-optimization\">\n<span class=\"ltx_tag ltx_tag_subsection\">III-D </span><span class=\"ltx_text ltx_font_italic\">Pattern-Aware LUT Optimization</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS4.p1\">\n<p class=\"ltx_p\">Our analysis of LLM inference workloads revealed that approximately 17% of input activation patterns repeat within computation batches. To exploit this redundancy, we implemented a Pattern-Aware LUT Access mechanism. Each Data Feeding Module (DFM) contains a 32-entry fully-associative Pattern Reuse Table (PRT). The PRT stores a 32-bit hash of the NBW-bit input pattern along with the previous LUT result. On a PRT hit, the DFM bypasses the C-SRAM access and reuses the stored result. This optimization reduces computation cycles by 13.8%.</p>\n</div>\n<div class=\"ltx_para\" id=\"S3.SS4.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S3.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iii-e-in-memory-parallel-type-conversion\">\n<span class=\"ltx_tag ltx_tag_subsection\">III-E </span><span class=\"ltx_text ltx_font_italic\">In-Memory Parallel Type Conversion</span>\n</h3>\n<div class=\"ltx_para\" id=\"S3.SS5.p1\">\n\n</div>\n<figure class=\"ltx_float ltx_float_algorithm ltx_framed ltx_framed_top\" id=\"alg1\">\n<figcaption class=\"ltx_caption\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_float\"><span class=\"ltx_text ltx_font_bold\">Algorithm 1</span> </span> In-memory parallel type conversion to float</figcaption>\n<div class=\"ltx_listing ltx_listing\">\n<div class=\"ltx_listingline\" id=\"alg1.l1\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">1:</span></span><math alttext=\"n\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l1.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">n</mi><annotation encoding=\"application/x-tex\">n</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">-bit signed integer </span><math alttext=\"A=(a_{n-1},...,a_{0})\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l1.m2\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">A</mi><mo mathsize=\"0.900em\">=</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><msub><mi mathsize=\"0.900em\">a</mi><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">1</mn></mrow></msub><mo mathsize=\"0.900em\">,</mo><mi mathsize=\"0.900em\" mathvariant=\"normal\">\u2026</mi><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">a</mi><mn mathsize=\"0.900em\">0</mn></msub><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">A=(a_{n-1},...,a_{0})</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">, where </span><math alttext=\"n\\leq 25\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l1.m3\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2264</mo><mn mathsize=\"0.900em\">25</mn></mrow><annotation encoding=\"application/x-tex\">n\\leq 25</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l2\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">2:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">32-bit IEEE-754 single-precision floating-point number </span><math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l2.m1\" intent=\":literal\"><semantics><mi mathsize=\"0.900em\">R</mi><annotation encoding=\"application/x-tex\">R</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l3\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">3:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><math alttext=\"C:=(c_{n-2},...,c_{0})=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">C</mi><mo mathsize=\"0.900em\">:=</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><msub><mi mathsize=\"0.900em\">c</mi><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">2</mn></mrow></msub><mo mathsize=\"0.900em\">,</mo><mi mathsize=\"0.900em\" mathvariant=\"normal\">\u2026</mi><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">c</mi><mn mathsize=\"0.900em\">0</mn></msub><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">C:=(c_{n-2},...,c_{0})=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; </span><math alttext=\"D:=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m2\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">D</mi><mo mathsize=\"0.900em\">:=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">D:=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; </span><math alttext=\"R:=(r_{31},...,r_{0})=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m3\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">R</mi><mo mathsize=\"0.900em\">:=</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><msub><mi mathsize=\"0.900em\">r</mi><mn mathsize=\"0.900em\">31</mn></msub><mo mathsize=\"0.900em\">,</mo><mi mathsize=\"0.900em\" mathvariant=\"normal\">\u2026</mi><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">r</mi><mn mathsize=\"0.900em\">0</mn></msub><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">R:=(r_{31},...,r_{0})=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; </span><math alttext=\"Sum:=(s_{4},s_{3},s_{2},s_{1},s_{0})=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l3.m4\" intent=\":literal\"><semantics><mrow><mrow><mi mathsize=\"0.900em\">S</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi mathsize=\"0.900em\">u</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mi mathsize=\"0.900em\">m</mi></mrow><mo mathsize=\"0.900em\">:=</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><msub><mi mathsize=\"0.900em\">s</mi><mn mathsize=\"0.900em\">4</mn></msub><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">s</mi><mn mathsize=\"0.900em\">3</mn></msub><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">s</mi><mn mathsize=\"0.900em\">2</mn></msub><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">s</mi><mn mathsize=\"0.900em\">1</mn></msub><mo mathsize=\"0.900em\">,</mo><msub><mi mathsize=\"0.900em\">s</mi><mn mathsize=\"0.900em\">0</mn></msub><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">Sum:=(s_{4},s_{3},s_{2},s_{1},s_{0})=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l4\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">4:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"i=n-2\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">i</mi><mo mathsize=\"0.900em\">=</mo><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">2</mn></mrow></mrow><annotation encoding=\"application/x-tex\">i=n-2</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> to </span><math alttext=\"0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l4.m2\" intent=\":literal\"><mn mathsize=\"0.900em\">0</mn></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">do</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l5\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">5:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2004</span><math alttext=\"D:=D\\,|\\,a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l5.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">D</mi><mo mathsize=\"0.900em\">:=</mo><mrow><mi mathsize=\"0.900em\">D</mi><mo fence=\"false\" lspace=\"0.448em\" mathsize=\"0.900em\" rspace=\"0.448em\">|</mo><msub><mi mathsize=\"0.900em\">a</mi><mi mathsize=\"0.900em\">i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">D:=D\\,|\\,a_{i}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; </span><math alttext=\"c_{i}:=c_{i}\\,|\\,D\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l5.m2\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">c</mi><mi mathsize=\"0.900em\">i</mi></msub><mo mathsize=\"0.900em\">:=</mo><mrow><msub><mi mathsize=\"0.900em\">c</mi><mi mathsize=\"0.900em\">i</mi></msub><mo fence=\"false\" mathsize=\"0.900em\" rspace=\"0.448em\">|</mo><mi mathsize=\"0.900em\">D</mi></mrow></mrow><annotation encoding=\"application/x-tex\">c_{i}:=c_{i}\\,|\\,D</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Find leading 1 and record position\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l6\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">6:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">end</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l7\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">7:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"i=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">i</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">i=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> to </span><math alttext=\"n-2\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l7.m2\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">2</mn></mrow><annotation encoding=\"application/x-tex\">n-2</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">do</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l8\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">8:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2004</span><math alttext=\"\\texttt{Carry}:=c_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l8.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">Carry</mtext><mo mathsize=\"0.900em\">:=</mo><msub><mi mathsize=\"0.900em\">c</mi><mi mathsize=\"0.900em\">i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\texttt{Carry}:=c_{i}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l9\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">9:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2004</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"j=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l9.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">j</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">j=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> to </span><math alttext=\"4\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l9.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">4</mn><annotation encoding=\"application/x-tex\">4</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">do</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l10\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">10:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2003\u2002\u2005</span><math alttext=\"\\texttt{{c1}}:=s_{j}\\,\\&amp;\\,\\texttt{Carry}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l10.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">c1</mtext><mo mathsize=\"0.900em\">:=</mo><mrow><msub><mi mathsize=\"0.900em\">s</mi><mi mathsize=\"0.900em\">j</mi></msub><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.392em\">&amp;</mo><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">Carry</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{{c1}}:=s_{j}\\,\\&amp;\\,\\texttt{Carry}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; </span><math alttext=\"s_{j}:=s_{j}\\oplus\\texttt{Carry}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l10.m2\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">s</mi><mi mathsize=\"0.900em\">j</mi></msub><mo mathsize=\"0.900em\">:=</mo><mrow><msub><mi mathsize=\"0.900em\">s</mi><mi mathsize=\"0.900em\">j</mi></msub><mo mathsize=\"0.900em\">\u2295</mo><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">Carry</mtext></mrow></mrow><annotation encoding=\"application/x-tex\">s_{j}:=s_{j}\\oplus\\texttt{Carry}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">; </span><math alttext=\"\\texttt{Carry}:=\\texttt{{c1}}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l10.m3\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">Carry</mtext><mo mathsize=\"0.900em\">:=</mo><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">c1</mtext></mrow><annotation encoding=\"application/x-tex\">\\texttt{Carry}:=\\texttt{{c1}}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l11\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">11:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2004</span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">end</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l12\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">12:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">end</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l13\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">13:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><math alttext=\"\\texttt{Sum}:=\\texttt{Sum}+126\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l13.m1\" intent=\":literal\"><semantics><mrow><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">Sum</mtext><mo mathsize=\"0.900em\">:=</mo><mrow><mtext class=\"ltx_mathvariant_monospace\" mathsize=\"0.900em\">Sum</mtext><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">126</mn></mrow></mrow><annotation encoding=\"application/x-tex\">\\texttt{Sum}:=\\texttt{Sum}+126</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Compute biased exponent\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l14\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">14:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><math alttext=\"r_{31}:=r_{31}\\,|\\,a_{n-1}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l14.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">r</mi><mn mathsize=\"0.900em\">31</mn></msub><mo mathsize=\"0.900em\">:=</mo><mrow><msub><mi mathsize=\"0.900em\">r</mi><mn mathsize=\"0.900em\">31</mn></msub><mo fence=\"false\" mathsize=\"0.900em\" rspace=\"0.448em\">|</mo><msub><mi mathsize=\"0.900em\">a</mi><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">1</mn></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">r_{31}:=r_{31}\\,|\\,a_{n-1}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Set sign bit\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l15\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">15:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"i=23\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l15.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">i</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">23</mn></mrow><annotation encoding=\"application/x-tex\">i=23</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> to </span><math alttext=\"27\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l15.m2\" intent=\":literal\"><semantics><mn mathsize=\"0.900em\">27</mn><annotation encoding=\"application/x-tex\">27</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">do</span><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l16\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">16:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2004</span><math alttext=\"r_{i}:=r_{i}\\,|\\,s_{i-23}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l16.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">r</mi><mi mathsize=\"0.900em\">i</mi></msub><mo mathsize=\"0.900em\">:=</mo><mrow><msub><mi mathsize=\"0.900em\">r</mi><mi mathsize=\"0.900em\">i</mi></msub><mo fence=\"false\" mathsize=\"0.900em\" rspace=\"0.448em\">|</mo><msub><mi mathsize=\"0.900em\">s</mi><mrow><mi mathsize=\"0.900em\">i</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">23</mn></mrow></msub></mrow></mrow><annotation encoding=\"application/x-tex\">r_{i}:=r_{i}\\,|\\,s_{i-23}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Set biased exponent\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l17\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">17:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">end</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l18\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">18:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><math alttext=\"C:=\\mathrm{BitReverse}(C+1)&lt;&lt;1\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l18.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">C</mi><mo mathsize=\"0.900em\">:=</mo><mrow><mi mathsize=\"0.900em\">BitReverse</mi><mo lspace=\"0em\" rspace=\"0em\">\u200b</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mi mathsize=\"0.900em\">C</mi><mo mathsize=\"0.900em\">+</mo><mn mathsize=\"0.900em\">1</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">&lt;&lt;</mo><mn mathsize=\"0.900em\">1</mn></mrow><annotation encoding=\"application/x-tex\">C:=\\mathrm{BitReverse}(C+1)&lt;&lt;1</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Bit-serial operations\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l19\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">19:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><math alttext=\"A:=A*C\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l19.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">A</mi><mo mathsize=\"0.900em\">:=</mo><mrow><mi mathsize=\"0.900em\">A</mi><mo lspace=\"0.222em\" mathsize=\"0.900em\" rspace=\"0.222em\">\u2217</mo><mi mathsize=\"0.900em\">C</mi></mrow></mrow><annotation encoding=\"application/x-tex\">A:=A*C</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Align mantissa bits\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l20\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">20:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><math alttext=\"i=0\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l20.m1\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">i</mi><mo mathsize=\"0.900em\">=</mo><mn mathsize=\"0.900em\">0</mn></mrow><annotation encoding=\"application/x-tex\">i=0</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> to </span><math alttext=\"n-3\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l20.m2\" intent=\":literal\"><semantics><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">3</mn></mrow><annotation encoding=\"application/x-tex\">n-3</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">do</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text\" style=\"font-size:90%;float:right;\"><span class=\"ltx_text ltx_font_bold\">//</span>\u2009Set mantissa, remove hidden 1\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l21\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">21:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\">\u2003\u2004</span><math alttext=\"r_{22-(n-3)+i}:=r_{22-(n-3)+i}\\,|\\,a_{i}\" class=\"ltx_Math\" display=\"inline\" id=\"alg1.l21.m1\" intent=\":literal\"><semantics><mrow><msub><mi mathsize=\"0.900em\">r</mi><mrow><mrow><mn mathsize=\"0.900em\">22</mn><mo mathsize=\"0.900em\">\u2212</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">3</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">+</mo><mi mathsize=\"0.900em\">i</mi></mrow></msub><mo mathsize=\"0.900em\">:=</mo><mrow><msub><mi mathsize=\"0.900em\">r</mi><mrow><mrow><mn mathsize=\"0.900em\">22</mn><mo mathsize=\"0.900em\">\u2212</mo><mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">(</mo><mrow><mi mathsize=\"0.900em\">n</mi><mo mathsize=\"0.900em\">\u2212</mo><mn mathsize=\"0.900em\">3</mn></mrow><mo maxsize=\"0.900em\" minsize=\"0.900em\">)</mo></mrow></mrow><mo mathsize=\"0.900em\">+</mo><mi mathsize=\"0.900em\">i</mi></mrow></msub><mo fence=\"false\" mathsize=\"0.900em\" rspace=\"0.448em\">|</mo><msub><mi mathsize=\"0.900em\">a</mi><mi mathsize=\"0.900em\">i</mi></msub></mrow></mrow><annotation encoding=\"application/x-tex\">r_{22-(n-3)+i}:=r_{22-(n-3)+i}\\,|\\,a_{i}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">\n</span>\n</div>\n<div class=\"ltx_listingline\" id=\"alg1.l22\">\n<span class=\"ltx_tag ltx_tag_listingline\"><span class=\"ltx_text\" style=\"font-size:90%;\">22:</span></span><span class=\"ltx_text\" style=\"font-size:90%;\"></span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">end</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">for</span>\n</div>\n</div>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S4\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"iv-implementation\">\n<span class=\"ltx_tag ltx_tag_section\">IV </span><span class=\"ltx_text ltx_font_smallcaps\">Implementation</span>\n</h2>\n<div class=\"ltx_para\" id=\"S4.p1\">\n<p class=\"ltx_p\">In this section, we introduce our implementation of SAIL, which supports the proposed tensor-level scheduling strategy designed to merge batched inference with LUT-based GEMV.</p>\n</div>\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iv-a-isa-design\">\n<span class=\"ltx_tag ltx_tag_subsection\">IV-A </span><span class=\"ltx_text ltx_font_italic\">ISA Design</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS1.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S4.SS1.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iv-b-overall-architecture\">\n<span class=\"ltx_tag ltx_tag_subsection\">IV-B </span><span class=\"ltx_text ltx_font_italic\">Overall Architecture</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S4.F7\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>Overview of our reference implementation. (a) A scalable multicore with shared LLC and individual private caches, extended with C-SRAMs and Data Feeding Modules, (b) C-SRAM, (c) BC-SRAM, (d) Modified SAs, and (e) Transposer.\n</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S4.F8\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>\nExtension of RISC-V instruction set architecture.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S4.SS2.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iv-c-modification-on-address-hasher\">\n<span class=\"ltx_tag ltx_tag_subsection\">IV-C </span><span class=\"ltx_text ltx_font_italic\">Modification on Address Hasher</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S4.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"iv-d-instruction-execution-on-reference-implementation\">\n<span class=\"ltx_tag ltx_tag_subsection\">IV-D </span><span class=\"ltx_text ltx_font_italic\">Instruction Execution on Reference Implementation</span>\n</h3>\n<div class=\"ltx_para\" id=\"S4.SS4.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p2\">\n\n</div>\n<div class=\"ltx_para\" id=\"S4.SS4.p3\">\n\n</div>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S5\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"v-evaluation\">\n<span class=\"ltx_tag ltx_tag_section\">V </span><span class=\"ltx_text ltx_font_smallcaps\">Evaluation</span>\n</h2>\n<section class=\"ltx_subsection\" id=\"S5.SS1\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-a-experimental-setup\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-A </span><span class=\"ltx_text ltx_font_italic\">Experimental Setup</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS1.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p2\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p3\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS1.p4\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S5.T1\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">TABLE I: </span>System and Architectural Parameters (cyc.: cycle)</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:342.8pt;height:143.3pt;vertical-align:-69.6pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-19.0pt,8.0pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">OOO Cores </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib10\" title=\"\">10</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">32 cores, 3 GHz</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"3\"><span class=\"ltx_text\" style=\"font-size:90%;\">Func. Units<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib53\" title=\"\">53</a>]</cite></span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">3 Int ALU</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">2 FP ALU/SIMD</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">4-way decode, 8-way issue</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">L1 I-Cache</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8 MSHRs, 4-way, 64KB, 1 cyc.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">L1 D-Cache</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">20 MSHRs, 4-way, 64KB, 3 cyc.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Priv. L2 Cache</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">46 MSHRs, 8-way, 1MB, 34 cyc.</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">Replacement</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">LRURP</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">L1/2/3 Stride Pf.</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">16 streams, 16 pf./stream</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Shared L3 Cache</span></td>\n<td class=\"ltx_td ltx_align_left\"><span class=\"ltx_text\" style=\"font-size:90%;\">128 MSHRs, 16-way, 32MB, 58 cyc., 32 slices</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">NoC </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib1\" title=\"\">1</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">32B 1cyc. 8</span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u00d7</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">8 Mesh, 2 GHz</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">DRAM</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">8 channels 3200 MHz DDR4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">C-SRAM Array</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">256</span><math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u00d7</mo><annotation encoding=\"application/x-tex\">\\times</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">512 bits. 0.828\u00a0mm</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">2</span></sup><span class=\"ltx_text\" style=\"font-size:90%;\">, 37.076\u00a0mW (est.)</span>\n</td>\n</tr>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS2\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-b-overall-performance\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-B </span><span class=\"ltx_text ltx_font_italic\">Overall Performance</span>\n</h3>\n<figure class=\"ltx_table\" id=\"S5.T2\">\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">TABLE II: </span>Comparison of Inference Performance Across Quantization Levels and Parallelism on CPU Implementations</figcaption><div class=\"ltx_flex_figure\">\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<div class=\"ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer\" style=\"width:500.6pt;height:184pt;vertical-align:-90.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-62.6pt,23.0pt) scale(0.8,0.8) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Tokens</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">1 Thread</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">2 Threads</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">4 Threads</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">8 Threads</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">16 Threads</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">/Second</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ARM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">AMX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"--ltx-bg-color:#E6E6E6;\">SAIL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ARM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">AMX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"--ltx-bg-color:#E6E6E6;\">SAIL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ARM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">AMX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"--ltx-bg-color:#E6E6E6;\">SAIL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ARM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">AMX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"--ltx-bg-color:#E6E6E6;\">SAIL</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">ARM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">AMX</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" style=\"--ltx-bg-color:#E6E6E6;\">SAIL</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">7b-q2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">6.42</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.34</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">12.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.63</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">24.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">4.97</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">14.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">43.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">9.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">24.96</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">81.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">7b-q3</span></td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">2.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.53</span></td>\n<td class=\"ltx_td ltx_align_center\">1.38</td>\n<td class=\"ltx_td ltx_align_center\">3.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">10.93</span></td>\n<td class=\"ltx_td ltx_align_center\">2.71</td>\n<td class=\"ltx_td ltx_align_center\">7.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">20.87</span></td>\n<td class=\"ltx_td ltx_align_center\">5.11</td>\n<td class=\"ltx_td ltx_align_center\">13.69</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">38.40</span></td>\n<td class=\"ltx_td ltx_align_center\">9.62</td>\n<td class=\"ltx_td ltx_align_center\">24.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">73.75</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">7b-q4</span></td>\n<td class=\"ltx_td ltx_align_center\">0.70</td>\n<td class=\"ltx_td ltx_align_center\">3.45</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">4.82</span></td>\n<td class=\"ltx_td ltx_align_center\">1.37</td>\n<td class=\"ltx_td ltx_align_center\">6.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">9.61</span></td>\n<td class=\"ltx_td ltx_align_center\">2.67</td>\n<td class=\"ltx_td ltx_align_center\">11.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">18.67</span></td>\n<td class=\"ltx_td ltx_align_center\">5.15</td>\n<td class=\"ltx_td ltx_align_center\">21.13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">35.17</span></td>\n<td class=\"ltx_td ltx_align_center\">9.85</td>\n<td class=\"ltx_td ltx_align_center\">33.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">72.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">(Intel-q4)</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(2.32)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(4.39)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(8.22)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(14.42)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(22.49)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">7b-q5</span></td>\n<td class=\"ltx_td ltx_align_center\">0.60</td>\n<td class=\"ltx_td ltx_align_center\">1.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.98</span></td>\n<td class=\"ltx_td ltx_align_center\">1.17</td>\n<td class=\"ltx_td ltx_align_center\">2.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">7.96</span></td>\n<td class=\"ltx_td ltx_align_center\">2.32</td>\n<td class=\"ltx_td ltx_align_center\">4.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">15.52</span></td>\n<td class=\"ltx_td ltx_align_center\">4.48</td>\n<td class=\"ltx_td ltx_align_center\">9.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">29.62</span></td>\n<td class=\"ltx_td ltx_align_center\">8.49</td>\n<td class=\"ltx_td ltx_align_center\">16.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">61.84</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">7b-q6</span></td>\n<td class=\"ltx_td ltx_align_center\">0.79</td>\n<td class=\"ltx_td ltx_align_center\">1.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.34</span></td>\n<td class=\"ltx_td ltx_align_center\">1.20</td>\n<td class=\"ltx_td ltx_align_center\">2.33</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">6.67</span></td>\n<td class=\"ltx_td ltx_align_center\">2.36</td>\n<td class=\"ltx_td ltx_align_center\">4.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">12.97</span></td>\n<td class=\"ltx_td ltx_align_center\">4.52</td>\n<td class=\"ltx_td ltx_align_center\">8.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">24.60</span></td>\n<td class=\"ltx_td ltx_align_center\">8.31</td>\n<td class=\"ltx_td ltx_align_center\">14.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">50.63</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">7b-q8</span></td>\n<td class=\"ltx_td ltx_align_center\">0.66</td>\n<td class=\"ltx_td ltx_align_center\">2.30</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">2.60</span></td>\n<td class=\"ltx_td ltx_align_center\">1.28</td>\n<td class=\"ltx_td ltx_align_center\">4.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.22</span></td>\n<td class=\"ltx_td ltx_align_center\">2.51</td>\n<td class=\"ltx_td ltx_align_center\">7.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">10.28</span></td>\n<td class=\"ltx_td ltx_align_center\">4.69</td>\n<td class=\"ltx_td ltx_align_center\">13.55</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">19.86</span></td>\n<td class=\"ltx_td ltx_align_center\">5.54</td>\n<td class=\"ltx_td ltx_align_center\">18.39</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">43.27</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">(Intel-q8)</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(1.60)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(3.09)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(5.80)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(10.32)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(15.59)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">13b-q2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.77</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.06</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">7.44</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">3.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">14.34</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.68</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">7.28</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">26.63</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">5.05</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\">12.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">52.55</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">13b-q3</span></td>\n<td class=\"ltx_td ltx_align_center\">0.35</td>\n<td class=\"ltx_td ltx_align_center\">1.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.67</span></td>\n<td class=\"ltx_td ltx_align_center\">0.69</td>\n<td class=\"ltx_td ltx_align_center\">2.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">7.33</span></td>\n<td class=\"ltx_td ltx_align_center\">1.36</td>\n<td class=\"ltx_td ltx_align_center\">3.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">13.84</span></td>\n<td class=\"ltx_td ltx_align_center\">2.63</td>\n<td class=\"ltx_td ltx_align_center\">7.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">25.70</span></td>\n<td class=\"ltx_td ltx_align_center\">5.01</td>\n<td class=\"ltx_td ltx_align_center\">12.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">51.10</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">13b-q4</span></td>\n<td class=\"ltx_td ltx_align_center\">0.36</td>\n<td class=\"ltx_td ltx_align_center\">1.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">2.81</span></td>\n<td class=\"ltx_td ltx_align_center\">0.72</td>\n<td class=\"ltx_td ltx_align_center\">3.53</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.62</span></td>\n<td class=\"ltx_td ltx_align_center\">1.41</td>\n<td class=\"ltx_td ltx_align_center\">5.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">11.00</span></td>\n<td class=\"ltx_td ltx_align_center\">2.75</td>\n<td class=\"ltx_td ltx_align_center\">10.95</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">21.06</span></td>\n<td class=\"ltx_td ltx_align_center\">5.27</td>\n<td class=\"ltx_td ltx_align_center\">17.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">45.07</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">(Intel-q4)</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(1.21)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(2.31)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(4.39)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(7.87)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(12.74)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">13b-q5</span></td>\n<td class=\"ltx_td ltx_align_center\">0.31</td>\n<td class=\"ltx_td ltx_align_center\">0.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">2.32</span></td>\n<td class=\"ltx_td ltx_align_center\">0.61</td>\n<td class=\"ltx_td ltx_align_center\">1.32</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">4.64</span></td>\n<td class=\"ltx_td ltx_align_center\">1.20</td>\n<td class=\"ltx_td ltx_align_center\">2.52</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">9.10</span></td>\n<td class=\"ltx_td ltx_align_center\">2.34</td>\n<td class=\"ltx_td ltx_align_center\">4.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">17.60</span></td>\n<td class=\"ltx_td ltx_align_center\">4.44</td>\n<td class=\"ltx_td ltx_align_center\">8.56</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">38.24</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">13b-q6</span></td>\n<td class=\"ltx_td ltx_align_center\">0.32</td>\n<td class=\"ltx_td ltx_align_center\">0.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">1.94</span></td>\n<td class=\"ltx_td ltx_align_center\">0.62</td>\n<td class=\"ltx_td ltx_align_center\">1.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.88</span></td>\n<td class=\"ltx_td ltx_align_center\">1.23</td>\n<td class=\"ltx_td ltx_align_center\">2.17</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">7.60</span></td>\n<td class=\"ltx_td ltx_align_center\">2.40</td>\n<td class=\"ltx_td ltx_align_center\">4.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">14.61</span></td>\n<td class=\"ltx_td ltx_align_center\">4.52</td>\n<td class=\"ltx_td ltx_align_center\">7.25</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">31.32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">13b-q8</span></td>\n<td class=\"ltx_td ltx_align_center\">0.34</td>\n<td class=\"ltx_td ltx_align_center\">1.15</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">1.51</span></td>\n<td class=\"ltx_td ltx_align_center\">0.68</td>\n<td class=\"ltx_td ltx_align_center\">2.20</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.03</span></td>\n<td class=\"ltx_td ltx_align_center\">1.29</td>\n<td class=\"ltx_td ltx_align_center\">3.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">5.98</span></td>\n<td class=\"ltx_td ltx_align_center\">2.46</td>\n<td class=\"ltx_td ltx_align_center\">7.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">10.75</span></td>\n<td class=\"ltx_td ltx_align_center\">4.80</td>\n<td class=\"ltx_td ltx_align_center\">10.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">26.25</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">(Intel-q8)</span></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(0.83)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(1.62)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(3.06)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(5.57)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n<td class=\"ltx_td\"></td>\n<td class=\"ltx_td ltx_align_center\">(8.56)</td>\n<td class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\">Geo-Mean</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.52</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.31</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">3.50</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">1.02</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.58</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">6.97</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">2.00</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">4.74</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">13.49</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.85</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">8.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">25.37</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">7.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">14.67</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"--ltx-bg-color:#E6E6E6;\"><span class=\"ltx_text ltx_font_bold\" style=\"--ltx-bg-color:#E6E6E6;\">52.24</span></td>\n</tr>\n</table>\n</span></div>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n<p class=\"ltx_p ltx_figure_panel ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:80%;\">* ARM and SAIL results are from gem5 simulation while AMX is from real hardware execution. ARM results are validated with real hardware results.</span></p>\n</div>\n<div class=\"ltx_flex_break\"></div>\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\n\n</div>\n</div>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS2.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS2.p2\">\n<p class=\"ltx_p\">SAIL\u2019s performance scales significantly better with thread count compared to the other architectures. For instance, in the 7B-Q8 case, ARM\u2019s 16-thread per-thread performance drops to 54% of its single-thread performance, while SAIL maintains 87%. AMX\u2019s scalability is intermediate at 61%. This improved scalability is primarily due to SAIL\u2019s architecture mitigating cache contention and synchronization overhead that affects traditional multi-core CPUs during memory-intensive LLM inference. Consequently, while ARM and AMX show sublinear scaling, SAIL achieves near-linear performance growth.</p>\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS3\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-c-sensitivity-to-quantization-level\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-C </span><span class=\"ltx_text ltx_font_italic\">Sensitivity to Quantization Level</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S5.F9\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>\nSpeedups of SAIL over models with different quantization levels.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS3.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS4\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-d-sensitivity-to-batch-size\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-D </span><span class=\"ltx_text ltx_font_italic\">Sensitivity to Batch Size</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S5.F10\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>\nToken generation speeds of different platforms for various models and batch sizes.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS4.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS4.p2\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS5\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-e-comparison-with-latest-cpu-baselines\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-E </span><span class=\"ltx_text ltx_font_italic\">Comparison with Latest CPU Baselines</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S5.F11\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>Performance comparison of ARM, Non-AMX, AMX, and SAIL at 7B and 13B under Q2, Q4, and Q8.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS5.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS6\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-f-performance-breakdown\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-F </span><span class=\"ltx_text ltx_font_italic\">Performance Breakdown</span>\n</h3>\n<figure class=\"ltx_figure\" id=\"S5.F12\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>\nPerformance breakdown of the proposed in-memory LUT-based GEMV and in-memory type conversion approach, compared with the baseline CPU and Neural Cache (NC) implementations.\n</figcaption>\n</figure>\n<div class=\"ltx_para\" id=\"S5.SS6.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS7\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-g-comparison-with-gpu\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-G </span><span class=\"ltx_text ltx_font_italic\">Comparison with GPU</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS7.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS7.p2\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S5.T3\">\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:328.2pt;height:196.9pt;vertical-align:-96.4pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-18.2pt,10.9pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_border_r ltx_border_tt\" colspan=\"2\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">Tokens per second / batch size</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llama-2-7B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Llama-2-13B</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Quantization Level</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Q4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">Q8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Q4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">Q8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text\" style=\"font-size:90%;\">Context Length</span></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n<td class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">1xV100</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">512</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">216.3 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">190.5 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">173.9 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.02 / 4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">173.4 / 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">126.9 / 4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">126.4 / 4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">42.01 / 2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">123.6 / 2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">84.98 / 2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">85.47 / 2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">38.27 / 1</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#AAC8E6;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#AAC8E6;\">78.98 / 1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#AAC8E6;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#AAC8E6;\">41.62 / 1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#AAC8E6;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#AAC8E6;\">39.97 / 1</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#D9D9D9;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#D9D9D9;\">\u2717</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">2xV100</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">512</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">229.3 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">196.3 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">148.5 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">127.1 / 8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">179.6 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">163.3 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">114.7 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">104.5 / 8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">129.7 / 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">112.6 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">81.99 / 4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">67.71 / 4</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">88.02 / 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">81.90 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">51.15 / 2</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#C8DCF0;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#C8DCF0;\">44.68 / 2</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"4\"><span class=\"ltx_text\" style=\"font-size:90%;\">\n<span class=\"ltx_tabular ltx_align_middle\">\n<span class=\"ltx_tr\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\">A100</span></span>\n</span> </span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">512</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">670.7 / 32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">652.4 / 32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">442.4 / 32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">431.0 / 32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">1K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">425.8 / 32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">418.2 / 32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">278.8 / 32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">274.4 / 32</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">2K</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">255.8 / 32</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">252.7 / 32</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">117.9 / 8</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">109.7 / 8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text\" style=\"font-size:90%;\">4K</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">129.3 / 4</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">120.4 / 4</span></td>\n<td class=\"ltx_td ltx_align_center\" style=\"--ltx-bg-color:#E6F0FA;\"><span class=\"ltx_text\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">87.50 / 4</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">80.16 / 4</span></td>\n</tr>\n<tr class=\"ltx_tr\" style=\"--ltx-bg-color:#E6F0FA;\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">SAIL-16T-8B</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">199.28 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">134.22 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">113.84 / 8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;--ltx-bg-color:#E6F0FA;\">73.93 / 8</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"6\"><span class=\"ltx_text\" style=\"font-size:90%;\">\u2717 does not fit in GPU memory. Numbers after slash are best batch sizes.</span></td>\n</tr>\n</table>\n</span></div>\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">TABLE III: </span>Token Generation Speed Comparison with GPUs</figcaption>\n</figure>\n<figure class=\"ltx_figure\" id=\"S5.F13\">\n\n<br class=\"ltx_break ltx_break\"/>\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_figure\">Figure 13: </span>\nTokens per Dollar over different models and quantization levels across different platforms, with batch size 1 and 8.\n</figcaption>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS8\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-h-tokens-per-dollar-tpd-across-platforms\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-H </span><span class=\"ltx_text ltx_font_italic\">Tokens per Dollar (TPD) across Platforms</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS8.p1\">\n\n</div>\n<div class=\"ltx_para\" id=\"S5.SS8.p2\">\n<p class=\"ltx_p\">At moderate quantizations (Q8\u2013Q3), a single V100 GPU generally achieves higher TPD than a single-thread SAIL configuration, reflecting the GPU\u2019s substantial raw compute power in relation to its cost. However, when the bitwidth drops to Q2, SAIL-1T surpasses the GPU in TPD, revealing that the GPU is comparatively less efficient at ultra-low-bit operations. SAIL\u2019s LUT-based near-data approach, by contrast, retains robust throughput under low-precision settings. This advantage persists when scaling up from 7B to 13B, suggesting that even for larger model sizes, SAIL\u2019s specialized low-bit hardware offers favorable cost-effectiveness.</p>\n</div>\n<div class=\"ltx_para\" id=\"S5.SS8.p3\">\n<p class=\"ltx_p\">In addition to quantization effects, the data also illustrate the influence of batch size and thread count. Increasing the batch size from 1 to 8 enhances TPD across all platforms, but the degree of improvement varies. Multi-GPU setups see performance gains but suffer from steep hardware and operational costs, curtailing their overall TPD growth. CPU-based solutions also exhibit some improvement but often encounter memory bandwidth bottlenecks. As the batch size increases to 8, SAIL surpasses the single-GPU configuration in TPD across all tested quantization levels except for the 13B Q8 setting under a single-thread mode. This improvement stems from amortizing the LUT construction overhead across a greater number of tokens, thereby reducing the effective memory bandwidth demand and improving parallel utilization.</p>\n</div>\n<figure class=\"ltx_table\" id=\"S5.T4\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">TABLE IV: </span>Cost Estimation Based on GCP <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib4\" title=\"\">4</a>]</cite></figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:257.5pt;height:65.9pt;vertical-align:-30.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-14.3pt,3.7pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">System</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\"><span class=\"ltx_text\" style=\"font-size:90%;\">Monthly price ($)</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">5-core CPU w/ 32 GB DRAM</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text\" style=\"font-size:90%;\">292.31</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">16-core CPU w/ 32 GB DRAM</span></td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">665.45</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2-core CPU 1xV100 GPU</span><math alttext=\"{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u2020</mo><annotation encoding=\"application/x-tex\">{\\dagger}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> w/ 15GB DRAM</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">1861.5</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_center\">\n<span class=\"ltx_text\" style=\"font-size:90%;\">2-core CPU 4xV100 GPU</span><math alttext=\"{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u2020</mo><annotation encoding=\"application/x-tex\">{\\dagger}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\"> w/ 15GB DRAM</span>\n</td>\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text\" style=\"font-size:90%;\">7446.0</span></td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"2\">\n<math alttext=\"{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.m3\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u2020</mo><annotation encoding=\"application/x-tex\">{\\dagger}</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">16GB VRAM for each GPU</span>\n</td>\n</tr>\n</table>\n</span></div>\n</figure>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS9\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-i-hardware-overhead\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-I </span><span class=\"ltx_text ltx_font_italic\">Hardware Overhead</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS9.p1\">\n\n</div>\n</section>\n<section class=\"ltx_subsection\" id=\"S5.SS10\">\n<h3 class=\"ltx_title ltx_title_subsection\" id=\"v-j-overhead-comparison-with-asics-and-pims\">\n<span class=\"ltx_tag ltx_tag_subsection\">V-J </span><span class=\"ltx_text ltx_font_italic\">Overhead Comparison with ASICs and PIMs\n</span>\n</h3>\n<div class=\"ltx_para\" id=\"S5.SS10.p1\">\n\n</div>\n<figure class=\"ltx_table\" id=\"S5.T5\">\n<figcaption class=\"ltx_caption ltx_centering\" style=\"font-size:90%;\"><span class=\"ltx_tag ltx_tag_table\">TABLE V: </span>Overhead Comparison of Different Architectures</figcaption>\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" style=\"width:230.6pt;height:975.1pt;vertical-align:-945.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-12.8pt,54.2pt) scale(0.9,0.9) ;\">\n<table class=\"ltx_tabular ltx_align_middle table table-responsive\">\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:48.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Approach</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">HW Overhead</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Sys. Overhead</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:48.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Large-scale ASICs</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib37\" title=\"\">37</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Large buffers and dedicated logics</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Limited memory scalability</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:48.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Small-scale ASICs</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib41\" title=\"\">41</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Extra accelerator for tile-based MM</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Special instructions and compiler</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:48.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">PIMs</span><span class=\"ltx_text\" style=\"font-size:90%;\"> </span><cite class=\"ltx_cite ltx_citemacro_cite\"><span class=\"ltx_text\" style=\"font-size:90%;\">[</span><a class=\"ltx_ref\" href=\"https://arxiv.org/html/2509.25853v1#bib.bib9\" title=\"\">9</a><span class=\"ltx_text\" style=\"font-size:90%;\">]</span></cite></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Compute peripherals (</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m1\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u223c</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">10% area)</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">New instructions &amp; OS modifications</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:48.4pt;\"><span class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">SAIL</span></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Minimal CPU and cache modifications (</span><math alttext=\"\\sim\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T5.m2\" intent=\":literal\"><semantics><mo mathsize=\"0.900em\">\u223c</mo><annotation encoding=\"application/x-tex\">\\sim</annotation></semantics></math><span class=\"ltx_text\" style=\"font-size:90%;\">2% area)</span><sup class=\"ltx_sup\"><span class=\"ltx_text\" style=\"font-size:90%;\">1</span></sup></span>\n</span>\n</td>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\">\n<span class=\"ltx_inline-block ltx_align_top\">\n<span class=\"ltx_p\" style=\"width:74.0pt;\"><span class=\"ltx_text\" style=\"font-size:90%;\">Only One instruction; standard memory hierarchy</span></span>\n</span>\n</td>\n</tr>\n</table>\n<ul class=\"ltx_itemize\" id=\"S5.I1\">\n<li class=\"ltx_item\" id=\"S5.I1.ix1\" style=\"list-style-type:none;\">\n<span class=\"ltx_tag ltx_tag_item\">1</span>\n<div class=\"ltx_para\" id=\"S5.I1.ix1.p1\">\n<p class=\"ltx_p\"><span class=\"ltx_text\" style=\"font-size:90%;\">Includes DFM, adder tree, and pattern redundancy logic; each contributes &lt;0.1% of a 1MB cache slice area.</span></p>\n</div>\n</li>\n</ul>\n</span></div>\n</figure>\n</section>\n</section>\n<section class=\"ltx_section\" id=\"S6\">\n<h2 class=\"ltx_title ltx_title_section\" id=\"vi-conclusion\">\n<span class=\"ltx_tag ltx_tag_section\">VI </span><span class=\"ltx_text ltx_font_smallcaps\">Conclusion</span>\n</h2>\n<div class=\"ltx_para\" id=\"S6.p1\">\n<p class=\"ltx_p\">We present SAIL (SRAM-Accelerated Inference of Large Language Models), a parallel inference system that boosts batched input efficiency via tensor-level scheduling and in-SRAM LUT-based GEMV with pattern-aware optimization. By combining near-cache-slice and in-SRAM computing, SAIL overcomes prior PIM-based limitations, delivering efficient inference across diverse batch sizes, model scales, and quantization levels. SAIL achieves notable performance and cost benefits over CPU, GPU, and existing PIM platforms.</p>\n</div>\n</section>\n<section class=\"ltx_bibliography\" id=\"bib\">\n<h2 class=\"ltx_title ltx_title_bibliography\" id=\"references\">References</h2>\n<ul class=\"ltx_biblist\">\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\n<span class=\"ltx_bibblock\">\n[Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://developer.arm.com/documentation/100180/latest/\" title=\"\">https://developer.arm.com/documentation/100180/latest/</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\n<span class=\"ltx_bibblock\">\n\u201cintel-extension-for-pytorch: A python package for extending the official PyTorch that can easily obtain performance on intel platform.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/intel/intel-extension-for-pytorch\" title=\"\">https://github.com/intel/intel-extension-for-pytorch</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\n<span class=\"ltx_bibblock\">\nLocutusque/TinyMistral-248M \u00b7 hugging face. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M\" title=\"\">https://huggingface.co/Locutusque/TinyMistral-248M</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\n<span class=\"ltx_bibblock\">\nPricing. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://cloud.google.com/compute/all-pricing\" title=\"\">https://cloud.google.com/compute/all-pricing</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\n<span class=\"ltx_bibblock\">\n\u201cray-llm: RayLLM - LLMs on ray.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ray-project/ray-llm\" title=\"\">https://github.com/ray-project/ray-llm</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\n<span class=\"ltx_bibblock\">\n\u201ctriton: Development repository for the triton language and compiler.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/triton-lang/triton\" title=\"\">https://github.com/triton-lang/triton</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Aga, S.\u00a0Jeloka, A.\u00a0Subramaniyan, S.\u00a0Narayanasamy, D.\u00a0Blaauw, and R.\u00a0Das, \u201cCompute caches,\u201d in <em class=\"ltx_emph ltx_font_italic\">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</em>.\u2003ieeexplore.ieee.org, pp. 481\u2013492. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/HPCA.2017.21\" title=\"\">http://dx.doi.org/10.1109/HPCA.2017.21</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Ahmed and M.\u00a0Wahed, \u201cThe de-democratization of AI: Deep learning and the compute divide in artificial intelligence research.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2010.15581\" title=\"\">http://arxiv.org/abs/2010.15581</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Al-Hawaj, T.\u00a0Ta, N.\u00a0Cebry, S.\u00a0Agwa, O.\u00a0Afuye, E.\u00a0Hall, C.\u00a0Golden, A.\u00a0B. Apsel, and C.\u00a0Batten, \u201cEVE: Ephemeral vector engines,\u201d in <em class=\"ltx_emph ltx_font_italic\">2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>.\u2003ieeexplore.ieee.org, pp. 691\u2013704.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\n<span class=\"ltx_bibblock\">\nArm Ltd. Neoverse N1. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.arm.com/products/silicon-ip-cpu/neoverse/neoverse-n1\" title=\"\">https://www.arm.com/products/silicon-ip-cpu/neoverse/neoverse-n1</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Ashfaq, M.\u00a0AskariHemmat, S.\u00a0Sah, E.\u00a0Saboori, O.\u00a0Mastropietro, and A.\u00a0Hoffman, \u201cAccelerating deep learning model inference on arm CPUs with ultra-low bit quantization and runtime.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2207.08820\" title=\"\">http://arxiv.org/abs/2207.08820</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Asifuzzaman, N.\u00a0R. Miniskar, A.\u00a0R. Young, F.\u00a0Liu, and J.\u00a0S. Vetter, \u201cA survey on processing-in-memory techniques: Advances and challenges,\u201d vol.\u00a04, p. 100022. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.sciencedirect.com/science/article/pii/S2773064622000160\" title=\"\">https://www.sciencedirect.com/science/article/pii/S2773064622000160</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0Binkert, B.\u00a0Beckmann, G.\u00a0Black, S.\u00a0K. Reinhardt, A.\u00a0Saidi, A.\u00a0Basu, J.\u00a0Hestness, D.\u00a0R. Hower, T.\u00a0Krishna, S.\u00a0Sardashti, R.\u00a0Sen, K.\u00a0Sewell, M.\u00a0Shoaib, N.\u00a0Vaish, M.\u00a0D. Hill, and D.\u00a0A. Wood, \u201cThe gem5 simulator,\u201d vol.\u00a039, pp. 1\u20137. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://dl.acm.org/doi/abs/10.1145/2024716.2024718?casa_token=AO_OIQlFk_wAAAAA:Xb2Ni0Yb1oy3nD4dUZLwP-4tihD-2Cyp3p7foZBtay8imraKjZEeEi2QLRu1ec39vQHNGIrxYLe-Hg\" title=\"\">https://dl.acm.org/doi/abs/10.1145/2024716.2024718?casa_token=AO_OIQlFk_wAAAAA:Xb2Ni0Yb1oy3nD4dUZLwP-4tihD-2Cyp3p7foZBtay8imraKjZEeEi2QLRu1ec39vQHNGIrxYLe-Hg</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\n<span class=\"ltx_bibblock\">\nG.\u00a0K. Chen, P.\u00a0C. Knag, C.\u00a0Tokunaga, and R.\u00a0K. Krishnamurthy, \u201cAn eight-core RISC-V processor with compute near last level cache in intel 4 CMOS,\u201d vol.\u00a058, pp. 1117\u20131128. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/JSSC.2022.3228765\" title=\"\">http://dx.doi.org/10.1109/JSSC.2022.3228765</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Choi, Z.\u00a0Wang, S.\u00a0Venkataramani, P.\u00a0I.-J. Chuang, V.\u00a0Srinivasan, and K.\u00a0Gopalakrishnan, \u201cPact: Parameterized clipping activation for quantized neural networks,\u201d <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1805.06085</em>, 2018.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Dettmers, M.\u00a0Lewis, Y.\u00a0Belkada, and L.\u00a0Zettlemoyer, \u201cLLM.int8(): 8-bit matrix multiplication for transformers at scale.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2208.07339\" title=\"\">http://arxiv.org/abs/2208.07339</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0S. Ditto, V.\u00a0G. Jithin, and M.\u00a0S. Adarsh, \u201cInference acceleration for large language models on CPUs.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2406.07553\" title=\"\">http://arxiv.org/abs/2406.07553</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Dong, W.\u00a0Cheng, J.\u00a0Qin, and W.\u00a0Wang, \u201cQAQ: Quality adaptive quantization for LLM KV cache.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.04643\" title=\"\">http://arxiv.org/abs/2403.04643</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Duanmu, Z.\u00a0Yuan, X.\u00a0Li, J.\u00a0Duan, X.\u00a0Zhang, and D.\u00a0Lin, \u201cSKVQ: Sliding-window key and value cache quantization for large language models.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2405.06219\" title=\"\">http://arxiv.org/abs/2405.06219</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\n<span class=\"ltx_bibblock\">\nR.-G. Dumitru, V.\u00a0Yadav, R.\u00a0Maheshwary, P.-I. Clotan, S.\u00a0T. Madhusudhan, and M.\u00a0Surdeanu, \u201cLayer-wise quantization: A pragmatic and effective method for quantizing LLMs beyond integer bit-levels.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2406.17415\" title=\"\">http://arxiv.org/abs/2406.17415</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Eckert, X.\u00a0Wang, J.\u00a0Wang, A.\u00a0Subramaniyan, R.\u00a0Iyer, D.\u00a0Sylvester, D.\u00a0Blaaauw, and R.\u00a0Das, \u201cNeural Cache: Bit-Serial In-Cache Acceleration of Deep Neural Networks,\u201d in <em class=\"ltx_emph ltx_font_italic\">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</em>, pp. 383\u2013396.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Eckert, X.\u00a0Wang, J.\u00a0Wang, A.\u00a0Subramaniyan, R.\u00a0Iyer, D.\u00a0Sylvester, D.\u00a0Blaaauw, and R.\u00a0Das, \u201cNeural cache: Bit-serial in-cache acceleration of deep neural networks,\u201d in <em class=\"ltx_emph ltx_font_italic\">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</em>, pp. 383\u2013396. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/ISCA.2018.00040\" title=\"\">http://dx.doi.org/10.1109/ISCA.2018.00040</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0K. Esser, J.\u00a0L. McKinstry, D.\u00a0Bablani, R.\u00a0Appuswamy, and D.\u00a0S. Modha, \u201cLearned step size quantization,\u201d <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:1902.08153</em>, 2019.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Fujiki, S.\u00a0Mahlke, and R.\u00a0Das, \u201cDuality cache for data parallel acceleration,\u201d in <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 46th International Symposium on Computer Architecture</em>, ser. ISCA \u201919.\u2003Association for Computing Machinery, pp. 397\u2013410. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.1145/3307650.3322257\" title=\"\">https://doi.org/10.1145/3307650.3322257</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Gao, X.\u00a0Xin, Y.\u00a0Lu, Y.\u00a0Zhang, J.\u00a0Yang, and J.\u00a0Shu, \u201cParaBit: Processing parallel bitwise operations in NAND flash memory based SSDs,\u201d in <em class=\"ltx_emph ltx_font_italic\">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</em>, ser. MICRO \u201921.\u2003Association for Computing Machinery, pp. 59\u201370. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.1145/3466752.3480078\" title=\"\">https://doi.org/10.1145/3466752.3480078</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\n<span class=\"ltx_bibblock\">\nG.\u00a0Gerganov, \u201cllama.cpp: LLM inference in C/c++.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/ggerganov/llama.cpp\" title=\"\">https://github.com/ggerganov/llama.cpp</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Gholami, Z.\u00a0Yao, S.\u00a0Kim, C.\u00a0Hooper, M.\u00a0W. Mahoney, and K.\u00a0Keutzer, \u201cAI and memory wall.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.14123\" title=\"\">http://arxiv.org/abs/2403.14123</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Gong, J.\u00a0Liu, J.\u00a0Wang, X.\u00a0Cai, D.\u00a0Zhao, and R.\u00a0Yan, \u201cWhat makes quantization for large language models hard? an empirical study from the lens of perturbation.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.06408\" title=\"\">http://arxiv.org/abs/2403.06408</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\n<span class=\"ltx_bibblock\">\nG.\u00a0F. Grohoski, M.\u00a0Shah, J.\u00a0D. Davis, A.\u00a0Saulsbury, C.\u00a0Fu, V.\u00a0Iyengar, J.-Y. Tsai, and J.\u00a0Gibson, \u201cLevel 2 cache index hashing to avoid hot spots,\u201d patentus 7\u2009290\u2009116. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://patentimages.storage.googleapis.com/c9/ca/4d/f096c940c3081e/US7290116B1.pdf\" title=\"\">https://patentimages.storage.googleapis.com/c9/ca/4d/f096c940c3081e/US7290116B1.pdf</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\n<span class=\"ltx_bibblock\">\nM.\u00a0R. Guthaus, J.\u00a0E. Stine, S.\u00a0Ataei, B.\u00a0Chen, B.\u00a0Wu, and M.\u00a0Sarwar, \u201cOpenRAM: An open-source memory compiler,\u201d in <em class=\"ltx_emph ltx_font_italic\">2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)</em>.\u2003ieeexplore.ieee.org, pp. 1\u20136. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1145/2966986.2980098\" title=\"\">http://dx.doi.org/10.1145/2966986.2980098</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0G\u00f3mez-Luna, I.\u00a0El\u00a0Hajj, I.\u00a0Fernandez, C.\u00a0Giannoula, G.\u00a0F. Oliveira, and O.\u00a0Mutlu, \u201cBenchmarking memory-centric computing systems: Analysis of real processing-in-memory hardware,\u201d in <em class=\"ltx_emph ltx_font_italic\">2021 12th International Green and Sustainable Computing Conference (IGSC)</em>.\u2003IEEE, pp. 1\u20137. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/IGSC54211.2021.9651614\" title=\"\">http://dx.doi.org/10.1109/IGSC54211.2021.9651614</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Hooper, S.\u00a0Kim, H.\u00a0Mohammadzadeh, and others, \u201cKVQuant: Towards 10 million context length LLM inference with KV cache quantization.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://arxiv.org/abs/2401.18079\" title=\"\">https://arxiv.org/abs/2401.18079</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Hooper, S.\u00a0Kim, H.\u00a0Mohammadzadeh, M.\u00a0W. Mahoney, Y.\u00a0S. Shao, K.\u00a0Keutzer, and A.\u00a0Gholami, \u201cKvquant: Towards 10 million context length llm inference with kv cache quantization,\u201d 2024. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://arxiv.org/abs/2401.18079\" title=\"\">https://arxiv.org/abs/2401.18079</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Hu, H.\u00a0Huang, L.\u00a0Xu, X.\u00a0Chen, J.\u00a0Xu, S.\u00a0Chen, H.\u00a0Feng, C.\u00a0Wang, S.\u00a0Wang, Y.\u00a0Bao, N.\u00a0Sun, and Y.\u00a0Shan, \u201cInference without interference: Disaggregate LLM inference for mixed downstream workloads,\u201d vol. abs/2401.11181. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.48550/arXiv.2401.11181\" title=\"\">http://dx.doi.org/10.48550/arXiv.2401.11181</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Jeltes. Alternative hardware for artificial intelligence. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.cursor.tue.nl/en/background/2020/april/week-3/alternative-hardware-for-artificial-intelligence/\" title=\"\">https://www.cursor.tue.nl/en/background/2020/april/week-3/alternative-hardware-for-artificial-intelligence/</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Jiang, P.\u00a0Pan, Y.\u00a0Ou, and C.\u00a0Batten, \u201cPyMTL3: A python framework for open-source hardware modeling, generation, simulation, and verification,\u201d vol.\u00a040, pp. 58\u201366. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/MM.2020.2997638\" title=\"\">http://dx.doi.org/10.1109/MM.2020.2997638</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\n<span class=\"ltx_bibblock\">\nN.\u00a0P. Jouppi, C.\u00a0Young, N.\u00a0Patil, D.\u00a0Patterson, G.\u00a0Agrawal, R.\u00a0Bajwa, S.\u00a0Bates, S.\u00a0Bhatia, N.\u00a0Boden, A.\u00a0Borchers, R.\u00a0Boyle, P.-L. Cantin, C.\u00a0Chao, C.\u00a0Clark, J.\u00a0Coriell, M.\u00a0Daley, M.\u00a0Dau, J.\u00a0Dean, B.\u00a0Gelb, T.\u00a0V. Ghaemmaghami, R.\u00a0Gottipati, W.\u00a0Gulland, R.\u00a0Hagmann, C.\u00a0Richard\u00a0Ho, D.\u00a0Hogberg, J.\u00a0Hu, R.\u00a0Hundt, D.\u00a0Hurt, J.\u00a0Ibarz, A.\u00a0Jaffey, A.\u00a0Jaworski, A.\u00a0Kaplan, H.\u00a0Khaitan, A.\u00a0Koch, N.\u00a0Kumar, S.\u00a0Lacy, J.\u00a0Laudon, J.\u00a0Law, D.\u00a0Le, C.\u00a0Leary, Z.\u00a0Liu, K.\u00a0Lucke, A.\u00a0Lundin, G.\u00a0MacKean, A.\u00a0Maggiore, M.\u00a0Mahony, K.\u00a0Miller, R.\u00a0Nagarajan, R.\u00a0Narayanaswami, R.\u00a0Ni, K.\u00a0Nix, T.\u00a0Norrie, M.\u00a0Omernick, N.\u00a0Penukonda, A.\u00a0Phelps, J.\u00a0Ross, M.\u00a0Ross, A.\u00a0Salek, E.\u00a0Samadiani, C.\u00a0Severn, G.\u00a0Sizikov, M.\u00a0Snelham, J.\u00a0Souter, D.\u00a0Steinberg, A.\u00a0Swing, M.\u00a0Tan, G.\u00a0Thorson, B.\u00a0Tian, H.\u00a0Toma, E.\u00a0Tuttle, V.\u00a0Vasudevan, R.\u00a0Walter, W.\u00a0Wang, E.\u00a0Wilcox, and D.\u00a0H. Yoon, \u201cIn-datacenter performance analysis of a tensor processing unit.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/1704.04760\" title=\"\">http://arxiv.org/abs/1704.04760</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\n<span class=\"ltx_bibblock\">\nK.\u00a0Kamahori, Y.\u00a0Gu, K.\u00a0Zhu, and B.\u00a0Kasikci, \u201cFiddler: CPU-GPU orchestration for fast inference of mixture-of-experts models.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2402.07033\" title=\"\">http://arxiv.org/abs/2402.07033</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Kang, Q.\u00a0Zhang, S.\u00a0Kundu, G.\u00a0Jeong, Z.\u00a0Liu, T.\u00a0Krishna, and T.\u00a0Zhao, \u201cGEAR: An efficient KV cache compression recipe for near-lossless generative inference of LLM.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.05527\" title=\"\">http://arxiv.org/abs/2403.05527</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\n<span class=\"ltx_bibblock\">\nP.\u00a0Kennedy, \u201c[no title],\u201d https://www.servethehome.com/three-new-amd-epyc-genoa-x-skus-with-up-to-48mb-l3-cache-per-core-or-1-1gb-total/, accessed: 2024-1-11.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Kim, G.\u00a0Ye, N.\u00a0Wang, A.\u00a0Yazdanbakhsh, and N.\u00a0S. Kim, \u201cExploiting intel advanced matrix extensions (AMX) for large language model inference,\u201d vol.\u00a023, pp. 117\u2013120. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/LCA.2024.3397747\" title=\"\">http://dx.doi.org/10.1109/LCA.2024.3397747</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Kim, C.\u00a0Hooper, T.\u00a0Wattanawong, M.\u00a0Kang, R.\u00a0Yan, H.\u00a0Genc, G.\u00a0Dinh, Q.\u00a0Huang, K.\u00a0Keutzer, M.\u00a0W. Mahoney, Y.\u00a0S. Shao, and A.\u00a0Gholami, \u201cFull stack optimization of transformer inference: a survey.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2302.14017\" title=\"\">http://arxiv.org/abs/2302.14017</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[43]</span>\n<span class=\"ltx_bibblock\">\nR.\u00a0Krishnamoorthi, \u201cQuantizing deep convolutional networks for efficient inference: A whitepaper.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/1806.08342\" title=\"\">http://arxiv.org/abs/1806.08342</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[44]</span>\n<span class=\"ltx_bibblock\">\nW.\u00a0Kwon, Z.\u00a0Li, S.\u00a0Zhuang, Y.\u00a0Sheng, L.\u00a0Zheng, C.\u00a0H. Yu, J.\u00a0Gonzalez, H.\u00a0Zhang, and I.\u00a0Stoica, \u201cEfficient memory management for large language model serving with PagedAttention,\u201d in <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 29th Symposium on Operating Systems Principles</em>, ser. SOSP \u201923.\u2003Association for Computing Machinery, pp. 611\u2013626. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.1145/3600006.3613165\" title=\"\">https://doi.org/10.1145/3600006.3613165</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[45]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Lee, C.\u00a0Kim, S.\u00a0Kang, D.\u00a0Shin, S.\u00a0Kim, and H.-J. Yoo, \u201cUNPU: An energy-efficient deep neural network accelerator with fully variable weight bit precision,\u201d vol.\u00a054, pp. 173\u2013185. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/JSSC.2018.2865489\" title=\"\">http://dx.doi.org/10.1109/JSSC.2018.2865489</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[46]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Lin, J.\u00a0Tang, H.\u00a0Tang, S.\u00a0Yang, X.\u00a0Dang, and S.\u00a0Han, \u201cAWQ: Activation-aware weight quantization for LLM compression and acceleration.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2306.00978\" title=\"\">http://arxiv.org/abs/2306.00978</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[47]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Liu, B.\u00a0Oguz, C.\u00a0Zhao, E.\u00a0Chang, P.\u00a0Stock, Y.\u00a0Mehdad, Y.\u00a0Shi, R.\u00a0Krishnamoorthi, and V.\u00a0Chandra, \u201cLlm-qat: Data-free quantization aware training for large language models,\u201d <em class=\"ltx_emph ltx_font_italic\">arXiv preprint arXiv:2305.17888</em>, 2023.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[48]</span>\n<span class=\"ltx_bibblock\">\nC.\u00a0Lomont, \u201cIntroduction to intel advanced vector extensions,\u201d vol.\u00a023, pp. 1\u201321. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://hpc.llnl.gov/sites/default/files/intelAVXintro.pdf\" title=\"\">https://hpc.llnl.gov/sites/default/files/intelAVXintro.pdf</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[49]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Menshawy, Z.\u00a0Nawaz, and M.\u00a0Fahmy, \u201cNavigating challenges and technical debt in large language models deployment,\u201d in <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 4th Workshop on Machine Learning and Systems</em>, ser. EuroMLSys \u201924.\u2003Association for Computing Machinery, pp. 192\u2013199. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.1145/3642970.3655840\" title=\"\">https://doi.org/10.1145/3642970.3655840</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[50]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Minaee, T.\u00a0Mikolov, N.\u00a0Nikzad, M.\u00a0Chenaghlu, R.\u00a0Socher, X.\u00a0Amatriain, and J.\u00a0Gao, \u201cLarge language models: A survey.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2402.06196\" title=\"\">http://arxiv.org/abs/2402.06196</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[51]</span>\n<span class=\"ltx_bibblock\">\nNVIDIA, https://github.com/NVIDIA/FasterTransformer.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[52]</span>\n<span class=\"ltx_bibblock\">\nY.\u00a0Park, K.\u00a0Budhathoki, L.\u00a0Chen, J.\u00a0K\u00fcbler, J.\u00a0Huang, M.\u00a0Kleindessner, J.\u00a0Huan, V.\u00a0Cevher, Y.\u00a0Wang, and G.\u00a0Karypis, \u201cInference optimization of foundation models on AI accelerators.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2407.09111\" title=\"\">http://arxiv.org/abs/2407.09111</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib53\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[53]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Pellegrini, N.\u00a0Stephens, M.\u00a0Bruce, Y.\u00a0Ishii, J.\u00a0Pusdesris, A.\u00a0Raja, C.\u00a0Abernathy, J.\u00a0Koppanalil, T.\u00a0Ringe, A.\u00a0Tummala, J.\u00a0Jalal, M.\u00a0Werkheiser, and A.\u00a0Kona, \u201cThe arm neoverse n1 platform: Building blocks for the next-gen cloud-to-edge infrastructure soc,\u201d <em class=\"ltx_emph ltx_font_italic\">IEEE Micro</em>, vol.\u00a040, no.\u00a02, pp. 53\u201362, 2020.\n\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib54\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[54]</span>\n<span class=\"ltx_bibblock\">\nV.\u00a0G. Reddy, \u201cNeon technology introduction,\u201d vol.\u00a04, pp. 1\u201333. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://caxapa.ru/thumbs/301908/AT_-_NEON_for_Multimedia_Applications.pdf\" title=\"\">http://caxapa.ru/thumbs/301908/AT_-_NEON_for_Multimedia_Applications.pdf</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib55\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[55]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Reis, H.\u00a0Geng, M.\u00a0Niemier, and X.\u00a0S. Hu, \u201cIMCRYPTO: An in-memory computing fabric for AES encryption and decryption,\u201d vol.\u00a030, pp. 553\u2013565. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/TVLSI.2022.3157270\" title=\"\">http://dx.doi.org/10.1109/TVLSI.2022.3157270</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib56\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[56]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Sch\u00f6nleber, L.\u00a0Cavigelli, M.\u00a0Perotti, L.\u00a0Benini, and R.\u00a0Andri, \u201cStella nera: A differentiable maddness-based hardware accelerator for efficient approximate matrix multiplication.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2311.10207\" title=\"\">http://arxiv.org/abs/2311.10207</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib57\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[57]</span>\n<span class=\"ltx_bibblock\">\nE.\u00a0Seger, A.\u00a0Ovadya, B.\u00a0Garfinkel, D.\u00a0Siddarth, and A.\u00a0Dafoe, \u201cDemocratising AI: Multiple meanings, goals, and methods.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2303.12642\" title=\"\">http://arxiv.org/abs/2303.12642</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib58\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[58]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Shen, H.\u00a0Chang, B.\u00a0Dong, Y.\u00a0Luo, and H.\u00a0Meng, \u201cEfficient LLM inference on CPUs.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://arxiv.org/abs/2311.00502\" title=\"\">https://arxiv.org/abs/2311.00502</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib59\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[59]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0E. Stine, I.\u00a0Castellanos, M.\u00a0Wood, J.\u00a0Henson, F.\u00a0Love, W.\u00a0R. Davis, P.\u00a0D. Franzon, M.\u00a0Bucher, S.\u00a0Basavarajaiah, J.\u00a0Oh, and R.\u00a0Jenkal, \u201cFreePDK: An open-source variation-aware design kit,\u201d in <em class=\"ltx_emph ltx_font_italic\">2007 IEEE International Conference on Microelectronic Systems Education (MSE\u201907)</em>.\u2003IEEE, pp. 173\u2013174. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://ieeexplore.ieee.org/abstract/document/4231502/?casa_token=d_nll3XZ0LwAAAAA:JISouQ-xA33oIjqJgv_BJBPuw7Y0XBtmoKGBOBZ5QcuzxOz2ik-q5iBIf2MZKBO6ReNAiRfbF5LBYg\" title=\"\">https://ieeexplore.ieee.org/abstract/document/4231502/?casa_token=d_nll3XZ0LwAAAAA:JISouQ-xA33oIjqJgv_BJBPuw7Y0XBtmoKGBOBZ5QcuzxOz2ik-q5iBIf2MZKBO6ReNAiRfbF5LBYg</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib60\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[60]</span>\n<span class=\"ltx_bibblock\">\nF.\u00a0Strati, S.\u00a0Mcallister, A.\u00a0Phanishayee, J.\u00a0Tarnawski, and A.\u00a0Klimovic, \u201cD\u00e9j\u00e0Vu: KV-cache streaming for fast, fault-tolerant generative LLM serving.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.01876\" title=\"\">http://arxiv.org/abs/2403.01876</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib61\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[61]</span>\n<span class=\"ltx_bibblock\">\nH.\u00a0Touvron, L.\u00a0Martin, K.\u00a0Stone, P.\u00a0Albert, A.\u00a0Almahairi, Y.\u00a0Babaei, N.\u00a0Bashlykov, S.\u00a0Batra, P.\u00a0Bhargava, S.\u00a0Bhosale, D.\u00a0Bikel, L.\u00a0Blecher, C.\u00a0C. Ferrer, M.\u00a0Chen, G.\u00a0Cucurull, D.\u00a0Esiobu, J.\u00a0Fernandes, J.\u00a0Fu, W.\u00a0Fu, B.\u00a0Fuller, C.\u00a0Gao, V.\u00a0Goswami, N.\u00a0Goyal, A.\u00a0Hartshorn, S.\u00a0Hosseini, R.\u00a0Hou, H.\u00a0Inan, M.\u00a0Kardas, V.\u00a0Kerkez, M.\u00a0Khabsa, I.\u00a0Kloumann, A.\u00a0Korenev, P.\u00a0S. Koura, M.-A. Lachaux, T.\u00a0Lavril, J.\u00a0Lee, D.\u00a0Liskovich, Y.\u00a0Lu, Y.\u00a0Mao, X.\u00a0Martinet, T.\u00a0Mihaylov, P.\u00a0Mishra, I.\u00a0Molybog, Y.\u00a0Nie, A.\u00a0Poulton, J.\u00a0Reizenstein, R.\u00a0Rungta, K.\u00a0Saladi, A.\u00a0Schelten, R.\u00a0Silva, E.\u00a0M. Smith, R.\u00a0Subramanian, X.\u00a0E. Tan, B.\u00a0Tang, R.\u00a0Taylor, A.\u00a0Williams, J.\u00a0X. Kuan, P.\u00a0Xu, Z.\u00a0Yan, I.\u00a0Zarov, Y.\u00a0Zhang, A.\u00a0Fan, M.\u00a0Kambadur, S.\u00a0Narang, A.\u00a0Rodriguez, R.\u00a0Stojnic, S.\u00a0Edunov, and T.\u00a0Scialom, \u201cLlama 2: Open foundation and fine-tuned chat models.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2307.09288\" title=\"\">http://arxiv.org/abs/2307.09288</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib62\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[62]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Verma. Mastering LLM techniques: Inference optimization. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/\" title=\"\">https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib63\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[63]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Wang, Y.\u00a0Zhao, X.\u00a0Hou, and H.\u00a0Wang, \u201cLarge language model supply chain: A research agenda.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2404.12736\" title=\"\">http://arxiv.org/abs/2404.12736</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib64\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[64]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wang, C.\u00a0Liu, A.\u00a0Arora, L.\u00a0John, and T.\u00a0Nowatzki, \u201cInfinity stream: Portable and programmer-friendly in-/near-memory fusion,\u201d in <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3</em>, ser. ASPLOS 2023.\u2003Association for Computing Machinery, pp. 359\u2013375. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://doi.org/10.1145/3582016.3582032\" title=\"\">https://doi.org/10.1145/3582016.3582032</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib65\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[65]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Wang, J.\u00a0Weng, S.\u00a0Liu, and T.\u00a0Nowatzki, \u201cNear-stream computing: General and transparent near-cache acceleration,\u201d in <em class=\"ltx_emph ltx_font_italic\">2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</em>.\u2003IEEE, pp. 331\u2013345. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1109/HPCA53966.2022.00032\" title=\"\">http://dx.doi.org/10.1109/HPCA53966.2022.00032</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib66\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[66]</span>\n<span class=\"ltx_bibblock\">\nA.\u00a0Waterman, Y.\u00a0Lee, D.\u00a0Patterson, K.\u00a0Asanovic, V.\u00a0I.\u00a0U. level Isa, A.\u00a0Waterman, Y.\u00a0Lee, and D.\u00a0Patterson, \u201cThe RISC-V instruction set manual,\u201d vol.\u00a02, pp. 1\u201379. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://isrc.iscas.ac.cn/gitlab/mirrors/github.com/riscv_riscv-isa-manual/-/raw/3b4c695efcb7933ca9b647913792ccb04db02553/release/riscv-spec-v2.0.pdf\" title=\"\">https://isrc.iscas.ac.cn/gitlab/mirrors/github.com/riscv_riscv-isa-manual/-/raw/3b4c695efcb7933ca9b647913792ccb04db02553/release/riscv-spec-v2.0.pdf</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib67\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[67]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Wei, S.\u00a0Cao, T.\u00a0Cao, L.\u00a0Ma, L.\u00a0Wang, Y.\u00a0Zhang, and M.\u00a0Yang, \u201cT-MAC: CPU renaissance via table lookup for low-bit LLM deployment on edge,\u201d in <em class=\"ltx_emph ltx_font_italic\">Proceedings of the Twentieth European Conference on Computer Systems</em>.\u2003ACM, pp. 278\u2013292. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://dx.doi.org/10.1145/3689031.3696099\" title=\"\">http://dx.doi.org/10.1145/3689031.3696099</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib68\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[68]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Wolf, L.\u00a0Debut, V.\u00a0Sanh, J.\u00a0Chaumond, C.\u00a0Delangue, A.\u00a0Moi, P.\u00a0Cistac, T.\u00a0Rault, R.\u00a0Louf, M.\u00a0Funtowicz, J.\u00a0Davison, S.\u00a0Shleifer, P.\u00a0von Platen, C.\u00a0Ma, Y.\u00a0Jernite, J.\u00a0Plu, C.\u00a0Xu, T.\u00a0L. Scao, S.\u00a0Gugger, M.\u00a0Drame, Q.\u00a0Lhoest, and A.\u00a0M. Rush, \u201cTransformers: State-of-the-art natural language processing,\u201d in <em class=\"ltx_emph ltx_font_italic\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>.\u2003Online: Association for Computational Linguistics, Oct. 2020, pp. 38\u201345. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.aclweb.org/anthology/2020.emnlp-demos.6\" title=\"\">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib69\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[69]</span>\n<span class=\"ltx_bibblock\">\nG.\u00a0Xiao, J.\u00a0Lin, M.\u00a0Seznec, H.\u00a0Wu, J.\u00a0Demouth, and S.\u00a0Han, \u201cSmoothQuant: Accurate and efficient post-training quantization for large language models,\u201d pp. 38\u2009087\u201338\u2009099. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://proceedings.mlr.press/v202/xiao23c.html\" title=\"\">https://proceedings.mlr.press/v202/xiao23c.html</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib70\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[70]</span>\n<span class=\"ltx_bibblock\">\nG.\u00a0Yenduri, R.\u00a0M, C.\u00a0S. G, S.\u00a0Y, G.\u00a0Srivastava, P.\u00a0K.\u00a0R. Maddikunta, D.\u00a0R. G, R.\u00a0H. Jhaveri, P.\u00a0B, W.\u00a0Wang, A.\u00a0V. Vasilakos, and T.\u00a0R. Gadekallu, \u201cGenerative pre-trained transformer: A comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions,\u201d 2023. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://arxiv.org/abs/2305.10435\" title=\"\">https://arxiv.org/abs/2305.10435</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib71\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[71]</span>\n<span class=\"ltx_bibblock\">\nG.-I. Yu, J.\u00a0S. Jeong, G.-W. Kim, S.\u00a0Kim, and B.-G. Chun, \u201cOrca: A distributed serving system for <math alttext=\"\\{\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib71.m1\" intent=\":literal\"><semantics><mo stretchy=\"false\">{</mo><annotation encoding=\"application/x-tex\">\\{</annotation></semantics></math>Transformer-Based<math alttext=\"\\}\" class=\"ltx_Math\" display=\"inline\" id=\"bib.bib71.m2\" intent=\":literal\"><semantics><mo stretchy=\"false\">}</mo><annotation encoding=\"application/x-tex\">\\}</annotation></semantics></math> generative models,\u201d in <em class=\"ltx_emph ltx_font_italic\">16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)</em>.\u2003usenix.org, pp. 521\u2013538. [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://www.usenix.org/conference/osdi22/presentation/yu\" title=\"\">https://www.usenix.org/conference/osdi22/presentation/yu</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib72\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[72]</span>\n<span class=\"ltx_bibblock\">\nZ.\u00a0Zeng, M.\u00a0Davies, P.\u00a0Pulijala, K.\u00a0Sankaralingam, and V.\u00a0Singh, \u201cLookupFFN: Making transformers compute-lite for CPU inference.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.07221\" title=\"\">http://arxiv.org/abs/2403.07221</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib73\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[73]</span>\n<span class=\"ltx_bibblock\">\nJ.\u00a0Zhang, M.\u00a0Imani, and E.\u00a0Sadredini, \u201cBP-NTT: Fast and compact in-SRAM number theoretic transform with bit-parallel modular multiplication.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2303.00173\" title=\"\">http://arxiv.org/abs/2303.00173</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib74\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[74]</span>\n<span class=\"ltx_bibblock\">\nS.\u00a0Zhang, S.\u00a0Roller, N.\u00a0Goyal, M.\u00a0Artetxe, M.\u00a0Chen, S.\u00a0Chen, C.\u00a0Dewan, M.\u00a0Diab, X.\u00a0Li, X.\u00a0V. Lin, T.\u00a0Mihaylov, M.\u00a0Ott, S.\u00a0Shleifer, K.\u00a0Shuster, D.\u00a0Simig, P.\u00a0S. Koura, A.\u00a0Sridhar, T.\u00a0Wang, and L.\u00a0Zettlemoyer, \u201cOPT: Open pre-trained transformer language models.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2205.01068\" title=\"\">http://arxiv.org/abs/2205.01068</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib75\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[75]</span>\n<span class=\"ltx_bibblock\">\nT.\u00a0Zhang, J.\u00a0W. Yi, B.\u00a0Yao, Z.\u00a0Xu, and A.\u00a0Shrivastava, \u201cNoMAD-attention: Efficient LLM inference on CPUs through multiply-add-free attention.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2403.01273\" title=\"\">http://arxiv.org/abs/2403.01273</a>\n</span>\n</li>\n<li class=\"ltx_bibitem\" id=\"bib.bib76\">\n<span class=\"ltx_tag ltx_tag_bibitem\">[76]</span>\n<span class=\"ltx_bibblock\">\nD.\u00a0Zhao, S.\u00a0Samsi, J.\u00a0McDonald, B.\u00a0Li, D.\u00a0Bestor, M.\u00a0Jones, D.\u00a0Tiwari, and V.\u00a0Gadepally, \u201cSustainable supercomputing for AI: GPU power capping at HPC scale.\u201d [Online]. Available: <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"http://arxiv.org/abs/2402.18593\" title=\"\">http://arxiv.org/abs/2402.18593</a>\n</span>\n</li>\n</ul>\n</section>\n</article>\n</div>\n\n</div>",
    "sections": [
      {
        "id": "sail-sram-accelerated-llm-inference-system-with-lookup-table-based-gemv",
        "title": "SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV",
        "level": 1
      },
      {
        "id": "abstract",
        "title": "Abstract",
        "level": 6
      },
      {
        "id": "i-introduction",
        "title": "I Introduction",
        "level": 2
      },
      {
        "id": "ii-background",
        "title": "II Background",
        "level": 2
      },
      {
        "id": "ii-a-inference-and-quantization-of-llms",
        "title": "II-A Inference and Quantization of LLMs",
        "level": 3
      },
      {
        "id": "ii-b-processing-in-memory",
        "title": "II-B Processing-in-Memory",
        "level": 3
      },
      {
        "id": "ii-c-low-precision-matrix-multiplication-using-lookup-tables",
        "title": "II-C Low-precision Matrix Multiplication Using Lookup Tables",
        "level": 3
      },
      {
        "id": "ii-d-design-for-lut-gemv-in-pim",
        "title": "II-D Design for LUT-GEMV in PIM",
        "level": 3
      },
      {
        "id": "iii-our-approach-sail",
        "title": "III Our Approach: SAIL",
        "level": 2
      },
      {
        "id": "iii-a-tensor-level-scheduling-and-pipeline-design",
        "title": "III-A Tensor-level Scheduling and Pipeline Design",
        "level": 3
      },
      {
        "id": "iii-b-compatibility-with-kv-cache",
        "title": "III-B Compatibility with KV-cache",
        "level": 3
      },
      {
        "id": "iii-c-design-space-exploration",
        "title": "III-C Design Space Exploration",
        "level": 3
      },
      {
        "id": "iii-d-pattern-aware-lut-optimization",
        "title": "III-D Pattern-Aware LUT Optimization",
        "level": 3
      },
      {
        "id": "iii-e-in-memory-parallel-type-conversion",
        "title": "III-E In-Memory Parallel Type Conversion",
        "level": 3
      },
      {
        "id": "iv-implementation",
        "title": "IV Implementation",
        "level": 2
      },
      {
        "id": "iv-a-isa-design",
        "title": "IV-A ISA Design",
        "level": 3
      },
      {
        "id": "iv-b-overall-architecture",
        "title": "IV-B Overall Architecture",
        "level": 3
      },
      {
        "id": "iv-c-modification-on-address-hasher",
        "title": "IV-C Modification on Address Hasher",
        "level": 3
      },
      {
        "id": "iv-d-instruction-execution-on-reference-implementation",
        "title": "IV-D Instruction Execution on Reference Implementation",
        "level": 3
      },
      {
        "id": "v-evaluation",
        "title": "V Evaluation",
        "level": 2
      },
      {
        "id": "v-a-experimental-setup",
        "title": "V-A Experimental Setup",
        "level": 3
      },
      {
        "id": "v-b-overall-performance",
        "title": "V-B Overall Performance",
        "level": 3
      },
      {
        "id": "v-c-sensitivity-to-quantization-level",
        "title": "V-C Sensitivity to Quantization Level",
        "level": 3
      },
      {
        "id": "v-d-sensitivity-to-batch-size",
        "title": "V-D Sensitivity to Batch Size",
        "level": 3
      },
      {
        "id": "v-e-comparison-with-latest-cpu-baselines",
        "title": "V-E Comparison with Latest CPU Baselines",
        "level": 3
      },
      {
        "id": "v-f-performance-breakdown",
        "title": "V-F Performance Breakdown",
        "level": 3
      },
      {
        "id": "v-g-comparison-with-gpu",
        "title": "V-G Comparison with GPU",
        "level": 3
      },
      {
        "id": "v-h-tokens-per-dollar-tpd-across-platforms",
        "title": "V-H Tokens per Dollar (TPD) across Platforms",
        "level": 3
      },
      {
        "id": "v-i-hardware-overhead",
        "title": "V-I Hardware Overhead",
        "level": 3
      },
      {
        "id": "v-j-overhead-comparison-with-asics-and-pims",
        "title": "V-J Overhead Comparison with ASICs and PIMs",
        "level": 3
      },
      {
        "id": "vi-conclusion",
        "title": "VI Conclusion",
        "level": 2
      },
      {
        "id": "references",
        "title": "References",
        "level": 2
      }
    ],
    "has_math": true
  },
  "cached_at": 1761251582.995563
}